{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Lecture3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FK4Tiq1oxz3g",
        "Icj8bapFxz3g",
        "4eavG12Lxz3g",
        "6XDRWhHsxz3h",
        "C7o0pogyxz3i",
        "O3G9cM7_xz3j",
        "P42thfHrxz3j",
        "EjqgjBo7xz3l",
        "O7rjgpH5xz3q",
        "hybT2Yp3xz3q",
        "baWO_RQTxz3y",
        "f8CXxNDGxz3y",
        "0n7KsFlMxz3z",
        "78upWypwxz3z",
        "gGvs3k68xz3z",
        "QIs2bUT8xz3z",
        "LEEiUwXCxz3z",
        "POtbb9rdxz3z",
        "Kxjy-9r4xz3z",
        "YRBrz6_fxz31",
        "8ATPPUNPxz34",
        "BW6I4jP8xz34",
        "ZibjDJumxz35",
        "6Z6EvlrHxz35",
        "x5opw4oKxz37",
        "EKj7beubxz38",
        "ofPWsDLjxz38",
        "l9LiYiebxz38",
        "wKNxp-oOxz38",
        "06ZvGan1xz38",
        "48cvH63pxz4A",
        "3RDy2rT9xz4D",
        "t5eHrnPzxz4E",
        "uD--fwKixz4G",
        "2mcfAsIrxz4J",
        "vBeaOvlcxz4J",
        "sN-0k81uxz4N",
        "qGorL0fpxz4N",
        "kEeZ7EgXxz4N",
        "t3U0cviJxz4O",
        "yzsacy9yxz4P",
        "DSMB9nhnxz4R",
        "QZtucH1Yxz4U",
        "EYboX73Jxz4W",
        "knXxzIc8xz4X",
        "lSENnwKdxz4X",
        "VukfbGMGxz4Y",
        "uy2cI60Wxz4c",
        "s8_24bmDxz4g",
        "zJt3r94Vxz4h",
        "wPkccem4xz4k",
        "gSi07sbgxz4k",
        "E9fxpgACxz4k",
        "3WC0wd_yxz4l",
        "vGf0gpjNxz4l",
        "qJQ26qCoxz4l",
        "Cll_rwpVxz4p",
        "6u_bbxy7xz4p",
        "Ek2K3UqWxz4q",
        "gSFCc-mExz4r",
        "eLsUwUvyxz4s",
        "Y8--U1eOxz4s",
        "51sRwjaKxz4u",
        "_tGV2ubbxz4v",
        "pd3U5Q5-xz4v",
        "iBjATcZnxz4w",
        "meshN6JJxz4x",
        "fN6yFRo_xz4y",
        "MkW7uwApxz40",
        "63zcv0Uzxz41",
        "xLBopEiRxz42",
        "8W8rjaq7xz43",
        "OxfeEHSExz47",
        "zP3miGnWxz48",
        "u7UpbekLxz49",
        "XpCBVR6Pxz49",
        "3VaAdfu-xz4-",
        "yAGQ0wlTxz4-",
        "WaDAhpTSxz5C",
        "Z1VYtlbExz5E",
        "m2ihbkI5xz5H",
        "GdoKX0vXxz5J",
        "TMDZz9QPxz5J",
        "7umbsDjwxz5M",
        "PQG4E1mWxz5O",
        "gUG8affkxz5Q",
        "NmWeuv8qxz5R",
        "F_jbc85zxz5R"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzEMgWjGxzsT"
      },
      "source": [
        "<a ><img src=\"https://cdn.tuinvest.de/assets/logos/TUInvest_Logo-353d42494757660b8381a31c9f99a6ca.png\"  width=\"200\" align=\"left\"> </a>\n",
        "<div style=\"text-align: right\"> <h3><span style=\"color:gray\"> INTERNAL USE ONLY </span> </h3> </div>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<a ><img src=\"Pictures/QSeries.png\" Width=\"300\" align=\"center\"> </a>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "<h1><center>AlgoTrading 101 â€” A hands-on Introduction</center></h1>\n",
        "<h2><center> <span style=\"font-weight:normal\"><font color='#022F73'> Lecture 3: Backtesting and Portfolio Strategy </font>  </span></center></h2>\n",
        "\n",
        "\n",
        "<h3><center><font color='gray'>JONAS GOTTAL</font></center></h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO2hJc3wxzsW"
      },
      "source": [
        "<h4>About this Notebook</h4>\n",
        "The Objective of this Lecture is a comprehensible Introduction to Algo Trading <br>\n",
        "for the members of TU Invest, to raise curiosity and resurrect Alternative Strategies/Assets.\n",
        "\n",
        "Although the material showed is not fit to  *understand*  for beginners, with easier strategies <br>\n",
        "there won't be a profitable among them. Thus the math and stats behind the following stratgies <br>\n",
        "might be hard to understand but not to implement. And I hope this will lead some curious <br>\n",
        "minds to further study the concepts more thoroughly. <br>\n",
        "*Therefore the sole Momentum and Pair Trading Strategies will be very short.*\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<span style=\"color:gray\"> * *To limit our time spent per section, basics in Python and Statistics are welcomed. <br> If not, there will be links to a <a href=\"#11\"> Backup Section</a> with additional code and explanations.* </span>\n",
        "<br>\n",
        "<br>\n",
        "___\n",
        "___\n",
        "<a><span style=\"color:black\"> <b> Legend:</b></span> </a><a> &nbsp; &nbsp;&nbsp;&nbsp;&nbsp; </a> <a><span style=\"color:black\"> <b>|</b></span> </a> <a> &nbsp; &nbsp;&nbsp;&nbsp;&nbsp; </a><a><span style=\"color:#022F73\"> UNDERSTAND ME</span> </a> <a> &nbsp; &nbsp;&nbsp;&nbsp;&nbsp; </a><a><span style=\"color:black\"> <b>|</b></span> </a> <a> &nbsp; &nbsp;&nbsp;&nbsp;&nbsp; </a><a><span style=\"color:black\"> REMEMBER ME</span> </a><a> &nbsp; &nbsp;&nbsp;&nbsp;&nbsp; </a><a><span style=\"color:black\"> <b>|</b></span> </a> <a> &nbsp; &nbsp;&nbsp;&nbsp;&nbsp; </a><a><span style=\"color:gray\"> READ ME</span> </a>\n",
        "___\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqQx4LXuxzsY"
      },
      "source": [
        "<div style=\"text-align: center\"> <h3><span style=\"color:red\"> THIS IS A WORKING DRAFT TO BE SPLIT IN LECTURES WHEN FINISHED </span> </h3> </div>\n",
        "\n",
        "<div style=\"text-align: center\"> <h3><span style=\"color:red\"> <em> Many parts will be cut or at least reduced </em></span> </h3> </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI5wU5naxzsa"
      },
      "source": [
        "<h1>Table of contents</h1>\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"text-decoration:none; margin-top: 30px; background-color:#F2F2F2; border-color:#022F73\">\n",
        "    <span style=\"color:#022F73\">\n",
        "    <ol>\n",
        "        <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >Backtesting</span> </a></li>\n",
        "       <ol>\n",
        "        <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >Libraries</span> </a></li>   \n",
        "       <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >Biases: Look-Ahead, Survivorship and Overfitting</span> </a></li>    \n",
        "       <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >Transaction Costs</span> </a></li>      \n",
        "        </ol>\n",
        "       <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >Performance</span> </a></li>\n",
        "       <ol>\n",
        "        <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >Returns, Risk and Drawdown</span> </a></li>   \n",
        "       <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >Beta Hedging</span> </a></li>    \n",
        "       <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >Portfolio Overview</span> </a></li>      \n",
        "        </ol>\n",
        "        <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >Portfolio Strategy</span> </a></li>\n",
        "       <ol>\n",
        "        <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >Money Management</span> </a></li>    \n",
        "       <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >Markowitz</span> </a></li>      \n",
        "        </ol> \n",
        "        <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >APPENDIX</span> </a></li>\n",
        "        <ol>\n",
        "        <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >Outlook for Part II: Derivatives, Big Data and Sentiment Analysis</span> </a></li>    \n",
        "       <li><a href=\"#20\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
        "       >Additional Sources</span> </a></li> \n",
        "        </ol>\n",
        "    </ol>\n",
        "    </span>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq5rk3lDxzsg"
      },
      "source": [
        "# !pip install auquan_toolbox\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dtale\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import backtester\n",
        "import statsmodels\n",
        "import statsmodels.api as sm\n",
        "import os\n",
        "import sys\n",
        "from statsmodels.tsa.stattools import coint, adfuller\n",
        "from scipy import stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "FK4Tiq1oxz3g"
      },
      "source": [
        "# Backtesting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "Icj8bapFxz3g"
      },
      "source": [
        "## What is overfitting?\n",
        "\n",
        "When constructing a model, we tune both the parameters and the model by fitting to sample data. We then use the model and parameters to predict data we have not yet observed. We say a model is overfit when it is overly sensitive to noise and idiosyncracies in the sample data, and therefore does not reflect the underlying data-generating process.\n",
        "\n",
        "To understand why this happens, one has to consider the amount of noise present in any dataset. One can consider a set of data as $D_{T}$, the true underlying data that came from whatever process we are trying to model, and $\\epsilon$, some random noise. Therefore, what we actually see is $D = D_{T} + \\epsilon$. By building a complicated model we might fit our model to very perfectly predict for the given $\\epsilon$. However, when we test on future out of sample data, this model will fail to predict for $D_{T}$.\n",
        "\n",
        "In practice, we only care about fitting to the sample insofar as that gives an accurate fit to future data. The two broad causes of overfitting are:\n",
        "* small sample size, so that noise and trend are not distinguishable\n",
        "* choosing an overly complex model, so that it ends up contorting to fit the noise in the sample\n",
        "\n",
        "Generalizing this to stocks, if your model starts developing many specific rules based on specific past events, it is almost definitely overfitting. This is why black-box machine learning (neural networks, etc.) is so dangerous when not done correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "4eavG12Lxz3g"
      },
      "source": [
        "## Verbal Example: Too Many Rules (Complexity)\n",
        "\n",
        "Let's say you have the following dataset:\n",
        "\n",
        "| TV Channel | Room Lighting Intensity | Enjoyment |\n",
        "|------------|-------------------------|-----------|\n",
        "| 1          | 2                       | 1         |\n",
        "| 2          | 3                       | 2         |\n",
        "| 3          | 1                       | 3         |\n",
        "\n",
        "You are trying to predict enjoyment, so you create the following rules:\n",
        "\n",
        "1. If TV Channel is 1 and Lighting Intensity is 2, then Enjoyment will be 1.\n",
        "2. If TV Channel is 2 and Lighting Intensity is 3, then Enjoyment will be 2.\n",
        "3. If TV Channel is 3 and Lighting Intensity is 1, then Enjoyment will be 3.\n",
        "4. In all other cases predict an average enjoyment of 2.\n",
        "\n",
        "This is a well defined model for future data, however, in this case let's say your enjoyment is purely dependent on the tv channel and not on the lighting. Because we have a rule for each row in our dataset, our model is perfectly predictive in our historical data, but would performly poorly in real trials because  we are overfitting to random noise in the lighting intensity data.\n",
        "\n",
        "Generalizing this to stocks, if your model starts developing many specific rules based on specific past events, it is almost definitely overfitting. This is why black-box machine learning (neural networks, etc.) is so dangerous when not done correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "6XDRWhHsxz3h"
      },
      "source": [
        "## Example: Curve fitting\n",
        "\n",
        "Overfitting is most easily seen when we look at polynomial regression. Below we construct a dataset which noisily follows a quadratic. The linear model is underfit: simple linear models aren't suitable for all situations, especially when we have reason to believe that the data is nonlinear. The quadratic curve has some error but fits the data well.\n",
        "\n",
        "When we fit a ninth-degree polynomial to the data, the error is zero - a ninth-degree polynomial can be constructed to go through any 10 points - but, looking at the tails of the curve, we know that we can't expect it to accurately predict other samples from the same distribution. It fits the data perfectly, but that is because it also fits the noise perfectly, and the noise is not what we want to model. In this case we have selected a model that is too complex."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "zLJktslwxz3h"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels import regression\n",
        "from scipy import poly1d\n",
        "import auquanToolbox.dataloader as dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "2XF-aTGqxz3i"
      },
      "source": [
        "x = np.arange(10)\n",
        "y = 2*np.random.randn(10) + x**2\n",
        "xs = np.linspace(-0.25, 9.25, 200)\n",
        "\n",
        "lin = np.polyfit(x, y, 1)\n",
        "quad = np.polyfit(x, y, 2)\n",
        "many = np.polyfit(x, y, 9)\n",
        "\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.scatter(x, y)\n",
        "plt.plot(xs, poly1d(lin)(xs))\n",
        "plt.plot(xs, poly1d(quad)(xs))\n",
        "plt.plot(xs, poly1d(many)(xs))\n",
        "plt.ylabel('Y')\n",
        "plt.xlabel('X')\n",
        "plt.legend(['Underfit', 'Good fit', 'Overfit']);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ilcs3BTcxz3i"
      },
      "source": [
        "When working with real data, there is unlikely to ever be a situation where a ninth-degree polynomial is appropriate: our choice of function should reflect a belief about the underlying process, and real-world processes generally do not follow high-degree polynomial curves. This example is contrived, but it can be tempting to use a quadratic or cubic model just to decrease sample error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "C7o0pogyxz3i"
      },
      "source": [
        "## Note: Model/Parameter Parsimony\n",
        "\n",
        "Just as the most elegant physics models describe a tremendous amount of our world through a few equations, a good trading model should explain most of the data through a few rules. Any time you start to have a number of rules even close to the number of points in your data set, you can be sure you are overfitting. Since parameters can be thought of as rules as they equivalently constrain a model, the same is true of parameters. Fewer parameters is better, and it is better to explain 60% of the data with 2-3 paremeters than 90% with 10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "O3G9cM7_xz3j"
      },
      "source": [
        "## Beware of the perfect fit\n",
        "\n",
        "Because there is almost always noise present in real data, a perfect fit is almost always indicative of overfitting. It is almost impossible to know the percentage noise/signal in a given data set while you are developing the model, but use your common sense. Are the predictions surprisingly good? Then you're probably overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "P42thfHrxz3j"
      },
      "source": [
        "## Example: Regression parameters\n",
        "\n",
        "How do we know which variables to include in a model? If we're afraid of omitting something important, we might try different ones and include all the variables we can find that improve the fit. Below we regress one asset that is in the same sector as the asset whose price we're trying to predict, and three other unrelated ones. In our initial timeframe, we are able to fit the model more closely to the data when using multiple variables than when using just one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "9Xt5ud9oxz3j"
      },
      "source": [
        "# Load one year's worth of pricing data for five different assets\n",
        "start = '2014-01-01'\n",
        "end = '2016-01-01'\n",
        "m1='PEP'\n",
        "m2='MCD'\n",
        "m3 ='CVS'\n",
        "m4='DOW'\n",
        "m5='PG'\n",
        "data = dl.load_data_nologs('nasdaq', [m1,m2,m3,m4,m5], start, end)\n",
        "x1 = data['ADJ CLOSE'][m1]\n",
        "x2 = data['ADJ CLOSE'][m2]\n",
        "x3 = data['ADJ CLOSE'][m3]\n",
        "x4 = data['ADJ CLOSE'][m4]\n",
        "y = data['ADJ CLOSE'][m5]\n",
        "\n",
        "# Build a linear model using only x1 to explain y\n",
        "slr = regression.linear_model.OLS(y, sm.add_constant(x1)).fit()\n",
        "slr_prediction = slr.params[0] + slr.params[1]*x1\n",
        "\n",
        "# Run multiple linear regression using x1, x2, x3, x4 to explain y\n",
        "mlr = regression.linear_model.OLS(y, sm.add_constant(np.column_stack((x1,x2,x3,x4)))).fit()\n",
        "mlr_prediction = mlr.params[0] + mlr.params[1]*x1 + mlr.params[2]*x2 + mlr.params[3]*x3 + mlr.params[4]*x4\n",
        "\n",
        "# Compute adjusted R-squared for the two different models\n",
        "print ('Using only PEP R-squared:', slr.rsquared_adj)\n",
        "print ('Using only PEP p-value:', slr.f_pvalue)\n",
        "print ('Using a basket of stocks R-squared:', mlr.rsquared_adj)\n",
        "print ('Using a basket of stocks p-value:', mlr.f_pvalue)\n",
        "# Plot y along with the two different predictions\n",
        "y.plot(figsize=(15,7))\n",
        "slr_prediction.plot()\n",
        "mlr_prediction.plot()\n",
        "plt.legend(['PG', 'Only PEP', 'Basket of stocks']);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Udi-Xj50xz3k"
      },
      "source": [
        "However, when we use the same estimated parameters to model a different time period, we find that the single-variable model fits worse, while the multiple-variable model is entirely useless. It seems that the relationships we found are not consistent and are particular to the original sample period."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "lfpuCBePxz3k"
      },
      "source": [
        "# Load a year and a half of pricing data\n",
        "start = '2016-01-01'\n",
        "end = '2016-06-01'\n",
        "data = dl.load_data_nologs('nasdaq', [m1,m2,m3,m4,m5], start, end)\n",
        "x1 = data['ADJ CLOSE'][m1]\n",
        "x2 = data['ADJ CLOSE'][m2]\n",
        "x3 = data['ADJ CLOSE'][m3]\n",
        "x4 = data['ADJ CLOSE'][m4]\n",
        "y = data['ADJ CLOSE'][m5]\n",
        "\n",
        "# Extend our model from before to the new time period\n",
        "slr_prediction2 = slr.params[0] + slr.params[1]*x1\n",
        "mlr_prediction2 = mlr.params[0] + mlr.params[1]*x1 + mlr.params[2]*x2 + mlr.params[3]*x3 + mlr.params[4]*x4\n",
        "\n",
        "# Compute adjusted R-squared over the extended time period\n",
        "adj = float(len(y) - 1)/(len(y) - 5) # Compute adjustment factor\n",
        "SST = sum((y - np.mean(y))**2)\n",
        "SSRs = sum((slr_prediction2 - y)**2)\n",
        "print ('Using only PEP R-squared:', 1 - adj*SSRs/SST)\n",
        "SSRm = sum((mlr_prediction2 - y)**2)\n",
        "print ('Using a basket of stocks R-squared:', 1 - adj*SSRm/SST)\n",
        "\n",
        "# Plot y along with the two different predictions\n",
        "y.plot(figsize=(15,7))\n",
        "slr_prediction2.plot()\n",
        "mlr_prediction2.plot()\n",
        "plt.legend(['PG', 'Only PEP', 'Basket']);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "rlB4KnMAxz3l"
      },
      "source": [
        "If we wanted, we could scan our universe for variables that were correlated with the dependent variable, and construct an extremely overfitted model. However, in most cases the correlation will be spurious, and the relationship will not continue into the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "EjqgjBo7xz3l"
      },
      "source": [
        "## Example: Rolling windows\n",
        "\n",
        "One of the challenges in building a model that uses rolling parameter estimates, such as rolling mean or rolling beta, is choosing a window length. A longer window will take into account long-term trends and be less volatile, but it will also lag more when taking into account new observations. The choice of window length strongly affects the rolling parameter estimate and can change how we see and treat the data. Below we calculate the rolling averages of a stock price for different window lengths:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "_OqdoteXxz3l"
      },
      "source": [
        "# Load the pricing data for a stock\n",
        "start = '2012-01-01'\n",
        "end = '2014-06-30'\n",
        "assets = ['MCD']\n",
        "data = dl.load_data_nologs('nasdaq', assets, start, end)\n",
        "prices = data['ADJ CLOSE']\n",
        "asset = prices.iloc[:, 0]\n",
        "# Compute rolling averages for various window lengths\n",
        "mu_30d = asset.rolling(window=30, center=False).mean()\n",
        "mu_60d = asset.rolling(window=60, center=False).mean()\n",
        "mu_100d = asset.rolling(window=100, center=False).mean()\n",
        "\n",
        "# Plot asset pricing data with rolling means from the 100th day, when all the means become available\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.plot(asset[100:], label='Asset')\n",
        "plt.plot(mu_30d[100:], label='30d MA')\n",
        "plt.plot(mu_60d[100:], label='60d MA')\n",
        "plt.plot(mu_100d[100:], label='100d MA')\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ZBxb3anBxz3m"
      },
      "source": [
        "If we pick the length based on which seems best - say, on how well our model or algorithm performs - we are overfitting. Below we have a simple trading algorithm which bets on the stock price reverting to the rolling mean (for more details, check out the mean reversion notebook). We use the performance of this algorithm to score window lengths and find the best one. However, when we consider a different timeframe, this window length is far from optimal. This is because our original choice was overfitted to the sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "U8IGB4RAxz3m"
      },
      "source": [
        "# Trade using a simple mean-reversion strategy\n",
        "def trade(stock, length):\n",
        "    \n",
        "    # If window length is 0, algorithm doesn't make sense, so exit\n",
        "    if length == 0:\n",
        "        return 0\n",
        "    \n",
        "    # Compute rolling mean and rolling standard deviation\n",
        "    mu = stock.rolling(window=length, center=False).mean()\n",
        "    std = stock.rolling(window=length, center=False).std()\n",
        "    \n",
        "    # Compute the z-scores for each day using the historical data up to that day\n",
        "    zscores = (stock - mu)/std\n",
        "    \n",
        "    # Simulate trading\n",
        "    # Start with no money and no positions\n",
        "    money = 0\n",
        "    count = 0\n",
        "    for i in range(len(stock)):\n",
        "        # Sell short if the z-score is > 1\n",
        "        if zscores[i] > 1:\n",
        "            money += stock[i]\n",
        "            count -= 1\n",
        "        # Buy long if the z-score is < 1\n",
        "        elif zscores[i] < -1:\n",
        "            money -= stock[i]\n",
        "            count += 1\n",
        "        # Clear positions if the z-score between -.5 and .5\n",
        "        elif abs(zscores[i]) < 0.5:\n",
        "            money += count*stock[i]\n",
        "            count = 0\n",
        "    return money"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "yE4k5WMYxz3n"
      },
      "source": [
        "# Find the window length 0-254 that gives the highest returns using this strategy\n",
        "length_scores = [trade(asset, l) for l in range(255)]\n",
        "best_length = np.argmax(length_scores)\n",
        "print ('Best window length:', best_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "BcYCB7ASxz3o"
      },
      "source": [
        "# Get pricing data for a different timeframe\n",
        "start2 = '2014-06-30'\n",
        "end2 = '2017-01-01'\n",
        "assets = ['MCD']\n",
        "data2 = dl.load_data_nologs('nasdaq', assets, start2, end2)\n",
        "prices2 = data2['ADJ CLOSE']\n",
        "asset2 = prices2.iloc[:, 0]\n",
        "\n",
        "# Find the returns during this period using what we think is the best window length\n",
        "length_scores2 = [trade(asset2, l) for l in range(255)]\n",
        "print (best_length, 'day window:', length_scores2[best_length])\n",
        "\n",
        "# Find the best window length based on this dataset, and the returns using this window length\n",
        "best_length2 = np.argmax(length_scores2)\n",
        "print (best_length2, 'day window:', length_scores2[best_length2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "2eRXZNxgxz3o"
      },
      "source": [
        "Clearly fitting to our sample data doesn't always give good results in the future. Just for fun, let's plot the length scores computed from the two different timeframes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "jjxzeONSxz3p"
      },
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "plt.plot(length_scores)\n",
        "plt.plot(length_scores2)\n",
        "plt.xlabel('Window length')\n",
        "plt.ylabel('Score')\n",
        "plt.legend(['2012-2014', '2014-2016'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "fT24WGTgxz3p"
      },
      "source": [
        "To avoid overfitting, we can use economic reasoning or the nature of our algorithm to pick our window length. We can also use Kalman filters, which do not require us to specify a length; this method will be covered in another notebook later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "O7rjgpH5xz3q"
      },
      "source": [
        "## Avoiding overfitting\n",
        "\n",
        "We can try to avoid overfitting by taking large samples, choosing reasonable and simple models, and not cherry-picking parameters to fit the data; but just running two backtests is already overfitting.\n",
        "\n",
        "### Out of Sample Testing\n",
        "\n",
        "To make sure we haven't broken our model with overfitting, we have to test out of sample. That is, we need to gather data that we did not use in constructing the model, and test whether our model continues to work. If we cannot gather large amounts of additional data at will, we should split the sample we have into two parts, of which one is reserved for testing only.\n",
        "\n",
        "### Common Mistake: Abusing Out of Sample Data\n",
        "\n",
        "Sometimes people will construct a model on in-sample data, test on out-of-sample data, and conclude it doesn't work. They will then repeat this process until they find a model that works. This is still overfitting, as you have overfit the model to the out-of-sample data by using it many times, and when you actually test on true out-of-sample data your model will likely break down.\n",
        "\n",
        "### Cross Validation\n",
        "\n",
        "Cross validation is the process of splitting your data into n parts, then estimating optimal parameters for n-1 parts combined and testing on the final part. By doing this n times, one for each part held out, we can establish how stable our parameter estimates are and how predictive they are on data not from the original set.\n",
        "\n",
        "## Information Criterion\n",
        "\n",
        "Information criterion are a rigorous statistical way to test if the amount of complexity in your model is worth the extra predictive power. The test favors simpler models and will tell you if you are introducing a large amount of complexity without much return. One of the most common methods is [Akaike Information Criterion.](https://en.wikipedia.org/wiki/Akaike_information_criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "7A1s-Tnhxz3q"
      },
      "source": [
        "--- \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "hybT2Yp3xz3q"
      },
      "source": [
        "## Volume, Slippage, and Liquidity\n",
        "\n",
        "\n",
        "## What is Volume?\n",
        "Volume is how much trading occurs for a given instrument, or set of instruments, over a given time period. \n",
        "\n",
        "### Example Volume Computation\n",
        "For instance, consider a hypothetical equity asset $A$. If, over the course of a minute, $100,000$ shares of $A$ are bought, then the shares traded volume of $A$ is $100,000$. To find the dollar volume traded for $A$, which is the more often-used statistic, we need to take a weighted average of all the different prices $A$ traded for over the minute times the number of shares that traded at each price. This is equivalent to adding up the dollar volumes of all the individual trasnactions that occured. For instance, let's say in this case there were there separate transactions. One for 30,000 shares, one for $60,000$ shares, and one for $10,000$ shares. The prices were $30$ USD, $31$ USD, and $33$ USD, respectively. Let's model this out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "STIWO5Jjxz3q"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "cvChGiSjxz3r"
      },
      "source": [
        "num_shares = np.asarray([30000, 60000, 10000])\n",
        "prices = np.asarray([30, 31, 33])\n",
        "\n",
        "np.dot(num_shares, prices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "YzWfsE27xz3s"
      },
      "source": [
        "So total dollar volume is $3.09$ million USD. Notice that this is equivalent to taking the dollar volume averaged price and multiplying by the number of shares traded over that bar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "hED-lXtCxz3s"
      },
      "source": [
        "# Get the average trade price\n",
        "print \"Average trade price: %s\" % (np.mean(prices))\n",
        "\n",
        "# Get the volume weighted average\n",
        "vw = np.dot(num_shares, prices) / float(np.sum(num_shares))\n",
        "print \"Volume weighted average trade price: %s\" % (vw)\n",
        "\n",
        "# Go back to volume\n",
        "v = vw * np.sum(num_shares)\n",
        "print \"Volume: %s\" % (v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "DcJel_X1xz3t"
      },
      "source": [
        "Often in real datasets you will be given averaged or 'last traded' data rather than individual trades. With averaged data, some average is taken over a bar (time period). With last traded data, only the final observation from that bar is reported. It is important to know if the data is averaged, volume averaged, or simply last traded. All of these will need to be treated differently.\n",
        "\n",
        "For pricing data, Quantopian currently (as of April 2017) provides the last traded prices at a minute resolution. The volume is the sum of all volume in that bar. While we do not offer minutely volume-weighted price, daily volume-weighted price can be approximated from the minute bars.\n",
        "\n",
        "Let's look at some volume data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "8RGwR-gWxz3t"
      },
      "source": [
        "data = get_pricing('SPY', start_date='2015-6-1', end_date='2015-6-20', frequency='minute')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "QKo9lHaGxz3u"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "bTtRaTggxz3u"
      },
      "source": [
        "Notice that just plotting the volume gives us lots of gaps. This is because we are plotting data with time on the x-axis, and equities are only traded during part of the day. The rest of the day we have no data for volume. There is a small amount of overnight trading that occurs for equities, but it is often only useful as a data source as the liquidity is too low to trade. Quantopian does not currently have data on overnight trading, just for regular market hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "C60tc3sVxz3v"
      },
      "source": [
        "plt.plot(data['volume'])\n",
        "plt.ylabel('Volume (USD)');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Fo6Xq0rfxz3v"
      },
      "source": [
        "Let's look at how this volume is balanced out over a single day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "6w4ti-dKxz3w"
      },
      "source": [
        "plt.plot(data['volume']['2015-6-4'])\n",
        "plt.ylabel('Volume (USD)');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "QjAnAVASxz3w"
      },
      "source": [
        "This is pretty noisy, but with pandas we can easily take an average across all the days in our sample to get a sense of how volume fluctuates on average. Here is the average volume for a given minute over the whole time period."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "iZGJiVX7xz3x"
      },
      "source": [
        "avg_minute_volume = data.groupby([data.index.hour, data.index.minute]).mean()['volume']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "L-EAZ1qCxz3x"
      },
      "source": [
        "avg_minute_volume.plot()\n",
        "plt.ylabel('Volume (USD)');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "baWO_RQTxz3y"
      },
      "source": [
        "### High Equity Volume After Market Open\n",
        "There tends to be high volume right after market open. This is due to a variety of factors including people wanting to update their positions based on price changes and new information that emerged before trading started. Since there was a ton of time and possibly news overnight while the market could not react, trading right after the open can be a bit volatile as pricing adjusts.\n",
        "\n",
        "### High Equity Volume Before Market Close\n",
        "There is a similar effect at the end of the day when people want to get in their final orders before trading closes and they no longer have the chance. Again there are a variety of factors that drive this increase, but the effect tends to be consistent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "f8CXxNDGxz3y"
      },
      "source": [
        "### Modeling Volume\n",
        "Volume can change a lot, and summary statistics such as mean volume may not always be representative. Model it as you would any other quantity and pay attention to whether your volume forecasts or measurements are useful. We do not discuss any specific models in this lecture as the models are often specific to the markets and instruments being traded.\n",
        "\n",
        "As with any other model, a model of volume needs to be carefully validated using rigorous statistics. As a rule of thumb you can expect more volume at the start and end of day in the US Equity market, but this will vary by equity and day, plus there will be shocks and half days. In practice you should apply just as much care as you would trying to model returns.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "0n7KsFlMxz3z"
      },
      "source": [
        "## What is Slippage?\n",
        "Slippage occurs when large buy or sell orders drive the price of the market up or down respectively. Remember that for every share bought, a share must be sold by another party. If no parties are selling shares, buy orders will not be filled. This is a simplification of what actually occurs as there are complexities around issues such as bid/ask spread, but we will not consider those here. To understand why price shifts happen when large orders are placed, consider the following scenario:\n",
        "\n",
        "1. Person A places a buy order for $1000$ shares of stock X. Currently trading at $100$ USD / share.\n",
        "2. There is only one sell order for $500$ shares.\n",
        "3. The broker fills $500$ shares of the order at $100$ USD / share.\n",
        "3. There are $500$ shares remaining to fill. Other market participants notice that there is demand at $100$ USD / share, so they increase their sell price to $101$ USD / share.\n",
        "4. Another sell order gets placed for $500$ shares at $101$ USD / share.\n",
        "5. The broker fills the remaining $500$ shares of the original order at $101$ USD / share. Closing out the process.\n",
        "\n",
        "As you can see, the large order for stock X was not immediately filled, and the demand pushed the market price against the original order, resulting in a fill price worse than expected. The opposite occurs for sell orders. In general, the larger the order, the more slippage will be experienced by a trade.\n",
        "\n",
        "### Complications with Forecasting Slippage\n",
        "Modeling slippage is not trivial and is the subject of research by professional quantitative teams. There are different parameters and the behavior of slippage varies across markets and instruments. Things like the bid/ask spread will influence it. Although we will not get into any specific slippage models in this lecture, info on Quantopian's backtester slippage model can be found [here](https://www.quantopian.com/help#ide-slippage) and some more in-depth research on it can be found [here](https://blog.quantopian.com/accurate-slippage-model-comparing-real-simulated-transaction-costs/).\n",
        "\n",
        "### Why is this Important to Quantitative Strategies?\n",
        "Quantitative strategies often rely on finding small but consistent edges across many instruments in a market or multiple markets. If the amount of profit you expect to make off of each trade is very small, slippage can eat a significant amount of that profit. Quantitative strategies rely on executing orders as they have been defined, so anything that counteracts the orders can break the strategy.\n",
        "\n",
        "### Where is Slippage Worst?\n",
        "Slippage is often worst on stocks with very little volume. Some egregious offenders are small and micro cap stocks. They are very low volume and develop large inefficiencies, but when you try to trade the inefficiencies you are unable to due to the lack of trades on the other side of the market. If you wait long enough you may be able to fill your order, but in that time you will suffer a lot of slippage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "78upWypwxz3z"
      },
      "source": [
        "## What is Liquidity?\n",
        "Before we can define liquidity, we need to define a few key execution parameters. This is certainly not an exhaustive list of parameters that can decribe execution conditions, but they are the key ones for our purposes here.\n",
        "\n",
        "### Execution Parameters\n",
        "* Trade Size $s$\n",
        "* Market Price $p_m$\n",
        "* Time to Fill $t$\n",
        "* Realized Fill Price $p_f$\n",
        "\n",
        "### Defining Liquidity in Terms of Execution Feasibility\n",
        "Liquidity is a somewhat ambiguous term. There are different ways of defining it and therefore different ways of measuring it. In general it's the ability to execute large trades quickly without experiencing slippage. Another way to think about it is how easy it is to convert a held asset to cash quickly. Rather than having one precise definition, people often descibe liquidity according to their trading needs. For instance, if one regularly needs to execute trades of size $s$=100,000USD, and market $M_1$ can regularly support that with fills coming within $t$=1s and slippage or $p_f / p_m$ < 1.0001 (0.01% error), then we'd say that market $M$ is sufficiently liquid for one's purposes. On the other hand if another market $M_2$ can support trades of larger size, fill them in shorter time, or has consistently less slippage, then we would say that $M_2$ is *more* liquid.\n",
        "\n",
        "The takeaway is that liquidity depends on what you are trying to trade and how, and once you've established a benchmark you can compare it to other potential instruments, markets, brokers, or trading methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "gGvs3k68xz3z"
      },
      "source": [
        "## Liquidity vs. Volume\n",
        "Liquidity and volume are not the same thing. Liquidity is the important property that affects how easily we can trade. Volume is a related but indirect proxy for liquidity. In general people use volume as one of the main ways to determine how liquid a trading environemnt will be. If there is high volume, it's likely you'll be able to execute a trade quickly and with minimal slippage. If you're a high percentage of the volume, then you're in trouble. In general the higher the volume, the lower your percentage and the better things will be.\n",
        "\n",
        "Although volume is central to liquidity, one can have cases of high volume with low liquidity.\n",
        "\n",
        "### One-Sided Market\n",
        "A market may have high volume, but with a lot of demand in one direction. This can occur during a panic event in which case many people want to sell. This can result in large bid-ask spreads and, while trades may be closing, they will be closing at prices far away from what you would like (AKA high slippage). The high slippage indicates that this is an illiquid market. It is also more difficult to determine a fair market price when spreads and volatility are high. Momentum strategies can have higher slippage due to the fact that you're jumping into the same side of the market as everybody else.\n",
        "\n",
        "### Non-Continuous Prices\n",
        "Sometimes there can be large block transactions occuring in a single instrument. Imagine that a large shareholder is trying to get rid of their stake in a company. Rather than selling it on the open market and exposing themselves to a massive amount of slippage, they trade it through other routes. There will still be a large amount of trading volume, but because it's not accessible to you, any trades you place will be exposed to the same illiquid conditions as without that volume."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "QIs2bUT8xz3z"
      },
      "source": [
        "## Transaction Costs\n",
        "Whenever a trade is placed, you will pay a fee to the broker to trade. This fee can be based on trade size or just be flat, but is well defined and not an uncertain quantity. You will also experience slippage, which can be thought of as just another cost. Often both broker fees and slippage costs will be grouped under the term 'Transaction Costs', which describes generally how much it will cost to trade a specific strategy. If transaction costs eat too much of your profits a strategy might become infeasible.\n",
        "\n",
        "### Transaction Costs Vary by Broker and Market\n",
        "Because transaction costs vary based on trading environments, strategies that are inviable on a retail brokerage may be viable on an institutional prime brokerage with lower costs. Often quant strategies will fail on retail fees, but be fine for an institution. Just because you can't trade a strategy doesn't mean it's not valuable. You may be able to sell it to an organization that can trade it. When evaluating strategies for allocation Quantopian assumes trading conditions of a prime brokerage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "LEEiUwXCxz3z"
      },
      "source": [
        "## Why is liquidity important?\n",
        "Why do we actually care about modeling, forecasting, and understanding liquidity?\n",
        "\n",
        "### It's a bottleneck to trading any strategy.\n",
        "The final step of implementing any trading strategy is actually trading it. If you have statistically verified a mathematical model and it's pumping out target positions, that's not useful unless you can turn those positions into reality by trading in the market. Liquidity essentially defines your ability to move from current positions to desired future positions. With low liquidity you will not be able to execute on desired trades and there will be large infidelities between your model's requests and your real positions.\n",
        "\n",
        "#### There's no perfect case.\n",
        "You cannot completely eliminate liquidity concerns. Each instrument will have variable liquidity and there may be exogenous system shocks. As with any other quantity you can just model it and then try to trade instruments for which you expect good liquidity. Every strategy will have some liquidity drag during actual trading, but you can just try to make it as small as possible by avoiding instruments with high liquidity risk.\n",
        "\n",
        "### Capital Base vs. Liquidity\n",
        "The more capital you trade, the harder it is to fill trades without experiencing slippage, so the more expensive your trading will be. In general all strategies have capital ranges in which they are profitable. The upper limit is defined by liquidity constraints. Once you become too large a part of the market the slippage will eat all your returns. The lower end is defined by broker comissions and position errors. If you're attempting to trade a small amount of capital, flat trading costs from your broker may exceed returns. Likewise, if you are trading 100,000USD and place 1% of your portfolio into a single stock, you will not be able to buy precisely 100USD worth of the stock. The lack of fractional shares will result in large portfolio infidelities from your desired positions. Because trading few instruments results in [position concentration risk](https://www.quantopian.com/lectures/position-concentration-risk), trading small amounts of capital is often difficult."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "POtbb9rdxz3z"
      },
      "source": [
        "## Backtesting and Liquidity\n",
        "The purpose of backtesting is not to determine whether a model has predictive power. That should all be done at the [statistical analysis stage](https://www.quantopian.com/lectures/hypothesis-testing) and is inefficient to do in a backtester, not to mention the [overfitting risk](https://www.quantopian.com/lectures/the-dangers-of-overfitting).\n",
        "\n",
        "The purpose of backtesting is to check how robust a strategy is to real market frictions like illiquidity, slippage, and the resulting transaction costs. You can have highly predictive models that are simply not tradeable because the underlying instruments are not liquid enough.\n",
        "\n",
        "### Backtest Under a Range of Liquidity Assumptions\n",
        "It pays to backtest under a few different liquidity assumptions. This tells you whether your strategy is robust, or whether it will fall apart the moment you change liquidity conditions at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Kxjy-9r4xz3z"
      },
      "source": [
        "## Liquidity Varies by Market, Instrument, and Broker\n",
        "In general different instruments will experience broadly different liquidity conditions. We'll show an example of that here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "B2O4jfC7xz30"
      },
      "source": [
        "start_date = '2016-04-01'\n",
        "end_date = '2016-06-14'\n",
        "aapl_volume = get_pricing('AAPL', start_date=start_date, end_date=end_date, fields='volume')\n",
        "spy_future_volume = get_pricing('ESM16', start_date=start_date, end_date=end_date, fields='volume')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "SX6-LADXxz30"
      },
      "source": [
        "aapl_volume.plot()\n",
        "spy_future_volume.plot()\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "YRBrz6_fxz31"
      },
      "source": [
        "### Broad Liquidity Considerations in Equities\n",
        "* Liquidity will vary by market cap. Higher cap stocks experience more absolute trading volume and are more liquid. Micro-cap stocks are \n",
        "* Some stocks are hard to borrow/short, and may experience one sided liquidity. Longs are easy to purchase, but shorts are hard to obtain. Quantopian does not currently provide hard to borrow data.\n",
        "\n",
        "### Liquidity in Futures\n",
        "* Futures have much higher liquidity per instrument compared to equities.\n",
        "* Liquidity varies by type/class of future. Certain types of futures will be far less illiquid. We'll show an example here.\n",
        "* Liquidity has structure depending on expiration date. The volume of trading of a future contract will increase over time up to the expiration date. Far in advance there will be little volume as most people will be trading more current contracts. This volume effect must be considered when trying to trade futures. We'll show an example of this here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "Knyeq7IIxz31"
      },
      "source": [
        "crude_future_volume = get_pricing('CLM16', start_date=start_date, end_date=end_date, fields='volume')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "qomFx4Q6xz32"
      },
      "source": [
        "spy_future_volume.plot()\n",
        "crude_future_volume.plot()\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "TWKuOrkcxz33"
      },
      "source": [
        "palladium_future_volume = get_pricing('PAM16', start_date=start_date, end_date=end_date, fields='volume')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "suXfc5d3xz33"
      },
      "source": [
        "crude_future_volume.plot()\n",
        "palladium_future_volume.plot()\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "8ATPPUNPxz34"
      },
      "source": [
        "## Avoiding Illiquidity\n",
        "### Use a good universe.\n",
        "\n",
        "Quantopian has defined a universe (QTradableStocksUS) of liquid stocks in an effort to provide a nice clean starting point for people looking to develop models. You can find more about that here:\n",
        "\n",
        "https://www.quantopian.com/lectures/universe-selection\n",
        "\n",
        "### Execution Algorithms\n",
        "\n",
        "Algorithms exist that try to time orders and parcel them out into smaller chunks across many markets or timeframes. These algorithms are collectively known as execution algorithms and aim to reduce issues with slippage and liquidity. We will not go into execution algorithms in this lecture, but they are the subject of a huge amount of industry and academic research. Often firms will have a quantitative research team dedicated to developing strategies, and another dedicated to researching ways to execute the trades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "vXfS5-ssxz34"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "BW6I4jP8xz34"
      },
      "source": [
        "## Market Impact "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "1C3x5bg3xz35"
      },
      "source": [
        "non of us have enough money for this\n",
        "but if you have, here you [go!](https://github.com/quantopian/research_public/blob/master/notebooks/lectures/Market_Impact_Model/notebook.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "ZibjDJumxz35"
      },
      "source": [
        "# Biases: Overfitting, Look-Ahead and Survivorship "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "6Z6EvlrHxz35"
      },
      "source": [
        "## Survivorship Bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "JOWx8NlExz35"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Iyrzy18yxz36"
      },
      "source": [
        "<a ><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/98/Survivorship-bias.png\" Width=\"700\" align=\"center\"> </a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Hxqsy4Q9xz36"
      },
      "source": [
        "\n",
        "In order to evaluate a population based on sample observations, the sample must be unbiased. Otherwise, it is not representative of the population, and conclusions drawn from it will be invalid. For example, we always take into account the number of samples we have, and expect to have more accurate results if we have more observations. Here we will discuss four other types of sampling bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "x5opw4oKxz37"
      },
      "source": [
        "## Data-mining bias\n",
        "\n",
        "Data mining refers to testing a set of data for the presense of different patterns, and can lead to bias if used excessively. Because our analyses are always probabilistic, we can always try enough things that one will appear to work. For instance, if we test 100 different variables for correlation with a dataset using a 5% significance level, we expect to find 5 that are significantly correlated with the data just by random chance. Below we test this for random variables and a random dataset. The result will be different each time, so try rerunning the cell!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "Z_7_6AWAxz37"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Generate completely random numbers\n",
        "randos = [np.random.rand(100) for i in range(100)]\n",
        "y = np.random.rand(100)\n",
        "\n",
        "# Compute correlation coefficients (Pearson r) and record their p-values (2nd value returned by pearsonr)\n",
        "ps = [pearsonr(x,y)[1] for x in randos]\n",
        "\n",
        "# Print the p-values of the significant correlations, i.e. those that are less than .05\n",
        "print [p for p in ps if p < .05]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "x1OICzAFxz38"
      },
      "source": [
        "Above we data-mined by hand. There is also intergeneratinal data mining, which is using previous results about the same dataset you are investigating or a highly related one, which can also lead to bias.\n",
        "\n",
        "The problem here is that there is no reason to believe that the pattern we found will continue; for instance, if we continue to generate random numbers above, they will not continue to be correlated. [Meaningless correlations](http://tylervigen.com/view_correlation?id=866) can arise between datasets by coincidence. This is similar to the problem of overfitting, where a model is contorted to fit historical data perfectly but then fails out of sample. It is important to perform such an out-of-sample test (that is, using data not overlapping with that which was examined when creating the model) in order to check for data-mining bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "EKj7beubxz38"
      },
      "source": [
        "## Sample selection bias\n",
        "\n",
        "Bias resulting from data availability is called sample selection bias. Sometimes it is impossible to avoid, but awareness of the phenomenon can help avoid incorrect conclusions. Survivorship bias occurs when securities dropped from databases are not taken into account. This causes a bias because future analyses then do not take into account, for example, stocks of businesses that went bankrupt. However, businesses whose stock you buy now may very well go bankrupt in the future, so it is important to incorporate the behavior of such stocks into your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ofPWsDLjxz38"
      },
      "source": [
        "## Look-ahead bias\n",
        "\n",
        "Look-ahead bias occurs when attempting to analyze from the perspective of a day in the past and using information that was not available on that day. For instance, fundamentals data may not be reported in real time. Models subject to look-ahead bias cannot be used in practice since they would require information from the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "l9LiYiebxz38"
      },
      "source": [
        "## Time-period bias\n",
        "\n",
        "The choice of sample period affects results, and a model or analysis may not generalize to future periods. This is known as time-period bias. If we use only a short time period, we risk putting a lot of weight on a local phenomenon. However, if we use a long time period, we may include data from a prior regime that is no longer relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "4_cU6gY8xz38"
      },
      "source": [
        "--- \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "wKNxp-oOxz38"
      },
      "source": [
        "# Performance: Returns, Risk and Drawdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "06ZvGan1xz38"
      },
      "source": [
        "## How to compute $\\alpha$ and $\\beta$ for a given asset\n",
        "By Evgenia \"Jenny\" Nitishinskaya and Delaney Granizo-Mackenzie\n",
        "\n",
        "Notebook released under the Creative Commons Attribution 4.0 License.\n",
        "\n",
        "---\n",
        "\n",
        "A fundamental concept in portfolio management and risk allocation is computing the $\\beta$ of an asset. $\\beta$ is a measurement of the covariance of the asset with the market in general, and is expressed in this model.\n",
        "\n",
        "$$r_a \\approx \\alpha + \\beta r_b$$\n",
        "\n",
        "$r_a$ are the returns of the asset, and $r_b$ are the returns of the benchmark, usually a proxy for the market like the S&P 500.\n",
        "\n",
        "$\\beta$ can be defined as\n",
        "\n",
        "$$\\beta = \\frac{Cov(r_a, r_b)}{Var(r_b)}$$\n",
        "\n",
        "To actually compute $\\beta$, we can use linear regression. We find the OLS best fit line for all points $(r_{a, t}, r_{b, t})$, where $r_{a, t}$ is the asset's returns for time $t$, and $r_{b, t}$ the same for the benchmark. The slope of this line is $\\beta$, and the y-intercept is $\\alpha$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "AvRQbeO8xz38"
      },
      "source": [
        "We'll start by getting data for a specific time range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "lpGj5Uyuxz39"
      },
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "from statsmodels import regression\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "tWJrYu0Fxz39"
      },
      "source": [
        "# Get data for the specified period and stocks\n",
        "start = '2014-01-01'\n",
        "end = '2015-01-01'\n",
        "asset = get_pricing('TSLA', fields='price', start_date=start, end_date=end)\n",
        "benchmark = get_pricing('SPY', fields='price', start_date=start, end_date=end)\n",
        "\n",
        "# We have to take the percent changes to get to returns\n",
        "# Get rid of the first (0th) element because it is NAN\n",
        "r_a = asset.pct_change()[1:]\n",
        "r_b = benchmark.pct_change()[1:]\n",
        "\n",
        "# Let's plot them just for fun\n",
        "r_a.plot()\n",
        "r_b.plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Nm7N33J1xz3-"
      },
      "source": [
        "Now we have to fit a line to the data to determine the slope. We use Ordinary Least Squares (OLS) for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "KtHZ4ogCxz3-"
      },
      "source": [
        "# Let's define everything in familiar regression terms\n",
        "X = r_b.values # Get just the values, ignore the timestamps\n",
        "Y = r_a.values\n",
        "\n",
        "# We add a constant so that we can also fit an intercept (alpha) to the model\n",
        "# This just adds a column of 1s to our data\n",
        "X = sm.add_constant(X)\n",
        "model = regression.linear_model.OLS(Y, X)\n",
        "model = model.fit()\n",
        "# Remove the constant now that we're done\n",
        "X = X[:, 1]\n",
        "alpha = model.params[0]\n",
        "beta = model.params[1]\n",
        "print 'alpha: ' + str(alpha)\n",
        "print 'beta: ' + str(beta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "1ZuLKHUAxz3_"
      },
      "source": [
        "We can plot the line of best fit to visualize this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "Vdlrh_hoxz3_"
      },
      "source": [
        "X2 = np.linspace(X.min(), X.max(), 100)\n",
        "Y_hat = X2 * beta + alpha\n",
        "\n",
        "plt.scatter(X, Y, alpha=0.3) # Plot the raw data\n",
        "plt.xlabel(\"r_b\")\n",
        "plt.ylabel(\"r_a\")\n",
        "\n",
        "# Add the regression line, colored in red\n",
        "plt.plot(X2, Y_hat, 'r', alpha=0.9);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "48cvH63pxz4A"
      },
      "source": [
        "## How to compute the volatility for a given asset\n",
        "\n",
        "The volatility $\\sigma$ of an asset is the standard deviation of its returns. A low volatility means that the returns are generally close to the mean, while a high volatility corresponds to returns that are often much higher and often much lower than expected.\n",
        "\n",
        "We'll go ahead and continue using the stock from before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "tY7qO2utxz4A"
      },
      "source": [
        "# Use numpy to find the standard deviation of the returns\n",
        "SD = np.std(Y)\n",
        "print SD\n",
        "\n",
        "# Let's compute the volatility for our benchmark, as well\n",
        "benchSD = np.std(X)\n",
        "print benchSD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "dLy5XaCIxz4B"
      },
      "source": [
        "This gives the daily volatility. As expected, the benchmark has a much lower volatility than the stock - a volatile asset would not make a good benchmark.\n",
        "\n",
        "We generally compute the annualized volatility so that we can compare volatilities for daily, weekly, or monthly samples. To get it we normalize the standard deviation of the daily returns by multiplying by the square root of the number of trading days in a year:\n",
        "\n",
        "$$\\sigma_{\\text{annual}} = SD \\cdot \\sqrt{252}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "TnGbxHn8xz4B"
      },
      "source": [
        "vol = SD*(252**.5)\n",
        "print vol\n",
        "\n",
        "benchvol = benchSD*(252**.5)\n",
        "print benchvol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "2iE7p5uNxz4C"
      },
      "source": [
        "This tells us that we should expect the returns of a benchmark to cluster more closely around their mean than those of the stock. We can plot histograms of the returns to see this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "Hg3AdGzMxz4C"
      },
      "source": [
        "# Since we have many distinct values, we'll lump them into buckets of size .02\n",
        "x_min = int(math.floor(X.min()))\n",
        "x_max = int(math.ceil(X.max()))\n",
        "plt.hist(X, bins=[0.02*i for i in range(x_min*50, x_max*50)], alpha=0.5, label='S&P')\n",
        "plt.hist(Y, bins=[0.02*i for i in range(x_min*50, x_max*50)], alpha=0.5, label='Stock')\n",
        "plt.legend(loc='upper right');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "3RDy2rT9xz4D"
      },
      "source": [
        "## How to compute the Sharpe and information ratios for an asset\n",
        "\n",
        "The Sharpe and information ratios are used to calculate how well the historic returns of an asset compensate for its risk, relative to some benchmark or risk-free asset. An asset with a higher ratio has either higher returns, lower risk, or both. As when computing volatility, the standard deviation of the returns is used to measure risk.\n",
        "\n",
        "$$R = \\frac{E[r_a - r_b]}{\\sqrt{Var(r_a - r_b)}}$$\n",
        "\n",
        "$r_a$ are the returns of the asset, and $r_b$ are the returns of the benchmark; generally, Treasury bills are used when computing the Sharpe ratio, while the S&P 500 index is commonly used for the information ratio. We subtract the returns of the benchmark from the returns of the asset becasue we would like to get higher returns through our investment than we would, say, simply buying Treasury bills."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "jM21o6rAxz4D"
      },
      "source": [
        "# Get the returns for a treasury-tracking ETF to be used in the Sharpe ratio\n",
        "# Note that BIL is only being used in place of risk free rate,\n",
        "# and should not be used in such a fashion for strategy development\n",
        "riskfree = get_pricing('BIL', fields='price', start_date=start, end_date=end)\n",
        "r_b_S = riskfree.pct_change()[1:]\n",
        "X_S = r_b_S.values\n",
        "\n",
        "# Compute the Sharpe ratio for the asset we've been working with\n",
        "SR = np.mean(Y - X_S)/np.std(Y - X_S)\n",
        "\n",
        "# Compute the information ratio for the asset we've been working with, using the S&P index\n",
        "IR = np.mean(Y - X)/np.std(Y - X)\n",
        "\n",
        "# Print results\n",
        "print 'Sharpe ratio: ' + str(SR)\n",
        "print 'Information ratio: ' + str(IR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "t5eHrnPzxz4E"
      },
      "source": [
        "## How to compute the Sortino ratio for an asset\n",
        "\n",
        "The Sharpe and information ratios are useful, but they penalize stocks for going above the expected return as well as for going below it. The Sortino ratio is modified to take into account only returns that fall below the mean.\n",
        "\n",
        "$$S = \\frac{E[r_a - r_b]}{\\sqrt{Semivar(r_a - r_b)}}$$\n",
        "\n",
        "The semivariance is the variance below the mean, and so quantifies the downside risk of our asset. Here as in the Sharpe ratio returns on Treasury bills can be used for $r_b$. The more skewed the distribution of returns is, the more the Sortino ratio will differ from the Sharpe ratio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "4j_KzVwTxz4E"
      },
      "source": [
        "# To compute the semideviation, we want to filter out values which fall above the mean\n",
        "meandif = np.mean(Y - X_S)\n",
        "lows = [e for e in Y - X_S if e <= meandif]\n",
        "\n",
        "# Because there is no built-in semideviation, we'll compute it ourselves\n",
        "def dist(x):\n",
        "    return (x - meandif)**2\n",
        "semidev = math.sqrt(sum(map(dist,lows))/len(lows))\n",
        "\n",
        "Sortino = meandif/semidev\n",
        "print 'Sortino ratio: ' + str(Sortino)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "2TzzHQytxz4F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "d5HuNW1Sxz4F"
      },
      "source": [
        "--- \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "uD--fwKixz4G"
      },
      "source": [
        "## Beta Hedging\n",
        "By Evgenia \"Jenny\" Nitishinskaya and Delaney Granizo-Mackenzie with example algorithms by David Edwards\n",
        "\n",
        "Part of the Quantopian Lecture Series:\n",
        "\n",
        "* [www.quantopian.com/lectures](https://www.quantopian.com/lectures)\n",
        "* [github.com/quantopian/research_public](https://github.com/quantopian/research_public)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "##Factor Models\n",
        "\n",
        "Factor models are a way of explaining the returns of one asset via a linear combination of the returns of other assets. The general form of a factor model is\n",
        "\n",
        "$$Y = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n$$\n",
        "\n",
        "This looks familiar, as it is exactly the model type that a linear regression fits. The $X$'s can also be indicators rather than assets. An example might be a analyst estimation.\n",
        "\n",
        "##What is Beta?\n",
        "\n",
        "An asset's beta to another asset is just the $\\beta$ from the above model. For instance, if we regressed TSLA against the S&P 500 using the model $Y_{TSLA} = \\alpha + \\beta X$, then TSLA's beta exposure to the S&P 500 would be that beta. If we used the model $Y_{TSLA} = \\alpha + \\beta X_{SPY} + \\beta X_{AAPL}$, then we now have two betas, one is TSLA's exposure to the S&P 500 and one is TSLA's exposure to AAPL.\n",
        "\n",
        "Often \"beta\" will refer to a stock's beta exposure to the S&P 500. We will use it to mean that unless otherwise specified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "o04uD0JSxz4G"
      },
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "from statsmodels import regression\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "H6IQTz-Pxz4G"
      },
      "source": [
        "# Get data for the specified period and stocks\n",
        "start = '2014-01-01'\n",
        "end = '2015-01-01'\n",
        "asset = get_pricing('TSLA', fields='price', start_date=start, end_date=end)\n",
        "benchmark = get_pricing('SPY', fields='price', start_date=start, end_date=end)\n",
        "\n",
        "# We have to take the percent changes to get to returns\n",
        "# Get rid of the first (0th) element because it is NAN\n",
        "r_a = asset.pct_change()[1:]\n",
        "r_b = benchmark.pct_change()[1:]\n",
        "\n",
        "# Let's plot them just for fun\n",
        "r_a.plot()\n",
        "r_b.plot()\n",
        "plt.ylabel(\"Daily Return\")\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "FdCMP3tKxz4H"
      },
      "source": [
        "Now we can perform the regression to find $\\alpha$ and $\\beta$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "7RrQLC6Ixz4H"
      },
      "source": [
        "# Let's define everything in familiar regression terms\n",
        "X = r_b.values # Get just the values, ignore the timestamps\n",
        "Y = r_a.values\n",
        "\n",
        "def linreg(x,y):\n",
        "    # We add a constant so that we can also fit an intercept (alpha) to the model\n",
        "    # This just adds a column of 1s to our data\n",
        "    x = sm.add_constant(x)\n",
        "    model = regression.linear_model.OLS(y,x).fit()\n",
        "    # Remove the constant now that we're done\n",
        "    x = x[:, 1]\n",
        "    return model.params[0], model.params[1]\n",
        "\n",
        "alpha, beta = linreg(X,Y)\n",
        "print 'alpha: ' + str(alpha)\n",
        "print 'beta: ' + str(beta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "bEoAhCXoxz4I"
      },
      "source": [
        "If we plot the line $\\alpha + \\beta X$, we can see that it does indeed look like the line of best fit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "9wcEs3m7xz4I"
      },
      "source": [
        "X2 = np.linspace(X.min(), X.max(), 100)\n",
        "Y_hat = X2 * beta + alpha\n",
        "\n",
        "plt.scatter(X, Y, alpha=0.3) # Plot the raw data\n",
        "plt.xlabel(\"SPY Daily Return\")\n",
        "plt.ylabel(\"TSLA Daily Return\")\n",
        "\n",
        " # Add the regression line, colored in red\n",
        "plt.plot(X2, Y_hat, 'r', alpha=0.9);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "2mcfAsIrxz4J"
      },
      "source": [
        "##Risk Exposure\n",
        "\n",
        "More generally, this beta gets at the concept of how much risk exposure you take on by holding an asset. If an asset has a high beta exposure to the S&P 500, then while it will do very well while the market is rising, it will do very poorly when the market falls. A high beta corresponds to high speculative risk. You are taking out a more volatile bet.\n",
        "\n",
        "At Quantopian, we value stratgies that have negligible beta exposure to as many factors as possible. What this means is that all of the returns in a strategy lie in the $\\alpha$ portion of the model, and are independent of other factors. This is highly desirable, as it means that the strategy is agnostic to market conditions. It will make money equally well in a crash as it will during a bull market. These strategies are the most attractive to individuals with huge cash pools such as endowments and soverign wealth funds.\n",
        "\n",
        "##Risk Management\n",
        "\n",
        "The process of reducing exposure to other factors is known as risk management. Hedging is one of the best ways to perform risk management in practice.\n",
        "\n",
        "###Hedging\n",
        "\n",
        "If we determine that our portfolio's returns are dependent on the market via this relation\n",
        "\n",
        "$$Y_{portfolio} = \\alpha + \\beta X_{SPY}$$\n",
        "\n",
        "then we can take out a short position in SPY to try to cancel out this risk. The amount we take out is $-\\beta V$ where $V$ is the total value of our portfolio. This works because if our returns are approximated by $\\alpha + \\beta X_{SPY}$, then adding a short in SPY will make our new returns be $\\alpha + \\beta X_{SPY} - \\beta X_{SPY} = \\alpha$. Our returns are now purely alpha, which is independent of SPY and will suffer no risk exposure to the market.\n",
        "\n",
        "###Market Neutral\n",
        "\n",
        "When a stragy exhibits a consistent beta of 0, we say that this strategy is market neutral.\n",
        "\n",
        "###Problems with Estimation\n",
        "\n",
        "The problem here is that the beta we estimated is not necessarily going to stay the same as we walk forward in time. As such the amount of short we took out in the SPY may not perfectly hedge our portfolio, and in practice it is quite difficult to reduce beta by a significant amount.\n",
        "\n",
        "We will talk more about problems with estimating parameters in future lectures. In short, each estimate has a stardard error that corresponds with how stable the estimate is within the observed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "vBeaOvlcxz4J"
      },
      "source": [
        "## Implementing hedging\n",
        "\n",
        "Now that we know how much to hedge, let's see how it affects our returns. We will build our portfolio using the asset and the benchmark, weighing the benchmark by $-\\beta$ (negative since we are short in it)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "_dO2MLMqxz4K"
      },
      "source": [
        "# Construct a portfolio with beta hedging\n",
        "portfolio = -1*beta*r_b + r_a\n",
        "portfolio.name = \"TSLA + Hedge\"\n",
        "\n",
        "# Plot the returns of the portfolio as well as the asset by itself\n",
        "portfolio.plot(alpha=0.9)\n",
        "r_b.plot(alpha=0.5);\n",
        "r_a.plot(alpha=0.5);\n",
        "plt.ylabel(\"Daily Return\")\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ZhS5cSSVxz4K"
      },
      "source": [
        "It looks like the portfolio return follows the asset alone fairly closely. We can quantify the difference in their performances by computing the mean returns and the volatilities (standard deviations of returns) for both:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "TVs_PLYbxz4K"
      },
      "source": [
        "print \"means: \", portfolio.mean(), r_a.mean()\n",
        "print \"volatilities: \", portfolio.std(), r_a.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "YkX5EWJWxz4L"
      },
      "source": [
        "We've decreased volatility at the expense of some returns. Let's check that the alpha is the same as before, while the beta has been eliminated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "6qk_WEr5xz4L"
      },
      "source": [
        "P = portfolio.values\n",
        "alpha, beta = linreg(X,P)\n",
        "print 'alpha: ' + str(alpha)\n",
        "print 'beta: ' + str(beta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "hP_IFHLAxz4M"
      },
      "source": [
        "Note that we developed our hedging strategy using historical data. We can check that it is still valid out of sample by checking the alpha and beta values of the asset and the hedged portfolio in a different time frame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "PW9MiaE4xz4M"
      },
      "source": [
        "# Get the alpha and beta estimates over the last year\n",
        "start = '2014-01-01'\n",
        "end = '2015-01-01'\n",
        "asset = get_pricing('TSLA', fields='price', start_date=start, end_date=end)\n",
        "benchmark = get_pricing('SPY', fields='price', start_date=start, end_date=end)\n",
        "r_a = asset.pct_change()[1:]\n",
        "r_b = benchmark.pct_change()[1:]\n",
        "X = r_b.values\n",
        "Y = r_a.values\n",
        "historical_alpha, historical_beta = linreg(X,Y)\n",
        "print 'Asset Historical Estimate:'\n",
        "print 'alpha: ' + str(historical_alpha)\n",
        "print 'beta: ' + str(historical_beta)\n",
        "\n",
        "# Get data for a different time frame:\n",
        "start = '2015-01-01'\n",
        "end = '2015-06-01'\n",
        "asset = get_pricing('TSLA', fields='price', start_date=start, end_date=end)\n",
        "benchmark = get_pricing('SPY', fields='price', start_date=start, end_date=end)\n",
        "\n",
        "# Repeat the process from before to compute alpha and beta for the asset\n",
        "r_a = asset.pct_change()[1:]\n",
        "r_b = benchmark.pct_change()[1:]\n",
        "X = r_b.values\n",
        "Y = r_a.values\n",
        "alpha, beta = linreg(X,Y)\n",
        "print 'Asset Out of Sample Estimate:'\n",
        "print 'alpha: ' + str(alpha)\n",
        "print 'beta: ' + str(beta)\n",
        "\n",
        "# Create hedged portfolio and compute alpha and beta\n",
        "portfolio = -1*historical_beta*r_b + r_a\n",
        "P = portfolio.values\n",
        "alpha, beta = linreg(X,P)\n",
        "print 'Portfolio Out of Sample:'\n",
        "print 'alpha: ' + str(alpha)\n",
        "print 'beta: ' + str(beta)\n",
        "\n",
        "\n",
        "# Plot the returns of the portfolio as well as the asset by itself\n",
        "portfolio.name = \"TSLA + Hedge\"\n",
        "portfolio.plot(alpha=0.9)\n",
        "r_a.plot(alpha=0.5);\n",
        "r_b.plot(alpha=0.5)\n",
        "plt.ylabel(\"Daily Return\")\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "NyTDtcMjxz4N"
      },
      "source": [
        "As we can see, the beta estimate changes a good deal when we look at the out of sample estimate. The beta that we computed over our historical data doesn't do a great job at reducing the beta of our portfolio, but does manage to reduce the magnitude by about 1/2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "sN-0k81uxz4N"
      },
      "source": [
        "##The alpha/beta Tradeoff\n",
        "\n",
        "Hedging against a benchmark such as the market will indeed reduce your returns while the market is not doing poorly. This is, however, completely fine. If your algorithm is less volatile, you will be able to take out leverage on your strategy and multiply your returns back up to their original amount. Even better, your returns will be far more stable than the original volatile beta exposed strategy.\n",
        "\n",
        "By and large, even though high-beta strategies tend to be deceptively attractive due to their extremely good returns during periods of market growth, they fail in the long term as they will suffer extreme losses during a downturn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "qGorL0fpxz4N"
      },
      "source": [
        "## Other types of hedging\n",
        "\n",
        "Although we will not execute them here, there are strategies for hedging that may be better suited for other investment approaches.\n",
        "\n",
        "##Pairs Trading\n",
        "\n",
        "One is pairs trading, in which a second asset is used in place of the benchmark here. This would allow you, for instance, to cancel out the volatility in an industry by being long in the stock of one company and short in the stock of another company in the same industry.\n",
        "\n",
        "[www.quantopian.com/lectures](https://www.quantopian.com/lectures)\n",
        "\n",
        "##Long Short Equity\n",
        "\n",
        "In this case we define a ranking over a group of $n$ equities, then long the top $p\\%$ and short the bottom $p\\%$ in equal dollar volume. This has the advantage of being implicitly, versus explicitly, hedged when $n$ is large. To see why this is the case, imagine buying a set of 100 securities randomly. The chance that the market exposure beta of these 100 is far from 1.0 is very low, as we have taken a large sample of the market. Similarly, when we rank by some independent metric and buy the top 100, the chance that we select securities whose overall beta is far from 1.0 is low. So in selecting 100 long and 100 short, the strategy beta should be very close to 1 - 1 = 0. Obviously some ranking systems will introduce a sample bias and break this assumption, for example ranking by the estimated beta of the equity.\n",
        "\n",
        "Another advantage of long short equity strategies is that you are making a bet on the ranking, or in other words the differential in performance between the top and bottom ranked equities. This means that you don't have to even worry about the alpha/beta tradeoff encountered in hedging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "kEeZ7EgXxz4N"
      },
      "source": [
        "## Why You Should Hedge Beta and Sector Exposures (Part I)\n",
        "by Jonathan Larkin and Maxwell Margenot\n",
        "\n",
        "Part of the Quantopian Lecture Series:\n",
        "\n",
        "* [www.quantopian.com/lectures](https://www.quantopian.com/lectures)\n",
        "* [github.com/quantopian/research_public](https://github.com/quantopian/research_public)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "DPA7RHEIxz4O"
      },
      "source": [
        "Whenever we have a trading strategy of any sort, we need to be considering the impact of systematic risk. There needs to be some risk involved in a strategy in order for there to be a return above the risk-free rate, but systematic risk poisons the well, so to speak. By its nature, systematic risk provides a commonality between the many securities in the market that cannot be diversified away. As such, we need to construct a hedge to get rid of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "i2aFZzD-xz4O"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.covariance import LedoitWolf\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "t3U0cviJxz4O"
      },
      "source": [
        "## The Fundamental Law of Asset Management\n",
        "\n",
        "The primary driver of the value of any strategy is whether or not it provides a compelling risk-adjusted return, i.e., the Sharpe Ratio. As expressed in [The Foundation of Algo Success](https://blog.quantopian.com/the-foundation-of-algo-success/) and \"The Fundamental Law of Active Management\", by Richard Grinold, Sharpe Ratio can be decomposed into two components, skill and breadth, as:\n",
        "\n",
        "$$IR = IC \\sqrt{BR}$$\n",
        "\n",
        "Technically, this is the definition of the Information Ratio (IR), but for our purposes it is equivalent to the Sharpe Ratio. The IR is the ratio of the excess return of a portfolio over its benchmark per unit active risk, i.e., the excess return of a long-only portfolio less its benchmark per unit tracking error. In the time of Grinoldâ€™s publication, however, long/short investing was a rarity. Today, in the world of hedge funds and long/short investing, there is no benchmark. We seek absolute returns so, in this case, the IR is equivalent to the Sharpe ratio.\n",
        "\n",
        "In this equation, skill is measured by IC (Information Coefficient), calculated with [Alphalens](https://github.com/quantopian/alphalens). The IC is essentially the Spearman rank correlation, used to correlate your prediction and its realization. Breadth is measured as the number of **independent** bets in the period. The takeaway from this \"law\" is that, with any strategy, we need to:\n",
        "\n",
        "1. Bet well (high IC),\n",
        "2. Bet often (high number of bets), *and*\n",
        "3. **Make independent bets**\n",
        "\n",
        "If the bets are completely independent, then breadth is the total number of bets we have made for every individual asset, the number of assets times the number of periods. If the bets are not independent then the **effective breadth** can be much much less than the number of assets. Let's see precisely what beta exposure and sector exposure do to **effective breadth**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "JcfWwdQ-xz4P"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "<b>TL;DR:</b> Beta exposure and sector exposure lead to a significant increase in correlation among bets. Portfolios with beta and sector bets have very low effective breadth. In order to have high Sharpe then, these portfolios must have very high IC. It is easier to increase effective breadth by hedging beta and sector exposure than it is to increase your IC.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "yzsacy9yxz4P"
      },
      "source": [
        "## Forecasts and Bet Correlation\n",
        "\n",
        "We define a bet as the forecast of the *residual* of a security return. This forecast can be implicit -- i.e., we buy a stock and thus implicity we forecast that the stock will go up. What though do we mean by *residual*? Without any fancy math, this simply means the return **less a hedge**. Let's work through three examples. We use the Ledoit-Wolf covariance estimator to assess our covariance in all cases. For more information on why we use Ledoit-Wolf instead of typical sample covariance, check out [Estimating Covariance Matrices](https://www.quantopian.com/lectures/estimating-covariance-matrices).\n",
        "\n",
        "### Example 1: No Hedge!\n",
        "\n",
        "If we go long on a set of securities, but do not hold any short positions, there is no hedge! So the *residual* is the stock return itself.\n",
        "\n",
        "$$r_{resid,i} = r_i$$\n",
        "\n",
        "Let's see what the correlation of our bets are in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "bHzz0fQWxz4P"
      },
      "source": [
        "tickers = ['WFC', 'JPM', 'USB', 'XOM', 'BHI', 'SLB'] # The securities we want to go long on\n",
        "historical_prices = get_pricing(tickers, start_date='2015-01-01',end_date='2017-02-22') # Obtain prices\n",
        "rets = historical_prices['close_price'].pct_change().fillna(0) # Calculate returns\n",
        "lw_cov = LedoitWolf().fit(rets).covariance_ # Calculate Ledoit-Wolf estimator\n",
        "\n",
        "def extract_corr_from_cov(cov_matrix):\n",
        "    # Linear algebra result:\n",
        "    # https://math.stackexchange.com/questions/186959/correlation-matrix-from-covariance-matrix\n",
        "    d = np.linalg.inv(np.diag(np.sqrt(np.diag(cov_matrix))))\n",
        "    corr = d.dot(cov_matrix).dot(d)\n",
        "    return corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "wV0YmDfXxz4Q"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
        "fig.tight_layout()\n",
        "\n",
        "corr = extract_corr_from_cov(lw_cov)\n",
        "# Plot prices\n",
        "left = historical_prices['close_price'].plot(ax=ax1)\n",
        "# Plot covariance as a heat map\n",
        "right = sns.heatmap(corr, ax=ax2, fmt='d', vmin=-1, vmax=1, xticklabels=tickers, yticklabels=tickers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "M8jyssBIxz4Q"
      },
      "source": [
        "average_corr = np.mean(corr[np.triu_indices_from(corr, k=1)])\n",
        "print 'Average pairwise correlation: %.4f' % average_corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "oc-yeOJMxz4R"
      },
      "source": [
        "The result here is that we have six bets and they are all very highly correlated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "DSMB9nhnxz4R"
      },
      "source": [
        "### Example 2: Beta Hedge\n",
        "\n",
        "In this case, we will assume that each bet is hedged against the market (SPY). In this case, the residual is calculated as:\n",
        "\n",
        "$$ r_{resid,i} = r_i - \\beta_i r_i $$\n",
        "\n",
        "where $\\beta_i$ is the beta to the market of security $i$ calculated with the [CAPM](https://www.quantopian.com/lectures/the-capital-asset-pricing-model-and-arbitrage-pricing-theory) and $r_i$ is the return of security $i$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "6jtleFk2xz4R"
      },
      "source": [
        "tickers = ['WFC', 'JPM', 'USB', 'SPY', 'XOM', 'BHI', 'SLB' ] # The securities we want to go long on plus SPY\n",
        "historical_prices = get_pricing(tickers, start_date='2015-01-01',end_date='2017-02-22') # Obtain prices\n",
        "rets = historical_prices['close_price'].pct_change().fillna(0) # Calculate returns\n",
        "\n",
        "market = rets[symbols(['SPY'])]\n",
        "stock_rets = rets.drop(symbols(['SPY']), axis=1)\n",
        "residuals = stock_rets.copy()*0\n",
        "\n",
        "for stock in stock_rets.columns:\n",
        "    model = sm.OLS(stock_rets[stock], market.values)\n",
        "    results = model.fit()\n",
        "    residuals[stock] = results.resid\n",
        "\n",
        "lw_cov = LedoitWolf().fit(residuals).covariance_ # Calculate Ledoit-Wolf Estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "fQKpEjPjxz4S"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
        "fig.tight_layout()\n",
        "\n",
        "corr = extract_corr_from_cov(lw_cov)\n",
        "left = (1+residuals).cumprod().plot(ax=ax1)\n",
        "right = sns.heatmap(corr, ax=ax2, fmt='d', vmin=-1, vmax=1, xticklabels=tickers, yticklabels=tickers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "ZJ5PObf3xz4T"
      },
      "source": [
        "average_corr = np.mean(corr[np.triu_indices_from(corr, k=1)])\n",
        "print 'Average pairwise correlation: %.4f' % average_corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ka9tbSvmxz4T"
      },
      "source": [
        "The beta hedge has brought down the average correlation significanty. Theoretically, this should improve our breadth. It is obvious that we are left with two highly correlated clusters however. Let's see what happens when we hedge the sector risk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "QZtucH1Yxz4U"
      },
      "source": [
        "### Example 3: Sector Hedge\n",
        "\n",
        "The sector return and the market return are themselves highly correlated. As such, you cannot do a multivariate regression due to multicollinearity, a classic [violation of regression assumptions](https://www.quantopian.com/lectures/violations-of-regression-models). To hedge against both the market and a given security's sector, you first estimate the market beta residuals and then calculate the sector beta on *those* residuals.\n",
        "\n",
        "$$\n",
        "r_{resid,i} = r_i - \\beta_i r_i \\\\\n",
        "r_{resid_{SECTOR},i}= r_{resid,i} - \\beta_{SECTOR,i}r_{resid,i}\n",
        "$$\n",
        "\n",
        "Here, $r_{resid, i}$ is the residual between the security return and a market beta hedge and $r_{resid_{SECTOR}, i}$ is the residual between *that* residual and a hedge of that residual against the relevant sector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "2RNHU3Gixz4U"
      },
      "source": [
        "tickers = ['WFC', 'JPM', 'USB', 'XLF', 'SPY', 'XOM', 'BHI', 'SLB', 'XLE']\n",
        "historical_prices = get_pricing(tickers, start_date='2015-01-01',end_date='2017-02-22')\n",
        "rets = historical_prices['close_price'].pct_change().fillna(0)\n",
        "\n",
        "# Get market hedge ticker\n",
        "mkt = symbols(['SPY'])\n",
        "# Get sector hedge tickers\n",
        "sector_1_hedge = symbols(['XLF'])\n",
        "sector_2_hedge = symbols(['XLE'])\n",
        "# Identify securities for each sector\n",
        "sector_1_stocks = symbols(['WFC', 'JPM', 'USB'])\n",
        "sector_2_stocks = symbols(['XOM', 'BHI', 'SLB'])\n",
        "\n",
        "market_rets = rets[mkt]\n",
        "sector_1_rets = rets[sector_1_hedge]\n",
        "sector_2_rets = rets[sector_2_hedge]\n",
        "                           \n",
        "stock_rets = rets.drop(symbols(['XLF', 'SPY', 'XLE']), axis=1)\n",
        "residuals_market = stock_rets.copy()*0\n",
        "residuals = stock_rets.copy()*0\n",
        "# Calculate market beta of sector 1 benchmark\n",
        "model = sm.OLS(sector_1_rets.values, market.values)\n",
        "results = model.fit()\n",
        "sector_1_excess = results.resid\n",
        "# Calculate market beta of sector 2 benchmark\n",
        "model = sm.OLS(sector_2_rets.values, market.values)\n",
        "results = model.fit()\n",
        "sector_2_excess = results.resid\n",
        "\n",
        "for stock in sector_1_stocks:\n",
        "    # Calculate market betas for sector 1 stocks\n",
        "    model = sm.OLS(stock_rets[stock], market.values)\n",
        "    results = model.fit()\n",
        "    # Calculate residual of security + market hedge\n",
        "    residuals_market[stock] = results.resid\n",
        "    # Calculate sector beta for previous residuals\n",
        "    model = sm.OLS(residuals_market[stock], sector_1_excess)\n",
        "    results = model.fit()\n",
        "    # Get final residual\n",
        "    residuals[stock] = results.resid\n",
        "                           \n",
        "for stock in sector_2_stocks:\n",
        "    # Calculate market betas for sector 2 stocks\n",
        "    model = sm.OLS(stock_rets[stock], market.values)\n",
        "    results = model.fit()\n",
        "    # Calculate residual of security + market hedge\n",
        "    residuals_market[stock] = results.resid\n",
        "    # Calculate sector beta for previous residuals\n",
        "    model = sm.OLS(residuals_market[stock], sector_2_excess)\n",
        "    results = model.fit()\n",
        "    # Get final residual\n",
        "    residuals[stock] = results.resid\n",
        "\n",
        "# Get covariance of residuals\n",
        "lw_cov = LedoitWolf().fit(residuals).covariance_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "O19TbpZexz4U"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
        "fig.tight_layout()\n",
        "\n",
        "corr = extract_corr_from_cov(lw_cov)\n",
        "left = (1+residuals).cumprod().plot(ax=ax1)\n",
        "right = sns.heatmap(corr, ax=ax2, fmt='d', vmin=-1, vmax=1, xticklabels=tickers, yticklabels=tickers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "Z2qekxBDxz4V"
      },
      "source": [
        "average_corr = np.mean(corr[np.triu_indices_from(corr, k=1)])\n",
        "print 'Average pairwise correlation: %.4f' % average_corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "o8jW7_gYxz4W"
      },
      "source": [
        "There we go! The sector hedge brought down the correlation between our bets to close to zero. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "EYboX73Jxz4W"
      },
      "source": [
        "## Calculating Effective Breadth\n",
        "\n",
        "This section is based on \"How to calculate breadth: An evolution of the fundamental law of active portfolio management\", by David Buckle; Vol. 4, 6, 393-405, 2003, _Journal of Asset Management_. Buckle derives the \"semi-generalised fundamental law of active management\" under several weak assumptions. The key result of this paper (for us) is a closed-form calculaiton of effective breadth as a function of the correlation between bets. Buckle shows that breadth, $BR$, can be modeled as\n",
        "\n",
        "$$BR = \\frac{N}{1 + \\rho(N -1)}$$\n",
        "\n",
        "where N is the number of stocks in the portfolio and $\\rho$ is the assumed single correlation of the expected variation around the forecast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "FnODZo7Exz4W"
      },
      "source": [
        "def buckle_BR_const(N, rho):\n",
        "    return N/(1 + rho*(N - 1))\n",
        "\n",
        "corr = np.linspace(start=0, stop=1.0, num=500)\n",
        "plt.plot(corr, buckle_BR_const(6, corr))\n",
        "plt.title('Effective Breadth as a function of Forecast Correlation (6 Stocks)')\n",
        "plt.ylabel('Effective Breadth (Number of Bets)')\n",
        "plt.xlabel('Forecast Correlation');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Vw8ebesHxz4X"
      },
      "source": [
        "Here we see that in the case of the long-only portfolio, where the average correlation is 0.56, we are *effectively making only approximately 2 bets*. When we hedge beta, with a resulting average correlation of 0.22, things get a little better, *three effective bets*. When we add the sector hedge, we get close to zero correlation, and in this case the number of bets equals the number of assets, 6.\n",
        "\n",
        "**More independent bets with the same IC leads to higher Sharpe ratio.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "knXxzIc8xz4X"
      },
      "source": [
        "## Using this in Practice\n",
        "\n",
        "Trading costs money due to market impact and commissions. As such, the post hoc implementation of a hedge is almost always suboptimal. In that case, you are trading purely to hedge risk. It is preferable to think about your sector and market exposure *throughout the model development process*. Sector and market risk is naturally hedged in a pairs-style strategy; in a cross-sectional strategy, consider de-meaning the alpha vector by the sector average; with an event-driven strategy, consider adding additional alphas so you can find offsetting bets in the same sector. As a last resort, hedge with a well chosen sector ETF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "lSENnwKdxz4X"
      },
      "source": [
        "## Why You Should Hedge Beta and Sector Exposures (Part II)\n",
        "by Jonathan Larkin and Maxwell Margenot\n",
        "\n",
        "Part of the Quantopian Lecture Series:\n",
        "\n",
        "* [www.quantopian.com/lectures](https://www.quantopian.com/lectures)\n",
        "* [github.com/quantopian/research_public](https://github.com/quantopian/research_public)\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "AaqxtYKWxz4X"
      },
      "source": [
        "In the first lecture on [Why You Should Hedge Beta and Sector Exposure](quantopian.com/lectures/why-you-should-hedge-beta-and-sector-exposures-part-i), we covered the information coefficient (IC) and effective breadth, providing yet more reasons to make as many independent bets as possible. Here we expand upon the concepts detailed there by decomposing portfolios of varying numbers of securities to further explore the effects of systematic risk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "G8McH2m-xz4Y"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "VukfbGMGxz4Y"
      },
      "source": [
        "## Hedging Beta and Sector Risk is Good for Allocators (Which is Good for You!)\n",
        "\n",
        "Let's work from two basic beliefs:\n",
        "- You would like someone to fund your algorithm\n",
        "- The institution that funds your algorithm is not going to allocate 100% of its money to you. In other words, your algorithm is one in a portfolio of algorithms.\n",
        "\n",
        "The implication of the second belief is subtle. Why should it matter that your high Sharpe algo is part of a portfolio? The key to understanding the importance of this and what it has to do with beta and sector exposure is the following mathematical result:\n",
        "\n",
        "**In a portfolio, stock specific risk can be diversified out while common factor risk cannot.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "irSLggtJxz4Y"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "<b>TL;DR:</b> Beta and sector exposure are **common factors**, i.e., they are among a handful of risk characteristics that are shared among all stocks. Risk exposure to common factors does not diversify away in a portfolio of algos. An allocator will not be able to make a large allocation to you if your algo presents common factor risk. The combination of many algos with modest common factor risk can lead to overwhelming common factor risk at the portfolio level. Allocators do not like this. If you want to get a large capital allocation, you must have low beta and sector exposure consistently over time.\n",
        "</div>\n",
        "\n",
        "## Foundations\n",
        "\n",
        "### Single Stock Risk Decomposition\n",
        "\n",
        "To build intuition, let's posit a single factor model:\n",
        "\n",
        "$$r_i = \\alpha_i + \\beta_i r_m + \\epsilon_i$$\n",
        "\n",
        "where $\\alpha_i$ is the intercept, $\\epsilon_i$ is the error, and $r_m$ is the market return. This is the [Capital Asset Pricing Model (CAPM)](https://www.quantopian.com/lectures/the-capital-asset-pricing-model-and-arbitrage-pricing-theory), which posits that the returns to a stock can be attributable to its beta-weighted exposure to the market and a return which is idiosyncratic to that stock. Two important assumptions here are that the $\\epsilon_i$s are uncorrelated to the market and each other across stocks. See the [Lecture on Beta Hedging](https://www.quantopian.com/lectures/beta-hedging) for more background.\n",
        "\n",
        "In this case, the \"risk\", as measured by the variance, for an individual stock is:\n",
        "\n",
        "$$\\sigma_i^2 = \\beta_i^2 \\sigma_m^2 + \\sigma_{\\epsilon_i}^2$$\n",
        "\n",
        "A stocks variance is broken into the **common risk**, $\\beta_i^2\\sigma_m^2$, and **specific risk**, $\\sigma_{\\epsilon_i}$. **Common risk** is risk in the stock driven by market risk which is common among all stocks proportionate to the stock's beta. **Specific risk** is the risk that is unique to that individual stock.\n",
        "\n",
        "Let's look at two examples and decompose the risk into the percent due to common factor risk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "KkBxzvFXxz4Z"
      },
      "source": [
        "def stock_risk(beta, market_vol, idio_vol):\n",
        "    common_risk = (beta**2)*(market_vol**2)\n",
        "    specific_risk = idio_vol**2\n",
        "    total_risk = common_risk + specific_risk\n",
        "    return total_risk, common_risk/total_risk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "tuEKmYdBxz4Z"
      },
      "source": [
        "We take two separate stocks, each with different market beta exposures and idiosyncratic volatility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "pyO6MU-Nxz4Z"
      },
      "source": [
        "# Betas\n",
        "b1 = 1.2\n",
        "b2 = 1.1\n",
        "# Market volatility\n",
        "market_vol = 0.15\n",
        "# Idiosyncratic volatilities\n",
        "idio_vol_1 = 0.10 \n",
        "idio_vol_2 = 0.07"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "Od_vPKjdxz4a"
      },
      "source": [
        "total_1, pct_common_1 = stock_risk(b1, market_vol, idio_vol_1)\n",
        "total_2, pct_common_2 = stock_risk(b2, market_vol, idio_vol_2)\n",
        "\n",
        "print \"Stock 1 risk (annualized standard deviation): %0.4f \" % np.sqrt(total_1)\n",
        "print \"Stock 1: percent of total risk due to common risk: %0.4f \" % pct_common_1\n",
        "\n",
        "print \"\\nStock 2 risk (annualized standard deviation): %0.4f \" % np.sqrt(total_2)\n",
        "print \"Stock 2: percent of total risk due to common risk: %0.4f \" % pct_common_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "T0kq3tnQxz4b"
      },
      "source": [
        "This is just looking at the breakdown of the risk associated with each individual stock. We can combine these into a portfolio to see how their combined volatility is affected by common factor risk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "uy2cI60Wxz4c"
      },
      "source": [
        "### Two Stock Portfolio Risk Decomposition\n",
        "\n",
        "Now let's imagine you have a two stock portfolio with percentage weights $w_1$ and $w_2$. The risk of the portfolio (derived below), $\\Pi$, under the one-factor model is then:\n",
        "\n",
        "$$\\sigma_{\\Pi}^2 = \\overbrace{\\sigma_m^2\\left( w_1^2\\beta_1^2 + w_2^2\\beta_2^2 + 2w_1w_2\\beta_1\\beta_1 \\right)}^{\\text{common risk}} + \\overbrace{w_1^2\\epsilon_1^2 + w_2^2 \\epsilon_2^2}^{\\text{specifc risk}}$$\n",
        "\n",
        "This is the simplest possible example of portfolio factor risk, one factor and two assets, yet we can already use it to gain intuition about portfolio risk and hedging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "BQZVSDfVxz4c"
      },
      "source": [
        "# The weights for each security in our portfolio\n",
        "w1 = 0.5\n",
        "w2 = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "gQjf6Qdfxz4d"
      },
      "source": [
        "def two_stocks_one_factor(w1, w2, b1, b2, market_vol, idio_vol_1, idio_vol_2):\n",
        "    common_risk = (market_vol**2)*(w1*w1*b1*b1 + w2*w2*b2*b2 + 2*w1*w2*b1*b2)\n",
        "    specific_risk =  w1*w1*idio_vol_1**2 + w2*w2*idio_vol_2**2\n",
        "    total_risk = common_risk + specific_risk\n",
        "    return total_risk, common_risk/total_risk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "t3YKecE0xz4e"
      },
      "source": [
        "The risk for a two stock, equally-weighted, long-only portfolio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "YNA19YUUxz4e"
      },
      "source": [
        "total, pct_common = two_stocks_one_factor(w1, w2, b1, b2, market_vol, idio_vol_1, idio_vol_2)\n",
        "print \"Portfolio risk (annualized standard deviation): %0.4f \" % np.sqrt(total)\n",
        "print \"Percent of total risk due to common risk: %0.4f\" % pct_common"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "GEP84Mb1xz4f"
      },
      "source": [
        "The astute reader will notice that the proportion of risk in the portfolio due to common factor risk is **larger for the portfolio** than for the weighted sum of the common risk proportion for the two components. To repeat the key point in this lecture: **In a portfolio, stock specific risk diversifies while common factor risk does not.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "1GV2PvGkxz4f"
      },
      "source": [
        "The risk for a two stock, beta-hedged long-short portfolio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "SWdxzkhIxz4f"
      },
      "source": [
        "w2 = -w1*b1/b2    # set weight 2 such that the portfolio has zero beta\n",
        "\n",
        "total, pct_common = two_stocks_one_factor(w1, w2, b1, b2, market_vol, idio_vol_1, idio_vol_2)\n",
        "print \"Portfolio risk (annualized standard deviation): %0.4f \" % np.sqrt(total)\n",
        "print \"Percent of total risk due to common risk: %0.4f\" % pct_common"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "llxr4p8fxz4g"
      },
      "source": [
        "Note that we eliminated **all** the common risk with a perfect beta hedge.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "s8_24bmDxz4g"
      },
      "source": [
        "## Portfolio Risk\n",
        "\n",
        "If $X$ is a column vector of n random variables, $X_1,\\dots,X_n$, and $c$ is a column vector of coefficients (constants), then the [variance of the weighted sum](https://en.wikipedia.org/wiki/Variance) $c'X$ is\n",
        "\n",
        "$$\\text{Var}(c'X) = c'\\Sigma c$$\n",
        "\n",
        "where $\\Sigma$ is the covariance matrix of the $X$'s.\n",
        "\n",
        "In our application, $c$ is our stock weight vector $w$ and $\\Sigma$ is the covariance matrix of stock returns.\n",
        "\n",
        "$$\\sigma_{\\Pi}^2 = w' \\Sigma w$$\n",
        "\n",
        "Just as we decompose the single stock risk above, we can decompose the covariance matrix to separate *common risk* and *specific risk*\n",
        "\n",
        "$$\\Sigma = BFB' + D$$\n",
        "\n",
        "Thus\n",
        "\n",
        "$$\\sigma_{\\Pi}^2  = w'(BFB' + D)w$$\n",
        "$$\\sigma_{\\Pi}^2  = w'BFB'w + w'Dw$$\n",
        "\n",
        "Which for the two stock portfolio above works out to\n",
        "\n",
        "\\begin{equation}\n",
        "\\sigma_{\\Pi}^2 =\n",
        "\\overbrace{\n",
        "\\begin{bmatrix} w_1 & w_2 \\end{bmatrix}\n",
        "\\begin{bmatrix} \\beta_{1} \\\\ \\beta_{2} \\end{bmatrix} \n",
        "\\sigma_m^2\n",
        "\\begin{bmatrix} \\beta_{1} & \\beta_{2} \\end{bmatrix}\n",
        "\\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}\n",
        "}^{\\text{common risk}}\n",
        "+ \\overbrace{\\begin{bmatrix} w_1 & w_2 \\end{bmatrix}\n",
        "\\begin{bmatrix} \\epsilon_1^2 & 0\\\\ 0 & \\epsilon_2^2 \\end{bmatrix}\n",
        "\\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}}^{\\text{specific risk}}\n",
        "\\end{equation}\n",
        "\n",
        "If you work through this matrix multiplication, you get the stated result above\n",
        "\n",
        "$$\\sigma_{\\Pi}^2 = \\overbrace{\\sigma_m^2\\left( w_1^2\\beta_1^2 + w_2^2\\beta_2^2 + 2w_1w_2\\beta_1\\beta_1 \\right)}^{\\text{common risk}} + \\overbrace{w_1^2\\epsilon_1^2 + w_2^2 \\epsilon_2^2}^{\\text{specifc risk}}$$\n",
        "\n",
        "### Multi-Factor Models\n",
        "\n",
        "Of course, we can expand the CAPM to include *additional* risk factors besides market beta. We could posit that there are in total $m$ risks which are *common* to all stocks.\n",
        "\n",
        "$$r_i = \\alpha_i + \\beta_{1,i} f_1 + \\dots + \\beta_{m,i} f_m + \\epsilon_i$$\n",
        "\n",
        "or more concisely\n",
        "\n",
        "$$r_i = \\alpha_i + \\sum_{j=1}^m \\beta_{j,i} f_j + \\epsilon_i$$\n",
        "\n",
        "or, considering all stocks, $i$, from 1 to N, even more concisely, for a given period $t$,\n",
        "\n",
        "$$r = \\alpha + Bf + \\epsilon$$\n",
        "\n",
        "where $r$ is the Nx1 column vector of returns, $B$ is the Nx$m$ matrix of factor betas, $f$ is the mx1 column of factor returns, and $\\epsilon$ is the Nx1 column vector of idiosyncratic returns. Finally,\n",
        "\n",
        "$$\\sigma_{\\Pi}^2  = w'BFB'w + w'Dw$$\n",
        "\n",
        "where $B$ is the Nx$m$ matrix of factor betas, $F$ is the $m$x$m$ covariance matrix of factor returns, and $D$ is a NxN matrix with the variance of $\\epsilon_i$'s on diagonal, and zeros everywhere else.\n",
        "\n",
        "With this result, *assuming we had a suitable risk model giving us the matrices $B$, $F$, and $D$*, we could calculate our portfolio risk and the proportion of risk coming from common risk.\n",
        "\n",
        "Likewise, just as we set $w_2$ above in the two stock case to the value that neutralized the exposure to the single factor $\\beta$, in the multi-factor case we could use the factor betas matrix $B$ to construct a portfolio which is neutral to **all** common factors. **A portfolio which is neutral to all common factors has zero common factor risk.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "zJt3r94Vxz4h"
      },
      "source": [
        "## Portfolios of Algos\n",
        "\n",
        "Even without a risk model, we can get some intuition as to how the risk of a portfolio of algos looks.\n",
        "\n",
        "What does a resulting portfolio of algos look like when the individual algos have non-zero common risk? Taking some inspiration from a recent journal article [The Dangers of Diversification](http://www.iijournals.com/doi/abs/10.3905/jpm.2017.43.2.013?journalCode=jpm) by Garvey, Kahn, and Savi, imagine that each algo has a certain *budget of common risk* it can take. This budget is defined as the percent common risk of total risk in the algo.\n",
        "\n",
        "In the first case, we assume that all algos have this same budget (and use all the budget!) and the correlation between their common risks is 1.0. This is simular to the case of a single factor model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "KSIHFrNaxz4h"
      },
      "source": [
        "def portfolio_risk_decomposition(budget=0.2, correl=1.0, algo_count=2, algo_total_risk=0.04):\n",
        "    N = algo_count\n",
        "    \n",
        "    algo_common_risk = budget*(algo_total_risk**2)\n",
        "    algo_idio_risk = algo_total_risk**2 - algo_common_risk\n",
        "\n",
        "    w = 1./N\n",
        "    \n",
        "    covar = correl*algo_common_risk\n",
        "    common_risk = N*w*w*algo_common_risk + (N*N - N)*w*w*covar\n",
        "    idio_risk = algo_idio_risk*w\n",
        "    total_risk = common_risk + idio_risk\n",
        "    \n",
        "    return total_risk, common_risk/total_risk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "nO4C_1jlxz4h"
      },
      "source": [
        "a, b = portfolio_risk_decomposition(budget=0.2, algo_count=20, correl=1.0, algo_total_risk=0.04)\n",
        "print \"Portfolio total risk: %.4f \" % np.sqrt(a)\n",
        "print \"Portfolio percent of common risk: %.4f \" % b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "Sibez6mixz4i"
      },
      "source": [
        "algos = np.linspace(1,20)\n",
        "plt.plot(\n",
        "    algos,\n",
        "    portfolio_risk_decomposition(budget=0.2, correl=1.0, algo_count=algos)[1]\n",
        ")\n",
        "plt.plot(\n",
        "    algos,\n",
        "    portfolio_risk_decomposition(budget=0.4, correl=1.0, algo_count=algos)[1]\n",
        ")\n",
        "plt.ylim([0,1]);\n",
        "plt.title('Percent of Portfolio Risk due to Common Risk')\n",
        "plt.xlabel('Number of Algos in Portfolio')\n",
        "plt.ylabel('Percent of Portfolio of Algos Risk due to Common Risk')\n",
        "plt.legend(\n",
        "    ['20% Single Algo Common Risk Budget', '40% Single Algo Common Risk Budget']\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "wgaEVkpWxz4j"
      },
      "source": [
        "From this plot, you can see that from the allocator's perspective, a \"small\" budget that allows for 20% of individual algo total risk to be driven by common risk leads to a 20 algo portfolio **with 83%** of it's risk driven by common risk! Ideally an allocator wants you to have **zero common factor risk**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "TsMvWRVexz4j"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "<b>TL;DR:</b> Even if you can't predict portfolio risk and don't have a risk model to decompose risk, you can form a portfolio with **zero common risk** by hedging the beta exposure to common factors. The most important common factors in the US Equity market are market beta and sector beta. Hedge your beta and be sector neutral if you want a large allocation from any allocator.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "pE9yAR-axz4j"
      },
      "source": [
        "*This presentation is for informational purposes only and does not constitute an offer to sell, a solicitation to buy, or a recommendation for any security; nor does it constitute an offer to provide investment advisory or other services by Quantopian, Inc. (\"Quantopian\"). Nothing contained herein constitutes investment advice or offers any opinion with respect to the suitability of any security, and any views expressed herein should not be taken as advice to buy, sell, or hold any security or as an endorsement of any security or company.  In preparing the information contained herein, Quantopian, Inc. has not taken into account the investment needs, objectives, and financial circumstances of any particular investor. Any views expressed and data illustrated herein were prepared based upon information, believed to be reliable, available to Quantopian, Inc. at the time of publication. Quantopian makes no guarantees as to their accuracy or completeness. All information is subject to change and may quickly become unreliable for various reasons, including changes in market conditions or economic circumstances.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "Bot7hCOVxz4j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "wPkccem4xz4k"
      },
      "source": [
        "# Portfolio Strategy - Risk and Money Management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "gSi07sbgxz4k"
      },
      "source": [
        "## Markowitz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "E9fxpgACxz4k"
      },
      "source": [
        "## Markowitz portfolio optimization\n",
        "\n",
        "\n",
        "## Introduction\n",
        "In this blog post you will learn about the basic idea behind Markowitz portfolio optimization as well as how to do it in Python. We will then show how you can create a simple backtest that rebalances its portfolio in a Markowitz-optimal way. We hope you enjoy it and get a little more enlightened in the process. \n",
        "\n",
        "\n",
        "```js\n",
        "def random_portfolio(returns):\n",
        "    ''' \n",
        "    Returns the mean and standard deviation of returns for a random portfolio\n",
        "    '''\n",
        "\n",
        "    p = np.asmatrix(np.mean(returns, axis=1))\n",
        "    w = np.asmatrix(rand_weights(returns.shape[0]))\n",
        "    C = np.asmatrix(np.cov(returns))\n",
        "    \n",
        "    mu = w * p.T\n",
        "    sigma = np.sqrt(w * C * w.T)\n",
        "    \n",
        "    # This recursion reduces outliers to keep plots pretty\n",
        "    if sigma > 2:\n",
        "        return random_portfolio(returns)\n",
        "    return mu, sigma\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "CcFgiA1Jxz4k"
      },
      "source": [
        "In the code you will notice the calculation of the return with:\n",
        "\n",
        "$$ R = p^T w $$\n",
        "\n",
        "where $R$ is the expected return, $p^T$ is the transpose of the vector for the mean\n",
        "returns for each time series and w is the weight vector of the portfolio. $p$ is a Nx1\n",
        "column vector, so $p^T$ turns into a 1xN row vector which can be multiplied with the\n",
        "Nx1 weight (column) vector w to give a scalar result. This is equivalent to the dot\n",
        "product used in the code. Keep in mind that `Python` has a reversed definition of\n",
        "rows and columns and the accurate `NumPy` version of the previous equation would\n",
        "be `R = w * p.T`\n",
        "\n",
        "Next, we calculate the standard deviation with\n",
        "\n",
        "$$\\sigma = \\sqrt{w^T C w}$$\n",
        "\n",
        "where $C$ is the covariance matrix of the returns which is a NxN matrix. Please\n",
        "note that if we simply calculated the simple standard deviation with the appropriate weighting using `std(array(ret_vec).T*w)` we would get a slightly different\n",
        "â€™bulletâ€™. This is because the simple standard deviation calculation would not take\n",
        "covariances into account. In the covariance matrix, the values of the diagonal\n",
        "represent the simple variances of each asset while the off-diagonals are the variances between the assets. By using ordinary `std()` we effectively only regard the\n",
        "diagonal and miss the rest. A small but significant difference.\n",
        "\n",
        "Lets generate the mean returns and volatility for 500 random portfolios:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "GLzwhs_Mxz4l"
      },
      "source": [
        "Upon plotting those you will observe that they form a characteristic parabolic\n",
        "shape called the â€˜Markowitz bulletâ€˜ with the boundaries being called the â€˜efficient\n",
        "frontierâ€˜, where we have the lowest variance for a given expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "3WC0wd_yxz4l"
      },
      "source": [
        "## Markowitz optimization and the Efficient Frontier\n",
        "\n",
        "Once we have a good representation of our portfolios as the blue dots show we can calculate the efficient frontier Markowitz-style. This is done by minimising\n",
        "\n",
        "$$ w^T C w$$\n",
        "\n",
        "for $w$ on the expected portfolio return $R^T w$ whilst keeping the sum of all the\n",
        "weights equal to 1:\n",
        "\n",
        "$$ \\sum_{i}{w_i} = 1 $$\n",
        "Here we parametrically run through $R^T w = \\mu$ and find the minimum variance\n",
        "for different $\\mu$â€˜s. This can be done with `scipy.optimise.minimize` but we have\n",
        "to define quite a complex problem with bounds, constraints and a Lagrange multiplier. Conveniently, the `cvxopt` package, a convex solver, does all of that for us. We used one of their [examples]() with some modifications as shown below. You will notice that there are some conditioning expressions in the code. They are simply needed to set up the problem. For more information please have a look at the `cvxopt` example.\n",
        "\n",
        "The `mus` vector produces a series of expected return values $\\mu$ in a non-linear and more appropriate way. We will see later that we donâ€˜t need to calculate a lot of these as they perfectly fit a parabola, which can safely be extrapolated for higher values.\n",
        "\n",
        "```js\n",
        "def optimal_portfolio(returns):\n",
        "    n = len(returns)\n",
        "    returns = np.asmatrix(returns)\n",
        "    \n",
        "    N = 100\n",
        "    mus = [10**(5.0 * t/N - 1.0) for t in range(N)]\n",
        "    \n",
        "    # Convert to cvxopt matrices\n",
        "    S = opt.matrix(np.cov(returns))\n",
        "    pbar = opt.matrix(np.mean(returns, axis=1))\n",
        "    \n",
        "    # Create constraint matrices\n",
        "    G = -opt.matrix(np.eye(n))   # negative n x n identity matrix\n",
        "    h = opt.matrix(0.0, (n ,1))\n",
        "    A = opt.matrix(1.0, (1, n))\n",
        "    b = opt.matrix(1.0)\n",
        "    \n",
        "    # Calculate efficient frontier weights using quadratic programming\n",
        "    portfolios = [solvers.qp(mu*S, -pbar, G, h, A, b)['x'] \n",
        "                  for mu in mus]\n",
        "    ## CALCULATE RISKS AND RETURNS FOR FRONTIER\n",
        "    returns = [blas.dot(pbar, x) for x in portfolios]\n",
        "    risks = [np.sqrt(blas.dot(x, S*x)) for x in portfolios]\n",
        "    ## CALCULATE THE 2ND DEGREE POLYNOMIAL OF THE FRONTIER CURVE\n",
        "    m1 = np.polyfit(returns, risks, 2)\n",
        "    x1 = np.sqrt(m1[2] / m1[0])\n",
        "    # CALCULATE THE OPTIMAL PORTFOLIO\n",
        "    wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)['x']\n",
        "    return np.asarray(wt), returns, risks\n",
        "\n",
        "weights, returns, risks = optimal_portfolio(return_vec)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(stds, means, 'o')\n",
        "plt.ylabel('mean')\n",
        "plt.xlabel('std')\n",
        "plt.plot(risks, returns, 'y-o')\n",
        "py.iplot_mpl(fig, filename='efficient_frontier', strip_style=True)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "XwdJ3yrjxz4l"
      },
      "source": [
        "As you can see, the performance here is pretty good, even through the 2008 financial crisis. This is most likey due to our universe selection and shouldn't always be expected. Increasing the number of stocks in the universe might reduce the volatility as well. Please let us know in the comments section if you had any success with this strategy and how many stocks you used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "8KdthE88xz4l"
      },
      "source": [
        "[cont'd](https://towardsdatascience.com/algorithmic-trading-based-on-mean-variance-optimization-in-python-62bdf844ac5b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "KY52krLXxz4l"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "vGf0gpjNxz4l"
      },
      "source": [
        "## Portfolio Analysis\n",
        "\n",
        "\n",
        "In professional quant workflows, it is critical to demonstrate the efficacy of any portfolio through rigorous testing. This is fundamental to understanding the risk profile as well as the performance of the portfolio. As such, quants and developers often have to build in-house tools to measure these metrics. To this end, we have created a package called pyfolio. pyfolio is a Python library for performance and risk analysis of financial portfolios, available on github [here](https://github.com/quantopian/pyfolio). It allows us to easily generate tear sheets to analyze the risk and performance of trading algorithms as well as return streams in general. \n",
        "    \n",
        "\n",
        "## Issues with Backtesting\n",
        "\n",
        "It is often tempting to run many backtests while building an algorithm. A common pitfall is to use the success of backtests as a feedback metric to fine-tune an algorithm's parameters or features while still in the construction phase. This leads to the overfitting of the strategy to whichever time periods the user ran the backtests on. Ultimately, this results in poor performance when deployed out of sample in live trading.\n",
        "\n",
        "As such, running backtests and generating tearsheets should only occur at the tail end of the algorithm creation lifecycle. We then get a picture of the algorithm's performance, aiding the user in deciding whether to move forward with the deployment of the algorithm or to switch to another strategy.\n",
        "\n",
        "There are two main parts to a full pyfolio tearsheet. First, there are the performance statistics in table format. Useful metrics such as the annual return, market beta, and Sharpe ratio are all listed in this table. These metrics not only represent how well the strategy has performed during the time period of the backtest (annual rate of return), they also show the risk-adjusted return as measured by the different ratios. We will go into more detail about the meaning of these metrics. \n",
        "\n",
        "Next, there are plots which help to visualize a variety of the performance metrics. For instance, the user can use the drawdown plots to quickly pinpoint the time periods in which the strategy performed the worst. In addition, it will help the user to see if the strategy is performing as it should - if a strategy is market neutral, but suffers significant drawdowns during crisis periods, then there are clearly issues with the strategy's design or implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "qJQ26qCoxz4l"
      },
      "source": [
        "### Setup - Getting a Backtest into the Research Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "jx_lmC9Vxz4l"
      },
      "source": [
        "First, we import a backtest into the research envrionment. In this lecture, we will use the backtest from [this forum post](https://www.quantopian.com/posts/how-to-get-an-allocation-writing-an-algorithm-for-the-quantopian-investment-management-team)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "KRJSaHj_xz4m"
      },
      "source": [
        "import pyfolio as pf\n",
        "import matplotlib.pyplot as plt\n",
        "import empyrical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "AqHE7ITYxz4m"
      },
      "source": [
        "# Get benchmark returns\n",
        "benchmark_rets = pf.utils.get_symbol_rets('SPY')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "-CmcXMnGxz4n"
      },
      "source": [
        "# Get the backtest\n",
        "bt = get_backtest('58812b2977ca4c474bbf393f')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "OC--KbTMxz4o"
      },
      "source": [
        "Now, we want to understand the returns, positions and transactions of the trading algorithm over our backtest's time period. We can get these data points from `backtest` object attributes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "vbCG4Y2Bxz4o"
      },
      "source": [
        "bt_returns = bt.daily_performance['returns']\n",
        "bt_positions = bt.pyfolio_positions\n",
        "bt_transactions = bt.pyfolio_transactions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "1BvGTjNUxz4o"
      },
      "source": [
        "Now, we are ready to use pyfolio to dive into the different performance metrics and plots of our algorithm. Throughout the course of this lecture we will detail how to interpret the various individual plots generated by an pyfolio tear sheet and include the proper call to generate the whole tear sheet at once at the end. This function is built into our backtest object, removing the need to write out all the code in the long form presented here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Cll_rwpVxz4p"
      },
      "source": [
        "### Performance Statistics\n",
        "\n",
        "With pyfolio, there is a wealth of performance statistics which most professional fund managers would use to analyze the performance of the algorithm. These metrics range from the algorithm's annual and monthly returns, return quantiles, rolling beta and sharpe ratios to the turnover of the portfolio. The most critical metrics are discussed as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "6u_bbxy7xz4p"
      },
      "source": [
        "#### Risk-Adjusted Return\n",
        "\n",
        "The risk-adjusted return is an essential metric of any strategy. Risk-adjusted returns allow us to judge returns streams that have different individual volatilities by providing an avenue for meaningful comparison. There are different measures of risk-adjusted returns but one of the most popular is the Sharpe ratio. In this particular backtest, the annual return of $2\\%$ for $1\\%$ volatility is an example of a relatively low absolute return, but a relatively high risk-adjusted return. Then, with a low risk strategy, leverage can then be applied to increase the absolute return."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "0l_5gfZWxz4p"
      },
      "source": [
        "print \"The Sharpe Ratio of the backtest is: \", empyrical.sharpe_ratio(bt_returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "Ek2K3UqWxz4q"
      },
      "source": [
        "#### Market Beta\n",
        "\n",
        "The market beta of an algorithm is the exposure of that stategy to the broader market. For instance, a market beta of $1$ would mean that you're buying the the market, while a beta of $-1$ means that you are shorting the market. Any beta within this range signifies reduced market influence, while any beta outside this range signifies increased market influence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "BBTySvehxz4q"
      },
      "source": [
        "print \"The market beta of the backtest is: \", empyrical.beta(bt_returns,benchmark_rets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "hiIPkz5Jxz4r"
      },
      "source": [
        "In the case of this strategy, the beta is 0. This means that this strategy has no exposure to the broader market, it is *market neutral*. To institutional investors, market neutral strategies are very attractive. After all, if the investors want a strategy which is highly exposed to the market, they could simply buy an ETF or an index fund."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "gSFCc-mExz4r"
      },
      "source": [
        "#### Drawdowns\n",
        "\n",
        "A drawdown is the 'peak to trough decline' of an investment strategy. Intuitively speaking, it refers to the losses the strategy has experienced from the base amount of capital which it had at the peak. For instance, in the 2008 Financial Crisis, the market drawdown was over 50% from the peak in 2007 to the trough in 2009."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "070sRgTOxz4r"
      },
      "source": [
        "print \"The maxmimum drawdown of the backtest is: \", empyrical.max_drawdown(bt_returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "WRoWcHRZxz4s"
      },
      "source": [
        "This is another measure of the financial risk of an algorithm. If the net drawdown of a strategy is very significant, this generally means that the volatility of the algorithm is more significant. Good strategies try to limit drawdowns. A good benchmark is to have a maximum drawdown of less than 20%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "eLsUwUvyxz4s"
      },
      "source": [
        "### Plotting\n",
        "\n",
        "In pyfolio, there is a `plotting` module which allows users to quickly plot these metrics. These plots can be individually plotted using the following functions:\n",
        "\n",
        "- `plot_annual_returns`\n",
        "- `plot_daily_returns_similarity`\n",
        "- `plot_daily_volume`\n",
        "- `plot_drawdown_periods`\n",
        "- `plot_drawdown_underwater`\n",
        "- `plot_exposures`\n",
        "- `plot_gross_leverage`\n",
        "- `plot_holdings`\n",
        "- `plot_long_short_holdings`\n",
        "- `plot_monthly_returns_dist`\n",
        "- `plot_monthly_returns_heatmap`\n",
        "- `plot_multistrike_cones`\n",
        "- `plot_prob_profit_trade`\n",
        "- `plot_return_quantiles`\n",
        "- `plot_rolling_beta`\n",
        "- `plot_rolling_returns`\n",
        "- `plot_rolling_sharpe`\n",
        "- `plot_turnover`\n",
        "- `plot_txn_time_hist`\n",
        "- `show_and_plot_top_positions`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Y8--U1eOxz4s"
      },
      "source": [
        "### Returns Plots\n",
        "\n",
        "Plots of cumulative returns and daily, non-cumulative returns allow you to gain a quick overview of the algorithm's performance and pick out any anomalies across the time period of the backtest. The cumulative return plot also allows you to make a comparison against benchmark returns - this could be against another investment strategy or an index like the S&P 500."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "XKYs6SgAxz4s"
      },
      "source": [
        "# Cumulative Returns\n",
        "plt.subplot(2,1,1)\n",
        "pf.plotting.plot_rolling_returns(bt_returns, benchmark_rets)\n",
        "\n",
        "# Daily, Non-Cumulative Returns\n",
        "plt.subplot(2,1,2)\n",
        "pf.plotting.plot_returns(bt_returns)\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "2rqLtNFHxz4t"
      },
      "source": [
        "With the annual and monthly return plots, you can see which years and months the algorithm performed the best in. For instance, in the monthly heatmap plot, this algorithm performed the best in June 2014 (shaded in dark green). In a backtest with a longer period of time, these plots will reveal more information. Furthermore, the distribution of the monthly returns is also instructive in gauging how the algorithm performs in different periods throughout the year and if it is affected by seasonal patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "123DKClexz4t"
      },
      "source": [
        "fig = plt.figure(1)\n",
        "plt.subplot(1,3,1)\n",
        "pf.plot_annual_returns(bt_returns)\n",
        "plt.subplot(1,3,2)\n",
        "pf.plot_monthly_returns_dist(bt_returns)\n",
        "plt.subplot(1,3,3)\n",
        "pf.plot_monthly_returns_heatmap(bt_returns)\n",
        "plt.tight_layout()\n",
        "fig.set_size_inches(15,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "51sRwjaKxz4u"
      },
      "source": [
        "#### Return Quantiles\n",
        "\n",
        "These box and whisker plots provide an overview of the return quantiles broken down by the return timeframe (daily / weekly / monthly) across the entire backtest time period."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "lr3BxDSSxz4u"
      },
      "source": [
        "pf.plot_return_quantiles(bt_returns);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "N0CSHgHAxz4v"
      },
      "source": [
        "The center line in the middle of each box shows the median return, and the box shows the first quartile (25th percentile) as well as the 3rd quartile (75th percentile). While a high median return is always helpful, it is also important to understand the returns distribution. A tight box means that the bulk of the returns (25th - 75th percentile) fall within a tight bound - i.e. the returns are consistent and not volatile. A larger box means that the returns are more spread out. It is important, however, to take note of the scale to the left to put the quartiles in perspective. In addition, returns over longer periods of time will have a wider distribution as increasing the length of time increases the variability in returns. \n",
        "\n",
        "The 'whiskers' at the end indicate the returns which fall outside the 25th and 75th percentile. A tight box with long whiskers indicate that there may be outliers in the returns - which may not be ideal if the outliers are negative. This may indicate that your strategy may be susceptible to certain market conditions / time periods. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "_tGV2ubbxz4v"
      },
      "source": [
        "### Rolling Plots\n",
        "\n",
        "Below, we have several rolling plots which show how an estimate changes throughout backtest period. In the case of the rolling beta and the rolling Sharpe ratio, the rolling estimate gives us more information than single point estimate for the entire period. A rolling estimate allows the user to see if the risk-adjusted return of the algorithm (Sharpe ratio) is consistent over time or if it fluctuates significantly. A volatile Sharpe ratio may indicate that the strategy may be riskier at certain time points or that it does not perform as well at these time points. Likewise, a volatile rolling beta indicates that it is exposed to the market during certain time points - if the strategy is meant to be market neutral, this could be a red flag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "pd3U5Q5-xz4v"
      },
      "source": [
        "#### Rolling Beta Plot\n",
        "\n",
        "The plot below shows the rolling beta of the strategy against benchmark returns over the entire period of the backtest. In this instance, the benchmark return of the SPY was used. Thus, the lower the rolling portfolio beta to the SPY, the more market neutral an algorithm is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "XmfW5h8Bxz4w"
      },
      "source": [
        "pf.plot_rolling_beta(bt_returns, benchmark_rets);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "iBjATcZnxz4w"
      },
      "source": [
        "#### Rolling Sharpe Ratio Plot\n",
        "\n",
        "The plot below shows the rolling Sharpe ratio over the period of the backtest. This allows you to understand the performance of the algorithm at different time points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "FDZqbIYKxz4x"
      },
      "source": [
        "pf.plot_rolling_sharpe(bt_returns);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "QqYTpk-2xz4x"
      },
      "source": [
        "In the case of this strategy, the Sharpe ratio is above 2 for the first 4 months before dropping toward the end of the year. It would be helpful here to check if this algorithm is exposed to other risk factors. This may help to explain the end of year slump. In addition, it would be helpful to understand the market situation at that point in time to see if the strategy was in some way affected by market events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "meshN6JJxz4x"
      },
      "source": [
        "#### Rolling Fama-French Single Factor Betas Plot\n",
        "\n",
        "In this plot, we see how exposed the strategy is to the 3 classical Fama-French factors. A factor model can be used to analyze the sources of risks and returns in a strategy or of any return stream. By looking at a strategy's historical returns, we can determine how much of these returns can be attributed to speculation on different factors and how much is a result of asset-specific fluctuations. This allows you to find out the sources of risk the portfolio is exposed to. For more information about Factor Models, check out the [Factor Risk Exposure lecture](https://www.quantopian.com/lectures#Factor-Risk-Exposure)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "PHZjMpdFxz4y"
      },
      "source": [
        "pf.plot_rolling_fama_french(bt_returns);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "C5poJYNmxz4y"
      },
      "source": [
        "These classical risk factors measure for small market cap, high-growth, and momentum stocks. The SMB curve represents small-cap stocks minus big-cap stocks, HML curve represents high-growth minus low-growth stocks, and the UMD curve checks exposure to any momentum strategy (i.e. stocks which are trending up perform better than stocks which are trending down). The idea behind these risk factors is that even though they may provide higher returns, they are able to do so because they are riskier. Therefore, low measures of these in your strategy may indicate that your strategy is less risky. \n",
        "\n",
        "Similar to the beta exposure to the market, a high exposure to a fama french factor ( $\\geq 1$) means that you are simply buying these known risk factors. If an algorithm's return is made up of *known* risk factors, such as the Fama-French ones, then the strategy is not as valuable in generating alpha. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "fN6yFRo_xz4y"
      },
      "source": [
        "#### Drawdown Plots\n",
        "\n",
        "In this plot, we visualize the drawdown chart described above. This chart provides an overview of the worst drawdown periods in the backtest. These periods show the time windows in the backtest in which the top 5 drawdowns occurred. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "33t3qbw0xz4z"
      },
      "source": [
        "pf.plot_drawdown_periods(bt_returns);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ZsAgJKyrxz40"
      },
      "source": [
        "This, coupled with the underwater plot, allows for a quick check into the time periods during which the algorithm struggles. Generally speaking, the less volatile an algorithm is, the more minimal the drawdowns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "A3PzVKAGxz40"
      },
      "source": [
        "pf.plot_drawdown_underwater(bt_returns);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "MkW7uwApxz40"
      },
      "source": [
        "#### Gross Leverage\n",
        "\n",
        "Gross leverage is the sum of long and short leverage exposure per share divided by net asset value. This plot allows you to see the amount of leverage being applied to the portfolio over the backtest period."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "EZoKcEZCxz41"
      },
      "source": [
        "pf.plot_gross_leverage(bt_returns, bt_positions);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "KnsmCnz6xz41"
      },
      "source": [
        "Monitoring the leverage of a strategy is important as it affects how you trade on margin. Unlike discretionary strategies where you could actively increase or decrease the leverage used in going long or short, algorithmic strategies automatically apply leverage during trading. Therefore, it is useful to monitor the gross leverage plot to ensure that the amount of leverage that your strategy uses is within the limits that you are comfortable with.\n",
        "\n",
        "Good strategies generally start with an initial leverage of 1. Upon finding out the viability of the strategy by examining the Sharpe ratio and other metrics, leverage can be increased or decreased accordingly. A lower Sharpe ratio indicates that the strategy has a higher volatility per unit return, making it more risky to lever up. On the other hand, a higher Sharpe ratio indicates lower volatility per unit return, allowing you to increase the leverage and correspondingly, returns.\n",
        "\n",
        "For more details, take a look at the [lecture on leverage](https://www.quantopian.com/lectures#Leverage)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "63zcv0Uzxz41"
      },
      "source": [
        "### Positions Plots\n",
        "\n",
        "#### Top Long and Short Positions\n",
        "\n",
        "The tables below list the top 10 long and short positions of all time. The goal of each algorithm is to minimize the proportion of the portfolio invested in each security at any time point. This prevents the movement of any individual security from having a significant impact on the portfolio as a whole. The bigger the exposure a strategy has to any security, the greater the risk.\n",
        "\n",
        "Generally, the biggest failure point for many strategies is high portfolio concentration in a few securities. While this may produce significant positive returns over a given time period, the converse can easily occur. Huge swings in a small number of equities would result in significant drawdowns. Good strategies tend to be those in which no security comprises more than 10% of the portfolio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "EmbCs2dAxz42"
      },
      "source": [
        "pos_percent = pf.pos.get_percent_alloc(bt_positions)\n",
        "pf.plotting.show_and_plot_top_positions(bt_returns, pos_percent);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "xLBopEiRxz42"
      },
      "source": [
        "#### Holdings Per Day\n",
        "\n",
        "The holdings per day allows us to gain an insight into whether the total portfolio holdings fluctuate from day to day. This plot provides a good sanity check as to whether the algorithm is performing as it should, or if there were any bugs which should be fixed. For instance, we can use to holdings plot to check if the trading behavior is expected, i.e. if there are extended periods in which the number of holdings is exceptionally low or if that the algorithm is not trading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "jerYbe0mxz43"
      },
      "source": [
        "pf.plot_holdings(bt_returns, bt_positions);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "8W8rjaq7xz43"
      },
      "source": [
        "### Transaction Plots\n",
        "\n",
        "#### Daily Turnover\n",
        "\n",
        "This plot reflects how many shares are traded as a fraction of total shares. The higher the daily turnover, the higher the transaction costs associated with the algorithm. However, this also means that the returns and risk metrics are better able to capture the underlying performance of the algorithm as the higher quantity of trades provides more samples (of returns, risk, etc.) to draw from. This would in turn give a better estimation on *Out of Sample* periods as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "sMv96dYPxz44"
      },
      "source": [
        "pf.plot_turnover(bt_returns, bt_transactions, bt_positions);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "64vMCzihxz44"
      },
      "source": [
        "Likewise, the Daily Turnover Histogram gives you an overview of the distribution of the turnover of your portfolio. This shows you both the average daily turnover of your portfolio and any outlier trading days."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "1S6l7ASyxz44"
      },
      "source": [
        "pf.plotting.plot_daily_turnover_hist(bt_transactions, bt_positions);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "zL9o_eadxz45"
      },
      "source": [
        "Similarly, another plot which allows you to gauge the number of transactions per day is the Daily Trading Volume plot. This shows the number of shares traded per day and displays the all-time daily trading average as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "gUaoUf9jxz45"
      },
      "source": [
        "pf.plotting.plot_daily_volume(bt_returns, bt_transactions);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ylnQFv7Nxz46"
      },
      "source": [
        "The transaction time histogram shows you **when** the algorithm makes its trades during each day. You can specify the size of the bin (each column's width) as well as the timezone in the function's parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "fV3UQQuTxz46"
      },
      "source": [
        "pf.plotting.plot_txn_time_hist(bt_transactions);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "OxfeEHSExz47"
      },
      "source": [
        "### Round Trip Plots\n",
        "\n",
        "When evaluating the performance of an investment strategy, it is helpful to quantify the frequency, duration, and profitability of its independent bets, or \"round trip\" trades. A round trip trade is when a new long or short position is opened and later completely or partially closed out.\n",
        "\n",
        "The intent of the round trip tearsheet is to differentiate strategies that profited off of a few lucky trades from strategies that profited repeatedly off of genuine alpha. Breaking down round trip profitability by traded name and sector can also inform universe selection and identify exposure risks. For example, even if your equity curve looks robust, if only two securities in your universe of fifteen names contributed to overall profitability, you may have reason to question the logic of your strategy.\n",
        "\n",
        "To identify round trips, pyfolio reconstructs the complete portfolio based on the transactions that you pass in. When you make a trade, pyfolio checks if shares are already present in your portfolio purchased at a certain price. If there are, we compute the Profit and Loss (P&L), returns and duration of that round trip. In calculating round trips, pyfolio also appends position-closing transactions at the last timestamp in the positions data. This closing transaction will cause the P&L from any open positions to realized as completed round trips.\n",
        "\n",
        "Before the round trip plots, there is a table of summary statistics which provide useful information about the strategy. For instance, the `Percent profitable` statistic shows the percentage of all trades which are profitable. This allows us to calculate the probability of the strategy making a profitable decision. This probability is also reflected in the round trip plots. A quick check of this plot tells us if our strategy is performing better than chance. In addition, the `PnL stats` also break down our average net profit for each trade and allow us to see how much of a role our short side trades play versus our long side trades in contributing to our total profit. These statistics give you a quick overview of the profitability of the strategy.\n",
        "\n",
        "**Note**: These plots are not included by default in the `create_full_tear_sheet()` function. In order to plot the round trip plots, you have pass in `round_trips=True` as a parameter to the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "HQQj8vwDxz47"
      },
      "source": [
        "The easiest way to run the analysis is to call `pf.create_round_trip_tear_sheet()`. Passing in a sector map is optional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "FlJZaZl6xz47"
      },
      "source": [
        "pf.create_round_trip_tear_sheet(bt_returns, bt_positions, bt_transactions);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "zP3miGnWxz48"
      },
      "source": [
        "## Help Function\n",
        "\n",
        "The aforementioned functions are all available on the Quantopian research platform (as you have seen). If you're stuck on any particular function, you can directly access the help page with the `help` function as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "T6vGx-05xz48"
      },
      "source": [
        "help(pf.plotting.plot_rolling_sharpe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "u7UpbekLxz49"
      },
      "source": [
        "## Creating a Full Tear Sheet\n",
        "\n",
        "To put these all together, we use a single function call to `bt.create_full_tear_sheet`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "cfaW3ksZxz49"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "heading_collapsed": true,
        "hidden": true,
        "id": "XpCBVR6Pxz49"
      },
      "source": [
        "## Risk-Constrained Portfolio Optimization\n",
        "\n",
        "by Rene Zhang and Max Margenot\n",
        "\n",
        "Part of the Quantopian Lecture Series:\n",
        "* [www.quantopian.com/lectures](https://www.quantopian.com/lectures)\n",
        "* [https://github.com/quantopian/research_public](https://github.com/quantopian/research_public)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "3VaAdfu-xz4-"
      },
      "source": [
        "Risk management is critical for constructing portfolios and building algorithms. Its main function is to improve the quality and consistency of returns by adequately accounting for risk. Any returns obtained by *unexpected* risks, which are always lurking within our portfolio, can usually not be relied upon to produce profits over a long time. By limiting the impact of or eliminating these unexpected risks, the portfolio should ideally only have exposure to the alpha we are pursuing. In this lecture, we will focus on how to use factor model in risk management. \n",
        "\n",
        "## Factor Models\n",
        "We have written many lectures on [Factor Models](https://www.quantopian.com/lectures/the-capital-asset-pricing-model-and-arbitrage-pricing-theory) and the calculation of [Factor Risk Exposure](https://www.quantopian.com/lectures/factor-risk-exposure), as well as [how to analyze alpha factors](https://www.quantopian.com/lectures/factor-analysis). The notation we generally use when introducing a factor model is as follows:\n",
        "\n",
        "$$R_i = a_i + b_{i1} F_1 + b_{i2} F_2 + \\ldots + b_{iK} F_k + \\epsilon_i$$\n",
        "\n",
        "where:\n",
        "$$\\begin{eqnarray}\n",
        "k &=& \\text{the number of factors}\\\\\n",
        "R_i &=& \\text{the return for company $i$}, \\\\\n",
        "a_i &=& \\text{the intercept},\\\\\n",
        "F_j &=& \\text{the return for factor $j$, $j \\in [1,k]$}, \\\\\n",
        "b_{ij} &=& \\text{the corresponding exposure to factor $j$, $j \\in [1,k]$,} \\\\\n",
        "\\epsilon_i &=& \\text{specific fluctuation of company $i$.}\\\\\n",
        "\\end{eqnarray}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "1BrnPjSVxz4-"
      },
      "source": [
        "To quantify unexpected risks and have acceptable risk levels in a given portfolio, we need to answer 3 questions:\n",
        "\n",
        "1. What proportion of the variance of my portfolio comes from common risk factors?\n",
        "      \n",
        "2. How do I limit this risk?\n",
        "   \n",
        "3. Where does the return/PNL of my portfolio come from, i.e., to what do I attribute the performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "hidden": true,
        "id": "c21MPtZcxz4-"
      },
      "source": [
        "These risk factors can be:\n",
        "- Classical fundamental factors, such as those in the [CAPM](https://www.quantopian.com/lectures/the-capital-asset-pricing-model-and-arbitrage-pricing-theory) (market risk) or the [Fama-French 3-Factor Model](https://www.quantopian.com/lectures/fundamental-factor-models) (price-to-book (P/B) ratio, volatility)\n",
        "- Sector or industry exposure\n",
        "- Macroeconomic factors, such as inflation or interest rates\n",
        "- Statistical factors that are based on historical returns and derived from principal component\n",
        "  analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "yAGQ0wlTxz4-"
      },
      "source": [
        "### Universe \n",
        "\n",
        "The base universe of assets we use here is the QTradableStocksUS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "JK_5JFFkxz4-"
      },
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install statsmodels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "frZP1q4Zxz4_"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from zipline.pipeline import Pipeline\n",
        "#from zipline.pipeline.data import Fundamentals\n",
        "#from zipline.pipeline.data.builtin import USEquityPricing\n",
        "from zipline.pipeline.factors import CustomFactor, Returns\n",
        "#from zipline.pipeline.filters import QTradableStocksUS\n",
        "#from zipline.research import run_pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "yzkBJlmwxz4_"
      },
      "source": [
        "# date range for building risk model\n",
        "start = \"2009-01-01\"\n",
        "end = \"2011-01-01\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "3pigKWOexz5A"
      },
      "source": [
        "First we pull the returns of every asset in this universe across our desired time period."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "zh4nSZM_xz5A"
      },
      "source": [
        "def qtus_returns(start_date, end_date):\n",
        "    pipe = Pipeline(\n",
        "        columns={'Close': USEquityPricing.close.latest},\n",
        "        screen = QTradableStocksUS()\n",
        "    )\n",
        "    stocks = run_pipeline(pipe, start_date, end_date)  \n",
        "    unstacked_results = stocks.unstack()\n",
        "    \n",
        "    prices = (unstacked_results['Close'].fillna(method='ffill').fillna(method='bfill')\n",
        "                  .dropna(axis=1,how='any').shift(periods=-1).dropna())  \n",
        "    qus_returns = prices.pct_change()[1:]\n",
        "    return qus_returns\n",
        "\n",
        "\n",
        "R = qtus_returns(start, end)\n",
        "print ('The universe we define includes {} assets.').format(R.shape[1])\n",
        "print 'The number of timestamps is {} from {} to {}.'.format(R.shape[0], start, end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "M8BcDeKhxz5B"
      },
      "source": [
        "assets = R.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "WaDAhpTSxz5C"
      },
      "source": [
        "### Factor Returns and Exposures\n",
        "\n",
        "We will start with the classic Fama-French factors. The Fama-French factors are the market, company size, and company price-to-book (PB) ratio. We compute each asset's exposures to these factors, computing the factors themselves using pipeline code borrowed from the [Fundamental Factor Models lecture](https://www.quantopian.com/lectures/fundamental-factor-models). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "sRYutsVKxz5C"
      },
      "source": [
        "def make_pipeline():\n",
        "    \"\"\"\n",
        "    Create and return our pipeline.\n",
        "    \n",
        "    We break this piece of logic out into its own function to make it easier to\n",
        "    test and modify in isolation.\n",
        "    \n",
        "    In particular, this function can be copy/pasted into the backtester and run by itself.\n",
        "    \"\"\"\n",
        "    # Market Cap\n",
        "    market_cap = Fundamentals.shares_outstanding.latest/USEquityPricing.close.latest\n",
        "    # Book to Price ratio\n",
        "    book_to_price = 1/Fundamentals.pb_ratio.latest\n",
        "    \n",
        "    # Build Filters representing the top and bottom 500 stocks by our combined ranking system.\n",
        "    biggest = market_cap.top(500, mask=QTradableStocksUS())\n",
        "    smallest = market_cap.bottom(500, mask=QTradableStocksUS())\n",
        "    \n",
        "    highpb = book_to_price.top(500, mask=QTradableStocksUS())\n",
        "    lowpb = book_to_price.bottom(500, mask=QTradableStocksUS())\n",
        "    \n",
        "    universe = biggest | smallest | highpb | lowpb\n",
        "    \n",
        "    pipe = Pipeline(\n",
        "        columns = {\n",
        "            'returns' : Returns(window_length=2),\n",
        "            'market_cap' : market_cap,\n",
        "            'book_to_price' : book_to_price,\n",
        "            'biggest' : biggest,\n",
        "            'smallest' : smallest,\n",
        "            'highpb' : highpb,\n",
        "            'lowpb' : lowpb\n",
        "        },\n",
        "        screen=universe\n",
        "    )\n",
        "    return pipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "-1yTpH1vxz5C"
      },
      "source": [
        "Here we run our pipeline and create the return streams for high-minus-low and small-minus-big."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "nzCv5rdZxz5D"
      },
      "source": [
        "pipe = make_pipeline()\n",
        "# This takes a few minutes.\n",
        "results = run_pipeline(pipe, start, end)\n",
        "R_biggest = results[results.biggest]['returns'].groupby(level=0).mean()\n",
        "R_smallest = results[results.smallest]['returns'].groupby(level=0).mean()\n",
        "\n",
        "R_highpb = results[results.highpb]['returns'].groupby(level=0).mean()\n",
        "R_lowpb = results[results.lowpb]['returns'].groupby(level=0).mean()\n",
        "\n",
        "SMB = R_smallest - R_biggest\n",
        "HML = R_highpb - R_lowpb\n",
        "\n",
        "df = pd.DataFrame({\n",
        "         'SMB': SMB, # company size\n",
        "         'HML': HML  # company PB ratio\n",
        "    },columns =[\"SMB\",\"HML\"]).shift(periods =-1).dropna()\n",
        "\n",
        "MKT = get_pricing('SPY', start_date=start, end_date=end, fields='price').pct_change()[1:]\n",
        "MKT = pd.DataFrame({'MKT':MKT})\n",
        "\n",
        "F = pd.concat([MKT,df],axis = 1).dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "9ORLgW_Bxz5D"
      },
      "source": [
        "ax = ((F + 1).cumprod() - 1).plot(subplots=True, title='Cumulative Fundamental Factors')\n",
        "ax[0].set(ylabel = \"daily returns\")\n",
        "ax[1].set(ylabel = \"daily returns\")\n",
        "ax[2].set(ylabel = \"daily returns\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Z1VYtlbExz5E"
      },
      "source": [
        "### Calculating the Exposures\n",
        "\n",
        "Running a multiple linear regression on the fundamental factors for each asset in our universe, we can obtain the corresponding factor exposure for each asset. Here we express:\n",
        "\n",
        "$$ R_i = \\alpha_i + \\beta_{i, MKT} R_{i, MKT} + \\beta_{i, HML} R_{i, HML} + \\beta_{i, SMB} R_{i, SMB} + \\epsilon_i$$\n",
        "\n",
        "for each asset $S_i$. This shows us how much of each individual security's return is made up of these risk factors.\n",
        "\n",
        "We calculate the risk exposures on an asset-by-asset basis in order to get a more granular view of the risk of our portfolio. This approach requires that we know the holdings of the portfolio itself, on any given day, and is computationally expensive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "oyuCajqGxz5E"
      },
      "source": [
        "# factor exposure\n",
        "B = pd.DataFrame(index=assets, dtype=np.float32)\n",
        "epsilon = pd.DataFrame(index=R.index, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "q7Q7Zsm3xz5F"
      },
      "source": [
        "x = sm.add_constant(F)\n",
        "\n",
        "for i in assets:\n",
        "    y = R.loc[:,i]\n",
        "    y_inlier = y[np.abs(y - y.mean())<=(3*y.std())]\n",
        "    x_inlier = x[np.abs(y - y.mean())<=(3*y.std())]\n",
        "    result = sm.OLS(y_inlier, x_inlier).fit()\n",
        "\n",
        "    B.loc[i,\"MKT_beta\"] = result.params[1]\n",
        "    B.loc[i,\"SMB_beta\"] = result.params[2]\n",
        "    B.loc[i,\"HML_beta\"] = result.params[3]\n",
        "    epsilon.loc[:,i] = y - (x.iloc[:,0] * result.params[0] +\n",
        "                            x.iloc[:,1] * result.params[1] + \n",
        "                            x.iloc[:,2] * result.params[2] +\n",
        "                            x.iloc[:,3] * result.params[3])\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "YaPmz5wMxz5F"
      },
      "source": [
        "The factor exposures are shown as follows. Each individual asset in our universe will have a different exposure to the three included risk factors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "f_GzhdOixz5G"
      },
      "source": [
        "fig,axes = plt.subplots(3, 1)\n",
        "ax1,ax2,ax3 =axes\n",
        "\n",
        "B.iloc[0:10,0].plot.barh(ax=ax1, figsize=[15,15], title=B.columns[0])\n",
        "B.iloc[0:10,1].plot.barh(ax=ax2, figsize=[15,15], title=B.columns[1])\n",
        "B.iloc[0:10,2].plot.barh(ax=ax3, figsize=[15,15], title=B.columns[2])\n",
        "\n",
        "ax1.set(xlabel='beta')\n",
        "ax2.set(xlabel='beta')\n",
        "ax3.set(xlabel='beta')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "abtAeAbLxz5G"
      },
      "source": [
        "B.loc[symbols('AAPL'),:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "m2ihbkI5xz5H"
      },
      "source": [
        "### Summary of the Setup:\n",
        "1. returns of assets in universe: `R`\n",
        "2. fundamental factors: `F`\n",
        "3. Exposures of these fundamental factors: `B`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "_ZP6jNQ6xz5H"
      },
      "source": [
        "Currently, the `F` DataFrame contains the return streams for MKT, SMB, and HML, by date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "0RRe0CRpxz5H"
      },
      "source": [
        "F.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Y10MJJFNxz5I"
      },
      "source": [
        "While the `B` DataFrame contains point estimates of the beta exposures **to** MKT, SMB, and HML for every asset in our universe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "tCRZ-BH6xz5I"
      },
      "source": [
        "B.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "wTmZXYgcxz5J"
      },
      "source": [
        "Now that we have these values, we can start to crack open the variance of any portfolio that contains these assets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "GdoKX0vXxz5J"
      },
      "source": [
        "### Splitting Variance into Common Factor Risks\n",
        "\n",
        "The portfolio variance can be represented as:\n",
        "  \n",
        "  $$\\sigma^2 = \\omega BVB^{\\top}\\omega^{\\top} + \\omega D\\omega^{\\top}$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\\begin{eqnarray}\n",
        "B &=& \\text{the matrix of factor exposures of $n$ assets to the factors} \\\\\n",
        "    V &=& \\text{the covariance matrix of factors} \\\\\n",
        "    D &=& \\text{the specific variance} \\\\\n",
        "    \\omega &=& \\text{the vector of portfolio weights for $n$ assets}\\\\\n",
        "    \\omega BVB^{\\top}\\omega^{\\top} &=& \\text{common factor variance} \\\\\n",
        "    \\omega D\\omega^{\\top} &=& \\text{specific variance} \\\\\n",
        "\\end{eqnarray}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "TMDZz9QPxz5J"
      },
      "source": [
        "#### Computing Common Factor and Specific Variance:\n",
        "\n",
        "Here we build functions to break out the risk in our portfolio. Suppose that our portfolio consists of all stocks in the Q3000US, equally-weighted. Let's have a look at how much of the variance of the returns in this universe are due to common factor risk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "-8RRpRLrxz5J"
      },
      "source": [
        "w = np.ones([1,R.shape[1]])/R.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "1Wn7bH39xz5K"
      },
      "source": [
        "def compute_common_factor_variance(factors, factor_exposures, w):   \n",
        "    B = np.asarray(factor_exposures)\n",
        "    F = np.asarray(factors)\n",
        "    V = np.asarray(factors.cov())\n",
        "    \n",
        "    return w.dot(B.dot(V).dot(B.T)).dot(w.T)\n",
        "\n",
        "common_factor_variance = compute_common_factor_variance(F, B, w)[0][0]\n",
        "print(\"Common Factor Variance: {0}\".format(common_factor_variance))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "MVIMHhFuxz5L"
      },
      "source": [
        "def compute_specific_variance(epsilon, w):       \n",
        "    \n",
        "    D = np.diag(np.asarray(epsilon.var())) * epsilon.shape[0] / (epsilon.shape[0]-1)\n",
        "\n",
        "    return w.dot(D).dot(w.T)\n",
        "\n",
        "specific_variance = compute_specific_variance(epsilon, w)[0][0]\n",
        "print(\"Specific Variance: {0}\".format(specific_variance))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "VH0T08QTxz5L"
      },
      "source": [
        "In order to actually calculate the percentage of our portfolio variance that is made up of common factor risk, we do the following:\n",
        "\n",
        "\n",
        "$$\\frac{\\text{common factor variance}}{\\text{common factor variance + specific variance}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "ViC96eHYxz5L"
      },
      "source": [
        "common_factor_pct = common_factor_variance/(common_factor_variance + specific_variance)*100.0\n",
        "print(\"Percentage of Portfolio Variance Due to Common Factor Risk: {0:.2f}%\".format(common_factor_pct))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "eBWyt6JFxz5M"
      },
      "source": [
        "So we see that if we just take every single security in the Q3000US and equally-weight them, we will end up possessing a portfolio that effectively only contains common risk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "7umbsDjwxz5M"
      },
      "source": [
        "### Risk-Constrained Optimization\n",
        "\n",
        "Currently we are operating with an equal-weighted portfolio. However, we can reapportion those weights in such a way that we minimize the common factor risk illustrated by our common factor exposures. This is a portfolio optimization problem to find the optimal weights.\n",
        "\n",
        "We define this problem as:\n",
        "\n",
        "\\begin{array}{ll} \\mbox{$\\text{minimize/maximum}$}_{w} & \\text{objective function}\\\\\n",
        "\\mbox{subject to} & {\\bf 1}^T \\omega = 1, \\quad f=B^T\\omega\\\\\n",
        "& \\omega \\in {\\cal W}, \\quad f \\in {\\cal F},\n",
        "\\end{array}\n",
        "\n",
        "where the variable $w$ is the vector of allocations, the variable $f$ is weighted factor exposures, and  the variable ${\\cal F}$ provides our constraints for $f$. We set ${\\cal F}$ as a vector to bound the weighted factor exposures of the porfolio. These constraints allow us to reject weightings that do not fit our criteria. For example, we can set the maximum factor exposures that our portfolios can have by changing the value of ${\\cal F}$. A value of $[1,1,1]$ would indicate that we want the maximum factor exposure of the portfolio to each factor to be less than $1$, rejecting any portfolios that do not meet that condition.\n",
        "\n",
        "We define the objective function as whichever business goal we value highest. This can be something such as maximizing the Sharpe ratio or minimizing the volatility. Ultimately, what we want to solve for in this optimization problem is the weights, $\\omega$.\n",
        "\n",
        "Let's quickly generate some random weights to see how the weighted factor exposures of the portfolio change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "kdmgofRdxz5M"
      },
      "source": [
        "w_0 = np.random.rand(R.shape[1])\n",
        "w_0 = w_0/np.sum(w_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "n8-Pe9evxz5N"
      },
      "source": [
        "The variable $f$ contains the weighted factor exposures of our portfolio, with size equal to the number of factors we have.  As we change $\\omega$, our weights, our weighted exposures, $f$, also change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "EEWrx9Atxz5N"
      },
      "source": [
        "f = B.T.dot(w_0)\n",
        "f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "BJy-L5jbxz5O"
      },
      "source": [
        "A concrete example of this can be found [here](http://nbviewer.jupyter.org/github/cvxgrp/cvx_short_course/blob/master/applications/portfolio_optimization.ipynb), in the docs for CVXPY."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "PQG4E1mWxz5O"
      },
      "source": [
        "### Performance Attribution\n",
        "\n",
        "Let's take a sample algo from the Quantopian forums and attribute its performance using pyfolio. This should give us an understanding of the specific and common risk associated with the algorithm's return stream."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "6TqrYXFtxz5O"
      },
      "source": [
        "bt_wsj = get_backtest('59232d19c931f1619e6423c9')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "tdTGWqN2xz5P"
      },
      "source": [
        "Now we'll run the algorithm using [Quantopian's built-in risk model](https://www.quantopian.com/posts/new-tool-for-quants-the-quantopian-risk-model) and performance attribution tearsheet. We extend beyond the Fama-French Factors, looking into common factor risk due to sectors and due to particular styles of investment that are common in the market."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "HRlwZ7plxz5P"
      },
      "source": [
        "bt_wsj.create_perf_attrib_tear_sheet();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "gUG8affkxz5Q"
      },
      "source": [
        "## References\n",
        "* Qian, E.E., Hua, R.H. and Sorensen, E.H., 2007. *Quantitative equity portfolio management: modern techniques and applications*. CRC Press.\n",
        "* Narang, R.K., 2013. *Inside the Black Box: A Simple Guide to Quantitative and High Frequency Trading*. John Wiley & Sons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "j89su0VCxz5Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "9QgkhGKoxz5R"
      },
      "source": [
        "--- \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "NmWeuv8qxz5R"
      },
      "source": [
        "# Back Up: Appendix\n",
        "<a id=\"11\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "9VImKsQkxz5R"
      },
      "source": [
        "--- \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "rAFAR_Ryxz5R"
      },
      "source": [
        "# Outlook for Part II: Derivatives, Big Data and Sentiment Analysis  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "45rB6UBBxz5R"
      },
      "source": [
        "--- \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "F_jbc85zxz5R"
      },
      "source": [
        "# Sources for a Stream of Information and novel Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "11KUqdUCxz5S"
      },
      "source": [
        "[Quant Library on GitHub](https://github.com/wilsonfreitas/awesome-quant)\n",
        "<br>\n",
        "[KPI Library/Stock Screener](http://www.stockfetcher.com)\n",
        "<br>\n",
        "[Machine Learning Financial Laboratory](https://mlfinlab.readthedocs.io/en/latest/index.html)\n",
        "<br>\n",
        "[Backtesting Library](https://pypi.org/project/Backtesting/)\n",
        "<br>\n",
        "[Free Historical Data](https://stooq.com/db/h/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "pkPKadrwxz5S"
      },
      "source": [
        "--- \n",
        "---"
      ]
    }
  ]
}