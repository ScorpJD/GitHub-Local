{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzEMgWjGxzsT"
   },
   "source": [
    "<a ><img src=\"https://cdn.tuinvest.de/assets/logos/TUInvest_Logo-353d42494757660b8381a31c9f99a6ca.png\"  width=\"200\" align=\"left\"> </a>\n",
    "<div style=\"text-align: right\"> <h3><span style=\"color:gray\"> INTERNAL USE ONLY </span> </h3> </div>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<a ><img src=\"Pictures/QSeries.png\" Width=\"300\" align=\"center\"> </a>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<h1><center>AlgoTrading 101 — A hands-on Introduction</center></h1>\n",
    "<h2><center> <span style=\"font-weight:normal\"><font color='#022F73'> Lecture 1: Data Collection, Preparation and Manipulation </font>  </span></center></h2>\n",
    "\n",
    "\n",
    "<h3><center><font color='gray'>JONAS GOTTAL</font></center></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dO2hJc3wxzsW"
   },
   "source": [
    "<h4>About this Notebook</h4>\n",
    "The Objective of this Lecture is a comprehensible Introduction to Algo Trading <br>\n",
    "for the members of TU Invest, to raise curiosity and resurrect Alternative Strategies/Assets.\n",
    "\n",
    "Although the material showed is not fit to  *understand*  for beginners, with easier strategies <br>\n",
    "there won't be a profitable among them. Thus the math and stats behind the following stratgies <br>\n",
    "might be hard to understand but not to implement. And I hope this will lead some curious <br>\n",
    "minds to further study the concepts more thoroughly. <br>\n",
    "*Therefore the sole Momentum and Pair Trading Strategies will be very short.*\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<span style=\"color:gray\"> * *To limit our time spent per section, basics in Python and Statistics are welcomed. <br> If not, there will be links to a <a href=\"#16\"> Backup Section</a> with additional code and explanations.* </span>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqQx4LXuxzsY"
   },
   "source": [
    "---\n",
    "---\n",
    "<div style=\"text-align: center\"> <h3><span style=\"color:red\"> THIS IS A WORKING DRAFT TO BE SPLIT IN LECTURES WHEN FINISHED </span> </h3> </div>\n",
    "\n",
    "<div style=\"text-align: center\"> <h3><span style=\"color:red\"> <em> Many parts will be cut or at least reduced </em></span> </h3> </div>\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kI5wU5naxzsa"
   },
   "source": [
    "<h1>Table of contents</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"text-decoration:none; margin-top: 30px; background-color:#F2F2F2; border-color:#022F73\">\n",
    "    <span style=\"color:#022F73\">\n",
    "    <ol>\n",
    "      <li><a href=\"#1\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Intro  </span> </a></li>\n",
    "          <ol>\n",
    "      <li><a href=\"#2\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >A comprehensive Overview  </span> </a></li>\n",
    "      <li><a href=\"#3\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Definition of AlgoTrading  </span> </a></li>\n",
    "      <li><a href=\"#4\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Benefits of an algorithmic approach</span> </a></li>\n",
    "      <li><a href=\"#5\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Our Focus and Scope</span> </a></li>\n",
    "        </ol>\n",
    "      <li><a href=\"#6\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Raw Data </span> </a></li>\n",
    "       <ol>\n",
    "       <li><a href=\"#7\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Provider and APIs </span> </a></li>\n",
    "       <li><a href=\"#8\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Let's limit our universe: What Markets and Instruments are fit for the start </span> </a></li>\n",
    "       <li><a href=\"#9\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Data Snooping: p-Hacking, SVD and PCA </span> </a></li>\n",
    "       </ol>   \n",
    "       <li><a href=\"#10\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Preferred Statistical Characteristics of Data</span> </a></li>\n",
    "       <ol>\n",
    "       <li><a href=\"#11\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Skewness and Kurtosis</span> </a></li>\n",
    "       <li><a href=\"#12\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Stationarity </span> </a></li>    \n",
    "       <li><a href=\"#13\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Integration</span> </a></li>\n",
    "       <li><a href=\"#14\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Cointegration </span> </a></li> \n",
    "       </ol>   \n",
    "       <li><a href=\"#15\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Todays Objective: processed dataset for our next lecture as feather pd </span> </a></li>\n",
    "        <li><a href=\"#16\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >APPENDIX</span> </a></li>\n",
    "        <ol> \n",
    "        <li><a href=\"#17\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Statistics 101</span> </a></li>\n",
    "       <li><a href=\"#18\"> <span style=\"color:#022F73;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Python 101</span> </a></li> \n",
    "        </ol>\n",
    "    </ol>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq5rk3lDxzsg"
   },
   "outputs": [],
   "source": [
    "# !pip install auquan_toolbox\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dtale\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import backtester\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "import sys\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "AxaGJt3Gxzsx"
   },
   "source": [
    "# 1. Intro <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## A Comprehensive Overview <a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a ><img src=\"Pictures/org.png\" Width=\"600\" align=\"center\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Technical based on Endogenous Factors and Quant Strategies mostly additional on Exogenous Indicators:\n",
    "\n",
    "Are these factors Endogenous or Exogenous?\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> Exogenous </span>: An unanticipated change in consumer tastes for your product\n",
    "\n",
    "<span style=\"color:red\"> Exogenous </span>: The U.S. Federal Reserve changes the cost of borrowing\n",
    "\n",
    "<span style=\"color:red\"> Exogenous </span>: A competitor has major safety issues with their products\n",
    "\n",
    "<span style=\"color:red\"> Exogenous </span>: The price of critical components for making phones and computers suddenly changes \n",
    "\n",
    "<span style=\"color:blue\"> Endogenous </span>: Quarterly earnings announcements \n",
    "\n",
    "<span style=\"color:red\"> Exogenous </span>: Shipping times from overseas grow due to new ocean weather patterns\n",
    "\n",
    "<span style=\"color:blue\"> Endogenous </span>: Trading volume matched with price to track Volume-Weighted Average Price\n",
    "\n",
    "<span style=\"color:red\"> Exogenous </span>: CEO is replaced unexpectedly\n",
    "\n",
    "<span style=\"color:blue\"> Endogenous </span>: Trends in stock prices measured by momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Quant Strategy:  HFT vs. StatArb\n",
    "\n",
    "High Frequency Trading exploits millisecond and sub-millisecond\n",
    "market microstructure inefficiencies.\n",
    "* Focuses on infrastructure flaws (uneven playing field) rather than longer term predictions of asset prices\n",
    "* Models behavior patterns of major players in a market\n",
    "* Seeks to uncover large orders hidden by stealth execution strategies\n",
    "\n",
    "Examples of High Frequency Trading Strategies\n",
    "* An investment fund wants to purchase a large block of stock.\n",
    "* Uses stealth execution strategy to mask the order but not perfectly.\n",
    "* Other market players detect the strategy and \"jump ahead\" of the order with their own buy orders\n",
    "* Spoofing \"fake\" orders\n",
    "\n",
    "Stat Arb\n",
    "* Create a set of rules to generate trade orders and manage position risk with minimal intervention\n",
    "* Identify statistically significant and repeatable market behavior and exploit it to generate profits.\n",
    "* Can be low-frequency (weekly, daily) to high-frequency (seconds, milliseconds,...)\n",
    "\n",
    "Quant Trading Challenges\n",
    "Trading is Competitive\n",
    "* Market is about 80% quant already\n",
    "* Strategies optimal for assumptions only\n",
    "* With risk there is no guarantee of a profit\n",
    "* Markets change (regimes, correlations. volatility...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "42MfywAkxzs6"
   },
   "source": [
    "### Statistical arbitrage or StatArb:\n",
    "1. trading signals are systematic, or rules-based, as opposed to driven by fundamentals, \n",
    "2. the trading book is market-neutral2, in the sense that it has zero beta with the market, and \n",
    "3. the mechanism for generating ex- cess returns is statistical” (Avellaneda and Lee, 2010, p. 761). \n",
    "\n",
    "*Additional*\n",
    "+ large numbers of securities \n",
    "+ very short holding periods \n",
    "+ substantial computational, trading, and information technology (IT) infrastructure\n",
    "\n",
    "Typical approaches range from plain vanilla pairs trading to sophisticated, nonlinear models from the domains of machine learning, physics and mathematics.\n",
    "Krauss (2015) provides a recent review of more than 90 statistical arbitrage pairs trading strategies, focusing on relative mispricings between two and more securities. [Krauss, C. (2015). Statistical arbitrage pairs trading strategies: Review and outlook. IWQW Discussion Paper Series, University of Erlangen-Nürnberg.]\n",
    "\n",
    "\n",
    "In contrast, classical financial research is primarily focused on identifying capital market anomalies with high explanatory value. [Jacobs, H. (2015). What explains the dynamics of 100 anomalies? Journal of Banking & Finance, 57:65–85.] provide explanations for capital market anomalies \n",
    "The Journal of Finance, one of the leading academic journals in that field produces 17 references for a search for ”neural networks”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Market participants\n",
    "1. Retail investors: The average person. He or she may be buying BTC because a friend recommended it, or selling mined BTC to get some cash. \n",
    "1. Institutional Investors: Organizations or high net-worth individuals who trade large quantities. They can be responsible for big market movements.\n",
    "1. Professional Human Traders: These people are actively trying to beat the market. They may make trades based on news, some combination of technical analysis indicators, or gut feeling.\n",
    "1. Algorithms: Trading algorithms receive market data, make decisions, and place orders automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### History\n",
    "#### Use of Al and ML in Algorithmic Trading \n",
    "\n",
    "* 1982 James Simons starts quant investment firm Renaissance Technologies\n",
    "* 1987 Black Monday 22% one-day crash in S&P 500 caused by automated \"program trading\"\n",
    "* 1988 David Shaw founds D.E. Shaw and is an early adopter of Al among its hedge funds\n",
    "* 2010 Flash Crash occurs on May 6. In 36 minutes. the S&P crashed 8%, before a rebound\n",
    "* 2012 Knight Capital loses \\\\$440 million in 45 minutes after deploying unverified trading software\n",
    "* 2017 Two Sigma hedge fund which uses ML. crosses the \\\\$50 billion in assets under management\n",
    "* 2018 Renaissance Technologies. citing reduced profitability, reduces the use of pattern-based strategies for futures trading in its Renaissance Institutional Diversified Alpha (RIDA) fund by more than 60%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Definition of AlgoTrading <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "OrrIgRAqxzs0"
   },
   "source": [
    "---\n",
    "<font color='black'>The term goes back to the universal scholar **Abu Dschaʿfar Muhammad ibn Musa al-Chwarizmi**, Latinized as *Algorismi*, a mathematician, astronomer and geographer during the Abbasid heyday around 800 AD, who lived in Baghdad in the \"House of wisdom”. Alchwarizmi revolutionized mathematics in the western world with a book on the Indian number system and arithmetic rules such as addition, subtraction, multiplication, division, fractions, and the extraction of roots. However, some well-known algorithms are much older, as the sieve of Eratosthenes, the Euclidean algorithm and Gaussian elimination were already described over two thousand years ago.\n",
    "</font>\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Benefits of an algorithmic Approach <a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "BN7hPW1oxzs2"
   },
   "source": [
    "<font color='black'>The rate of failure in quantitative finance is high, particularly so in financial ML. The few who succeed amass a large amount of assets and deliver consistently exceptional performance to their investors. However, that is a rare outcome, for reasons explained in this book. Over the past two decades, I have seen many faces come and go, firms started and shut down. In my experience, there is one critical mistake that underlies all those failures.\n",
    "\n",
    "1. **The Sisyphus Paradigm:** \n",
    "Discretionary portfolio managers (PMs) make investment decisions that do not fol- low a particular theory or rationale (if there were one, they would be systematic PMs).\n",
    "Because nobody fully understands the logic behind their bets, investment firms ask them to work independently from one another.\n",
    "\n",
    "\n",
    "2. **The Meta-Strategy Paradigm:** \n",
    "If you have been asked to develop ML strategies on your own, the odds are stacked against you. It takes almost as much effort to produce one true investment strategy as to produce a hundred, and the complexities are overwhelming: data curation and processing, HPC infrastructure, software development, feature analysis, execution simulators, backtesting, etc. As in a *Production Chain*:\n",
    "\n",
    "    1. Data Curators\n",
    "    collecting, cleaning, indexing, storing, adjusting, and delivering all data   \n",
    "    1. Feature Analysts\n",
    "    transforming raw data into informative signals.    \n",
    "    1.Strategists\n",
    "     informative features are transformed into actual investment algorithms. (Is it a behavioral bias? Asymmetric information? Regulatory constraints?)    \n",
    "    1. Backtesters\n",
    "     assesses the profitability of an investment strategy under various sce- narios    \n",
    "    1. Deployment Team\n",
    "     integrating the strategy code into the production line    \n",
    "    1. Portfolio Oversight\n",
    "     follows a *cursus honorum*\n",
    "     \n",
    "         1. *Embargo:* run on data observed after the end date of the backtest.     \n",
    "         1. *Papertrading:* run on a live, real-time feed\n",
    "         1. *Graduation:* strategy manages a real position\n",
    "         1. *Re-allocation:* Based on the production performance, the allocation to gradu- ated strategies is re-assessed frequently\n",
    "         1. *Decommission:* Eventually, all strategies are discontinued. \n",
    "         \n",
    "*Final Thought:* \n",
    "Amateurs develop individual strategies, believing that there is such a thing as a magical formula for riches. In contrast, professionals develop methods to mass-produce strategies. The money is not in making a car, it is in making a car factory.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Our Focus and Scope <a id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "FxrXDxHpxzs4"
   },
   "source": [
    "<a ><img src=\"Pictures/overview.png\" Width=\"900\" align=\"center\"> </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "DF1kCbydxztC"
   },
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. Raw Data <a id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Provider and APIs <a id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Let's limit our universe: What Markets and Instruments are fit for the start <a id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "ymOTnjUnxztD"
   },
   "source": [
    "## Data Snooping - How to find features for a Strategy <a id=\"9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "3QPlua1OxztE"
   },
   "source": [
    "### p Hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "-X41cM4TxztG"
   },
   "source": [
    "#### p-Hacking and Multiple Comparisons Bias\n",
    "\n",
    "By Delaney Mackenzie and Maxwell Margenot.\n",
    "\n",
    "Part of the Quantopian Lecture Series:\n",
    "\n",
    "* [www.quantopian.com/lectures](https://www.quantopian.com/lectures)\n",
    "* [github.com/quantopian/research_public](https://github.com/quantopian/research_public)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Multiple comparisons bias is a pervasive problem in statistics, data science, and in general forecasting/predictions. The short explanation is that the more tests you run, the more likely you are to get an outcome that you want/expect. If you ignore the multitude of tests that failed, you are clearly setting yourself up for failure by misinterpreting what's going on in your data.\n",
    "\n",
    "A particularly common example of this is when looking for relationships in large data sets comprising of many indepedent series or variables. In this case you run a test each time you evaluate whether a relationship exists between a set of variables.\n",
    "\n",
    "\n",
    "### Statistics Merely Illuminates This Issue\n",
    "\n",
    "Most folks also fall prey to multiple comparisons bias in real life. Any time you make a decision you are effectively taking an action based on an hypothesis. That hypothesis is often tested. You can end up unknowingly making many tests in your daily life.\n",
    "\n",
    "An example might be deciding which medicine is helping cure a cold you have. Many people will take multiple medicines at once to try and get rid of symptoms. You may think that a certain medicine worked, when in reality none did and the cold just happened to start getting better at some point.\n",
    "\n",
    "The point here is that this problem doesn't stem from statistical testing and p-values. Rather, these techniques give us much more information about the problem and when it might be occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "gAMwiyxrxztI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "JLo3LokoxztW"
   },
   "source": [
    "#### Refresher: Spearman Rank Correlation\n",
    "\n",
    "Please refer to [this lecture](https://www.quantopian.com/lectures/spearman-rank-correlation) for more full info, but here is a very brief refresher on Spearman Rank Correlation.\n",
    "\n",
    "It's a variation of correlation that takes into account the ranks of the data. This can help with weird distributions or outliers that would confuse other measures. The test also returns a p-value, which is key here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "46c5svWKxztX"
   },
   "source": [
    "A higher coefficient means a stronger estimated relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "fFw8RZtmxzta"
   },
   "outputs": [],
   "source": [
    "X = pd.Series(np.random.normal(0, 1, 100))\n",
    "Y = X\n",
    "\n",
    "r_s = stats.spearmanr(Y, X)\n",
    "print 'Spearman Rank Coefficient: ', r_s[0]\n",
    "print 'p-value: ', r_s[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "oe2GSK-8xztp"
   },
   "source": [
    "If we add some noise our coefficient will drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "tdZ6STBJxzts"
   },
   "outputs": [],
   "source": [
    "X = pd.Series(np.random.normal(0, 1, 100))\n",
    "Y = X + np.random.normal(0, 1, 100)\n",
    "\n",
    "r_s = stats.spearmanr(Y, X)\n",
    "print 'Spearman Rank Coefficient: ', r_s[0]\n",
    "print 'p-value: ', r_s[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "jhwYp5QUxzt5"
   },
   "source": [
    "#### p-value Refresher\n",
    "\n",
    "For more info on p-values see [this lecture](https://www.quantopian.com/lectures/hypothesis-testing). What's important to remember is they're used to test a hypothesis given some data. Here we are testing the hypothesis that a relationship exists between two series given the series values.\n",
    "\n",
    "####IMPORTANT: p-values must be treated as binary.\n",
    "\n",
    "A common mistake is that p-values are treated as more or less significant. This is bad practice as it allows for what's known as [p-hacking](https://en.wikipedia.org/wiki/Data_dredging) and will result in more false positives than you expect. Effectively, you will be too likely to convince yourself that relationships exist in your data.\n",
    "\n",
    "To treat p-values as binary, a cutoff must be set in advance. Then the p-value must be compared with the cutoff and treated as significant/not signficant. Here we'll show this.\n",
    "\n",
    "#### The Cutoff is our Significance Level\n",
    "\n",
    "We can refer to the cutoff as our significance level because a lower cutoff means that results which pass it are significant at a higher level of confidence. So if you have a cutoff of 0.05, then even on random data 5% of tests will pass based on chance. A cutoff of 0.01 reduces this to 1%, which is a more stringent test. We can therefore have more confidence in our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ycJOvsyIxzt6"
   },
   "outputs": [],
   "source": [
    "# Setting a cutoff of 5% means that there is a 5% chance\n",
    "# of us getting a significant p-value given no relationship\n",
    "# in our data (false positive).\n",
    "# NOTE: This is only true if the test's assumptions have been\n",
    "# satisfied and the test is therefore properly calibrated.\n",
    "# All tests have different assumptions.\n",
    "cutoff = 0.05\n",
    "\n",
    "X = pd.Series(np.random.normal(0, 1, 100))\n",
    "Y = X + np.random.normal(0, 1, 100)\n",
    "\n",
    "r_s = stats.spearmanr(Y, X)\n",
    "print 'Spearman Rank Coefficient: ', r_s[0]\n",
    "if r_s[1] < cutoff:\n",
    "    print 'There is significant evidence of a relationship.'\n",
    "else:\n",
    "    print 'There is not significant evidence of a relationship.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "MOaXfC1rxzuE"
   },
   "source": [
    "#### Experiment - Running Many Tests\n",
    "\n",
    "We'll start by defining a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "oUS_qRcMxzuG"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "vinpJZ9nxzuL"
   },
   "source": [
    "Now we'll populate it by adding `N` randomly generated timeseries of length `T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "tjraf0AOxzuM"
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "T = 100\n",
    "\n",
    "for i in range(N):\n",
    "    X = np.random.normal(0, 1, T)\n",
    "    X = pd.Series(X)\n",
    "    name = 'X%s' % i\n",
    "    df[name] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "U2Mbr9PJxzuV"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "B5jojcW_xzue"
   },
   "source": [
    "Now we'll run a test on all pairs within our data looking for instances where our p-value is below our defined cutoff of 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "P0gx3TPLxzuf"
   },
   "outputs": [],
   "source": [
    "cutoff = 0.05\n",
    "\n",
    "significant_pairs = []\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(i+1, N):\n",
    "        Xi = df.iloc[:, i]\n",
    "        Xj = df.iloc[:, j]\n",
    "        \n",
    "        results = stats.spearmanr(Xi, Xj)\n",
    "        \n",
    "        pvalue = results[1]\n",
    "        \n",
    "        if pvalue < cutoff:\n",
    "            significant_pairs.append((i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "tv9V6rJKxzuo"
   },
   "source": [
    "Before we check how many significant results we got, let's run out some math to check how many we'd expect. The formula for the number of pairs given N series is\n",
    "\n",
    "$$\\frac{N(N-1)}{2}$$\n",
    "\n",
    "There are no relationships in our data as it's all randomly generated. If our test is properly calibrated we should expect a false positive rate of 5% given our 5% cutoff. Therefore we should expect the following number of pairs that achieved significance based on pure random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "c3HrNz-0xzuo"
   },
   "outputs": [],
   "source": [
    "(N * (N-1) / 2) * 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "U0vF2-P_xzuz"
   },
   "source": [
    "Now let's compare to how many we actually found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "WCh9RQncxzu2"
   },
   "outputs": [],
   "source": [
    "len(significant_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "ZWoOucZjxzu7"
   },
   "source": [
    "We shouldn't expect the numbers to match too closely here on a consistent basis as we've only run one experiment. If we run many of these experiments we should see a convergence to what we'd expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "qjgL62JFxzu8"
   },
   "source": [
    "#### Repeating the Experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "afX6wRO6xzu9"
   },
   "outputs": [],
   "source": [
    "def do_experiment(N, T, cutoff=0.05):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Make random data\n",
    "    for i in range(N):\n",
    "        X = np.random.normal(0, 1, T)\n",
    "        X = pd.Series(X)\n",
    "        name = 'X%s' % i\n",
    "        df[name] = X\n",
    "\n",
    "    significant_pairs = []\n",
    "\n",
    "    # Look for relationships\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            Xi = df.iloc[:, i]\n",
    "            Xj = df.iloc[:, j]\n",
    "\n",
    "            results = stats.spearmanr(Xi, Xj)\n",
    "\n",
    "            pvalue = results[1]\n",
    "\n",
    "            if pvalue < cutoff:\n",
    "                significant_pairs.append((i, j))\n",
    "    \n",
    "    return significant_pairs\n",
    "\n",
    "\n",
    "num_experiments = 100\n",
    "\n",
    "results = np.zeros((num_experiments,))\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    # Run a single experiment\n",
    "    result = do_experiment(20, 100, cutoff=0.05)\n",
    "    \n",
    "    # Count how many pairs\n",
    "    n = len(result)\n",
    "    \n",
    "    # Add to array\n",
    "    results[i] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "sVq0ZDqtxzvF"
   },
   "source": [
    "The average over many experiments should be closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "TS0BBkt8xzvG"
   },
   "outputs": [],
   "source": [
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "846iUw-ixzvK"
   },
   "source": [
    "#### Visualizing What's Going On\n",
    "\n",
    "What's happening here is that p-values should be uniformly distributed, given no signal in the underlying data. Basically, they carry no information whatsoever and will be equally likely to be 0.01 as 0.99. Because they're popping out randomly, you will expect a certain percentage of p-values to be underneath any threshold you choose. The lower the threshold the fewer will pass your test.\n",
    "\n",
    "Let's visualize this by making a modified function that returns p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "JeTIaTj0xzvL"
   },
   "outputs": [],
   "source": [
    "def get_pvalues_from_experiment(N, T):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Make random data\n",
    "    for i in range(N):\n",
    "        X = np.random.normal(0, 1, T)\n",
    "        X = pd.Series(X)\n",
    "        name = 'X%s' % i\n",
    "        df[name] = X\n",
    "\n",
    "    pvalues = []\n",
    "\n",
    "    # Look for relationships\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            Xi = df.iloc[:, i]\n",
    "            Xj = df.iloc[:, j]\n",
    "\n",
    "            results = stats.spearmanr(Xi, Xj)\n",
    "\n",
    "            pvalue = results[1]\n",
    "\n",
    "            pvalues.append(pvalue)\n",
    "    \n",
    "    return pvalues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "xPGph0frxzvO"
   },
   "source": [
    "We'll now collect a bunch of pvalues. As in any case we'll want to collect quite a number of p-values to start getting a sense of how the underlying distribution looks. If we only collect few, it will be noisy like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "6Q16v-oCxzvP"
   },
   "outputs": [],
   "source": [
    "pvalues = get_pvalues_from_experiment(10, 100)\n",
    "plt.hist(pvalues)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Observed p-value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "tJGisoy9xzvW"
   },
   "source": [
    "Let's dial up our `N` parameter to get a better sense. Keep in mind that the number of p-values will increase at a rate of\n",
    "\n",
    "$$\\frac{N (N-1)}{2}$$\n",
    "\n",
    "or approximately quadratically. Therefore we don't need to increase `N` by much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "-Mir9soyxzvX",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pvalues = get_pvalues_from_experiment(50, 100)\n",
    "plt.hist(pvalues)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Observed p-value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "wQ1Zjx_Oxzva"
   },
   "source": [
    "Starting to look pretty flat, as we expected. Lastly, just to visualize the process of drawing a cutoff, we'll draw two artificial lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "qldNkvuMxzvc"
   },
   "outputs": [],
   "source": [
    "pvalues = get_pvalues_from_experiment(50, 100)\n",
    "plt.vlines(0.01, 0, 150, colors='r', linestyle='--', label='0.01 Cutoff')\n",
    "plt.vlines(0.05, 0, 150, colors='r', label='0.05 Cutoff')\n",
    "plt.hist(pvalues, label='P-Value Distribution')\n",
    "plt.legend()\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Observed p-value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "N2aVMd4Wxzvh"
   },
   "source": [
    "We can see that with a lower cutoff we should expect to get fewer false positives. Let's check that with our above experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "fIYNsIzFxzvm"
   },
   "outputs": [],
   "source": [
    "num_experiments = 100\n",
    "\n",
    "results = np.zeros((num_experiments,))\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    # Run a single experiment\n",
    "    result = do_experiment(20, 100, cutoff=0.01)\n",
    "    \n",
    "    # Count how many pairs\n",
    "    n = len(result)\n",
    "    \n",
    "    # Add to array\n",
    "    results[i] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "RYvvA9_wxzvp"
   },
   "outputs": [],
   "source": [
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "vS6MLAsnxzvt"
   },
   "source": [
    "And finally compare it to what we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ThjHJ_Xvxzvt"
   },
   "outputs": [],
   "source": [
    "(N * (N-1) / 2) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "2ur7GMcXxzvy"
   },
   "source": [
    "#### Sensitivity / Specificity Tradeoff\n",
    "\n",
    "As with any adjustment of p-value cutoff, we have a tradeoff. A lower cutoff decreases the rate of false positives, but also decreases the chance we find a real relationship (true positive). So you can't just decrease your cutoff to solve this problem.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
    "\n",
    "#### Reducing Multiple Comparisons Bias\n",
    "\n",
    "You can't really eliminate multiple comparisons bias, but you can reduce how much it impacts you. To do so we have two options.\n",
    "\n",
    "#### Option 1: Run fewer tests.\n",
    "\n",
    "This is often the best option. Rather than just sweeping around hoping you hit an interesting signal, use your expert knowledge of the system to develop a great hypothesis and test that. This process of exploring the data, coming up with a hypothesis, then gathering more data and testing the hypothesis on the new data is considered the gold standard in statistical and scientific research. It's crucial that the data set on which you develop your hypothesis is not the one on which you test it. Because you found the effect while exploring, the test will likely pass and not really tell you anything. What you want to know is how consistent the effect is. Moving to new data and testing there will not only mean you only run one test, but will be an 'unbiased estimator' of whether your hypothesis is true. We discuss this a lot in other lectures.\n",
    "\n",
    "#### Option 2: Adjustment Factors and Bon Ferroni Correction\n",
    "\n",
    "##### WARNING: This section gets a little technical. Unless you're comfortable with significance levels, we recommend looking at the code examples first and maybe reading the linked articles before fully diving into the text.\n",
    "\n",
    "If you must run many tests, try to correct your p-values. This means applying a correction factor to the cutoff you desire to obtain the one actually used when determining whether p-values are significant. The most conservative and common correction factor is Bon Ferroni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "A7FsRDU1xzv0"
   },
   "source": [
    "#### Example: Bon Ferroni Correction\n",
    "\n",
    "The concept behind Bon Ferroni is quite simple. It just says that if we run $m$ tests, and we have a significance level/cutoff of $a$, then we should use $a/m$ as our new cutoff when determining significance. The math works out because of the following.\n",
    "\n",
    "Let's say we run $m$ tests. We should expect to see $ma$ false positives based on random chance that pass out cutoff. If we instead use $a/m$ as our cutoff, then we should expect to see $ma/m = a$ tests that pass our cutoff. Therefore we are back to our desired false positive rate of $a$.\n",
    "\n",
    "Let's try it on our experiment above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "cfP0gKVuxzv3"
   },
   "outputs": [],
   "source": [
    "num_experiments = 100\n",
    "\n",
    "results = np.zeros((num_experiments,))\n",
    "\n",
    "N = 20\n",
    "\n",
    "T = 100\n",
    "\n",
    "desired_level = 0.05\n",
    "\n",
    "num_tests = N * (N - 1) / 2\n",
    "\n",
    "new_cutoff = desired_level / num_tests\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    # Run a single experiment\n",
    "    result = do_experiment(20, 100, cutoff=new_cutoff)\n",
    "    \n",
    "    # Count how many pairs\n",
    "    n = len(result)\n",
    "    \n",
    "    # Add to array\n",
    "    results[i] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "NK-xrphVxzv6"
   },
   "outputs": [],
   "source": [
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Lr2cGqZ4xzv8"
   },
   "source": [
    "As you can see, our number of significant results is now far lower on average. Which is good because the data was random to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "z1N2KYfSxzv8"
   },
   "source": [
    "#### These are Often Overly Conservative\n",
    "\n",
    "Because Bon Ferroni is so stringent, you can often end up passing over real relationships. There is a good example in the following article\n",
    "\n",
    "https://en.wikipedia.org/wiki/Multiple_comparisons_problem\n",
    "\n",
    "Effectively, it assumes that all the tests you are running are independent, and doesn't take into account any structure in your data. You may be able to design a more finely tuned correction factor, but this is adding a layer of complexity and therefore a point of failure to your research. In general any time you relax your stringency, you need to be very careful not to make a mistake.\n",
    "\n",
    "Because of the over-zealousness of Bon Ferroni, often running fewer tests is the better option. Or, if you must run many tests, reserve multiple sets of data so your candidate signals can undergo an out-of-sample round of testing. For example, you might have the following flow:\n",
    "\n",
    " * Let's say there are 100,000 possible relationships.\n",
    " * Run a test on each possible relationship, and pick those that passed the test.\n",
    " * With these candidates, run a test on a new out-of-sample set of data. Because you have many fewer candidates, you can now apply a Bon Ferroni correction to these p-values, or if necessary repeat another round of out-of-sample testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "qKGUNdSixzv9"
   },
   "source": [
    "#### What is p-Hacking?\n",
    "\n",
    "p-hacking is just intentional or accidental abuse of multiple comparisons bias. It is surprisingly common, even in academic literature. The excellent statistical news website FiveThirtyEight has a great visualization here:\n",
    "\n",
    "https://fivethirtyeight.com/features/science-isnt-broken/\n",
    "\n",
    "Wikipedia's article is also informative:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Data_dredging\n",
    "\n",
    "In general, the concept is simple. By running many tests or experiments and then focusing only on the ones that worked, you can present false positives as real results. Keep in mind that this also applies to running many different models or different types of experiments and on different data sets. Imagine that you spend a summer researching a new model to forecast corn future prices. You try 50 different models until finally one succeeds. Is this just luck at this point? Certainly you would want to be more careful about validating that model and testing it out-of-sample on new data before believing that it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "1f8Irg3Exzv9"
   },
   "source": [
    "#### Final Notes\n",
    "\n",
    "#### You can never eliminate, only reduce risk.\n",
    "\n",
    "In general you can never completely eliminate multiple comparisons bias, you can only reduce the risk of false positives using techniques we described above. At the end of the day most ideas tried in research don't work, so you'll end up testing many different hypotheses over time. Just try to be careful and use common sense about whether there is sufficient evidence that a hypothesis is true, or that you just happened to get lucky on this iteration.\n",
    "\n",
    "### Use Out-of-Sample Testing\n",
    "\n",
    "As mentioned above, out-of-sample testing is one of the best ways to reduce your risk. You should always use it, no matter the circumstances. Often one of the ways that false positives make it through your workflow is a lack of an out-of-sample test at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "V9Ux0psnxzv-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "dHjtiZrixzwA"
   },
   "source": [
    "### Singular Value Decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "to be included: https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491\n",
    "https://en.wikipedia.org/wiki/Singular_value_decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "WbwUc2ymxzwB"
   },
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "PH1EI6T_xzwC"
   },
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "2mDdqYKaxzwC"
   },
   "source": [
    "# 3. Preferred Statistical Characteristics of Data <a id=\"10\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "NhBGjfvpxzwD"
   },
   "source": [
    "You do not want to consume someone else’s processed dataset, as the likely outcome will be that you discover what someone else already knows or will figure out soon. Ideally your starting point is a collection of unstructured, raw data that you are going to process in a way that will lead to informative features.\n",
    "\n",
    "1. **Fundamental data:** is extremely regularized and low frequency. Being so accessible to the marketplace, it is rather unlikely that there is much value left to be exploited.\n",
    "1. **Market Data:** a raw feed, with all sorts of unstructured information, that even allows you to reconstruct the trading book, or the full collection of BWIC (bids wanted in competition). (Every market participant leaves a characteristic footprint in the trading records, and with enough patience, you will find a way to anticipate a competitor’s next move. For example, TWAP algorithms leave a very particular footprint that is used by predatory algorithms to front-run their end-of-day trading (usually hedging) activity.)\n",
    "1. **Analytics:** *derivative data* --> not readily available from an original source (e.g. in-depth analyses of companies’ business models, activities, competition, outlook, etc. Sentiment extracted from news reports... --> costly, the methodology used in their production may be biased or opaque, and you will not be the sole consumer.\n",
    "1. **Alternative Data:** What truly characterizes alternative data is that it is primary information, that is, information that has not made it to the other sources. e.g. : before Exxon Mobile reported increased earnings, there were movements of tankers and drillers and pipeline traffic. \n",
    "\n",
    "\n",
    "**Our todays objective:** learn how to produce a continuous, homogeneous, and structured dataset from a collection of unstructured financial data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "uu_99BfWxzwD"
   },
   "source": [
    "### BARS\n",
    "\n",
    "Most ML algorithms assume a table representation of the extracted data. Finance practitioners often refer to those tables’ rows as 'bars'.\n",
    "\n",
    "1. **Time Bars:** most popular among practitioners and academics, but they should be avoided for two reasons. First, markets do not process information at a constant time interval. The hour following the open is much more active than the hour around noon. Second, time-sampled series often exhibit poor statistical properties, like serial correlation, heteroscedasticity, and non-normality of returns. \n",
    "1. **Tick Bars:** The sample variables (timestamp, VWAP, open price, etc.) will be extracted each time a pre-defined number of transactions takes place, e.g., 1,000 ticks. This allows us to synchronize sampling with a proxy of information arrival (the speed at which ticks are originated). Mandelbrot and Taylor [1967] were among the first to realize that sampling as a function of the number of transactions exhibited desirable statistical properties: “Price changes over a fixed number of transactions may have a Gaussian distribution. Price changes over a fixed time period may follow a stable Paretian distribution, whose variance is infinite.\n",
    "1. **Volume Bars:** One problem with tick bars is that order fragmentation introduces some arbitrariness in the number of ticks. For example, suppose that there is one order sitting on the offer, for a size of 10. If we buy 10 lots, our one order will be recorded as one tick. If instead on the offer there are 10 orders of size 1, our one buy will be recorded as 10 separate transactions. After volume started to be reported as well, Clark [1973] realized that sampling returns by volume achieved even better statistical properties (i.e., closer to an IID Gaussian distribution) than sampling by tick bars.\n",
    "1. **Dollar Bars**: Dollar bars are formed by sampling an observation every time a pre-defined mar- ket value is exchanged. The number of shares traded is a function of the actual value exchanged. Therefore, it makes sense sampling bars in terms of dollar value exchanged, rather than ticks or volume, particularly when the analysis involves significant price fluctuations\n",
    "*(volume or dollar bars' volatilities are much closer to constant (homoscedasticity)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "SfvdyFeSxzwG"
   },
   "source": [
    "<a ><img src=\"Pictures/normality.png\" Width=\"700\" align=\"center\"> </a>\n",
    "\n",
    "\n",
    "Partial recovery of normality through a price sampling process subordinated to a volume, tick and dollar clock.<br >\n",
    " *(Normal Distribution is often an assumption for further calculations, e.g. Maximum Likelyhood Estimators for simple GARCH Models)*\n",
    "\n",
    "how to convert a time series to dollar bars as Pandas Dataframe: \n",
    "\n",
    "```js\n",
    "\n",
    "#!pip install mlfinlab\n",
    "\n",
    "get_volume_bars(file_path_or_df: Union[str, Iterable[str], pandas.core.frame.DataFrame], threshold: Union[float, pandas.core.series.Series] = 70000000, batch_size: int = 20000000, verbose: bool = True, to_csv: bool = False, output_path: Optional[str] = None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "s1TEhfQ_xzwH"
   },
   "source": [
    "## Skewness and Kurtosis <a id=\"11\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "pse7bQmgxzwI"
   },
   "source": [
    "Sometimes mean and variance are not enough to describe a distribution. When we calculate variance, we square the deviations around the mean. In the case of large deviations, we do not know whether they are likely to be positive or negative. This is where the skewness and symmetry of a distribution come in. A distribution is <i>symmetric</i> if the parts on either side of the mean are mirror images of each other. For example, the normal distribution is symmetric. The normal distribution with mean $\\mu$ and standard deviation $\\sigma$ is defined as\n",
    "$$ f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "knZ3jKa6xzwI"
   },
   "source": [
    "### Skewness\n",
    "A distribution which is not symmetric is called <i>skewed</i>. For instance, a distribution can have many small positive and a few large negative values (negatively skewed) or vice versa (positively skewed), and still have a mean of 0. A symmetric distribution has skewness 0. Positively skewed unimodal (one mode) distributions have the property that mean > median > mode. Negatively skewed unimodal distributions are the reverse, with mean < median < mode. All three are equal for a symmetric unimodal distribution.\n",
    "\n",
    "The explicit formula for skewness is:\n",
    "$$ S_K = \\frac{n}{(n-1)(n-2)} \\frac{\\sum_{i=1}^n (X_i - \\mu)^3}{\\sigma^3} $$\n",
    "\n",
    "Where $n$ is the number of observations, $\\mu$ is the arithmetic mean, and $\\sigma$ is the standard deviation. The sign of this quantity describes the direction of the skew as described above. We can plot a positively skewed and a negatively skewed distribution to see what they look like. For unimodal distributions, a negative skew typically indicates that the tail is fatter on the left, while a positive skew indicates that the tail is fatter on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "5E_0bxuNxzwI"
   },
   "source": [
    "### Kurtosis\n",
    "\n",
    "Kurtosis attempts to measure the shape of the deviation from the mean. Generally, it describes how peaked a distribution is compared the the normal distribution, called mesokurtic. All normal distributions, regardless of mean and variance, have a kurtosis of 3. A leptokurtic distribution (kurtosis > 3) is highly peaked and has fat tails, while a platykurtic distribution (kurtosis < 3) is broad. Sometimes, however, kurtosis in excess of the normal distribution (kurtosis - 3) is used, and this is the default in `scipy`. A leptokurtic distribution has more frequent large jumps away from the mean than a normal distribution does while a platykurtic distribution has fewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "Ij2wnMmxxzwJ"
   },
   "source": [
    "The formula for kurtosis is\n",
    "$$ K = \\left ( \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\frac{\\sum_{i=1}^n (X_i - \\mu)^4}{\\sigma^4} \\right ) $$\n",
    "\n",
    "while excess kurtosis is given by\n",
    "$$ K_E = \\left ( \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\frac{\\sum_{i=1}^n (X_i - \\mu)^4}{\\sigma^4} \\right ) - \\frac{3(n-1)^2}{(n-2)(n-3)} $$\n",
    "\n",
    "For a large number of samples, the excess kurtosis becomes approximately\n",
    "\n",
    "$$ K_E \\approx \\frac{1}{n} \\frac{\\sum_{i=1}^n (X_i - \\mu)^4}{\\sigma^4} - 3 $$\n",
    "\n",
    "Since above we were considering perfect, continuous distributions, this was the form that kurtosis took. However, for a set of samples drawn for the normal distribution, we would use the first definition, and (excess) kurtosis would only be approximately 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "YNacw7L7xzwJ"
   },
   "source": [
    "<a ><img src=\"Pictures/Kurtosis.jpg\" Width=\"850\" align=\"center\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "6TCsYvjZxzwJ"
   },
   "source": [
    "## Stationarity <a id=\"12\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "eShse07zxzwK"
   },
   "source": [
    "### Stationarity/Non-Stationarity\n",
    "\n",
    "A commonly untested assumption in time series analysis is the stationarity of the data. Data are stationary when the parameters of the data generating process do not change over time. \n",
    "\n",
    "*Mean of a time series $x_t$ is $E(x_t)=\\mu(t)$*\n",
    "\n",
    "*Variance of a time series $x_t$ is $\\sigma^2(t)=E[(x_t - \\mu(t))^2]$*\n",
    "\n",
    "**A time series is stationary in the mean if $\\mu(t)=\\mu$, i.e.mean is constant with time**\n",
    "\n",
    "**A time series is stationary in the variance if $\\sigma^2(t)=\\sigma^2$, i.e. variance is constant with time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "oI6VcdRnxzwL"
   },
   "source": [
    "<a ><img src=\"Pictures/stationarity.png\" Width=\"600\" align=\"center\"> </a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "ij5Fx7c2xzwM"
   },
   "source": [
    "#### Why Non-Stationarity is Dangerous\n",
    "\n",
    "Many statistical tests, deep down in the fine print of their assumptions, require that the data being tested are stationary. A stationary time series (TS) is simple to predict as we can assume that future statistical properties are the same or proportional to current statistical properties.If you naively use certain statistics on a non-stationary data set, you will get garbage results. As an example, let's take an average through our non-stationary $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ogOEIXDQxzwM"
   },
   "outputs": [],
   "source": [
    "# Set the number of datapoints\n",
    "T = 100\n",
    "\n",
    "B = pd.Series(index=range(T))\n",
    "B.name = 'B'\n",
    "\n",
    "for t in range(T):\n",
    "    # Now the parameters are dependent on time\n",
    "    # Specifically, the mean of the series changes over time\n",
    "    params = (t * 0.1, 1)\n",
    "    B[t] = generate_datapoint(params)\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(B)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(['Series B'])\n",
    "\n",
    "\n",
    "m = np.mean(B)\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(B)\n",
    "plt.hlines(m, 0, len(B), linestyles='dashed', colors='r')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(['Series B', 'Mean'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "SEkYfBWqxzwO"
   },
   "source": [
    "The computed mean will show the mean of all data points till date, but won't be useful for any forecasting of future state. It's meaningless when compared with any specfic time, as it's a collection of different states at different times mashed together. This is just a simple and clear example of why non-stationarity can screw with analysis, much more subtle problems can arise in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "9QZl9SeSxzwP"
   },
   "source": [
    "#### Testing for Stationarity\n",
    "\n",
    "Now we want to check for stationarity using a statistical test. We performthe standard [Augmented Dickey Fuller test](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "L7B5mJPkxzwP"
   },
   "outputs": [],
   "source": [
    "def check_for_stationarity(X, cutoff=0.01):\n",
    "    # We must observe significant p-value to convince ourselves that the series is stationary\n",
    "    pvalue = adfuller(X)[1]\n",
    "    if pvalue < cutoff:\n",
    "        print('p-value = ' + str(pvalue) + ' The series ' + X.name +' is likely stationary.')\n",
    "        return True\n",
    "    else:\n",
    "        print('p-value = ' + str(pvalue) + ' The series ' + X.name +' is likely non-stationary.')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "_f9eJDtsxzwT"
   },
   "outputs": [],
   "source": [
    "check_for_stationarity(B);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "iBsA5X77xzwa"
   },
   "source": [
    "A cyclic movement of the mean will be very difficult to tell apart from random noise. In practice on noisy data and limited sample size it can be hard to determine if a series is stationary and whether any drift is random noise or part of a trend. In each individual case the test may or may not pick up subtle effects like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "zGHJbIZFxzwa"
   },
   "outputs": [],
   "source": [
    "check_for_stationarity(C);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "7PBv8LlYxzwc"
   },
   "source": [
    "Let's try this out on some real pricing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "x_sbrUzzxzwc"
   },
   "outputs": [],
   "source": [
    "from backtester.dataSource.yahoo_data_source import YahooStockDataSource\n",
    "\n",
    "\n",
    "startDateStr = '2007/01/01'\n",
    "endDateStr = '2015/01/01'\n",
    "cachedFolderName = '../cointegration/'\n",
    "dataSetId = 'Data'\n",
    "instrumentIds = ['AAPL']\n",
    "ds = YahooStockDataSource(cachedFolderName=cachedFolderName,\n",
    "                            dataSetId=dataSetId,\n",
    "                            instrumentIds=instrumentIds,\n",
    "                            startDateStr=startDateStr,\n",
    "                            endDateStr=endDateStr,\n",
    "                            event='history')\n",
    "X = ds.getBookDataByFeature()['adjClose']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ZAqsbat4xzwe"
   },
   "outputs": [],
   "source": [
    "check_for_stationarity(X['AAPL']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "QpiaiAfjxzwg"
   },
   "outputs": [],
   "source": [
    "plt.plot(X.index, X.values)\n",
    "plt.ylabel('Price')\n",
    "plt.legend(X.columns.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "5D74poDcxzwj"
   },
   "source": [
    "Now let's take the delta of the series, giving us the additive returns. We'll check if this is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "RIVRxjJUxzwj"
   },
   "outputs": [],
   "source": [
    "X1 = X['AAPL'].diff()[1:]\n",
    "X1.name = (str(X.columns.values) + ' Additive Returns')\n",
    "check_for_stationarity(X1)\n",
    "plt.plot(X1.index, X1.values)\n",
    "plt.ylabel('Additive Returns')\n",
    "plt.legend(X.columns.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "J9BqbJ3Jxzwp"
   },
   "source": [
    "Seems like the additive returns are stationary. That means we will probably be able to model the returns much better than the price. It also means that the price was $I(1)$.\n",
    "\n",
    "Let's also check the multiplicative returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ZiSPYrDIxzwp"
   },
   "outputs": [],
   "source": [
    "X1 = X['AAPL'].pct_change()[1:]\n",
    "X1.name = str(X.columns.values) + ' Multiplicative Returns'\n",
    "check_for_stationarity(X1)\n",
    "plt.plot(X1.index, X1.values)\n",
    "plt.ylabel('Multiplicative Returns')\n",
    "plt.legend([X1.name])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "hG0RiF3nxzwr"
   },
   "source": [
    "Seems like the multiplicative returns are also stationary. Both the multiplicative and additive deltas on a series get at similar pieces of information, so it's not surprising both are stationary. In practice this might not always be the case.\n",
    "\n",
    "##### IMPORTANT NOTE\n",
    "\n",
    "As always, you should not naively assume that because a time series is stationary in the past it will continue to be stationary in the future. Tests for consistency of stationarity such as cross validation and out of sample testing are necessary. This is true of any statistical property, we just reiterate it here. Returns may also go in and out of stationarity, and may be stationary or non-stationary depending on the timeframe and sampling frequency.\n",
    "\n",
    "\n",
    "\n",
    "##### Note: Returns Analysis\n",
    "\n",
    "The reason returns are usually used for modeling in quantitive finance is that they are far more stationary than prices. This makes them easier to model and returns forecasting more feasible. Forecasting prices is more difficult, as there are many trends induced by their $I(1)$ integration. Even using a returns forecasting model to forecast price can be tricky, as any error in the returns forecast will be magnified over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "-0WjV-2rxzws"
   },
   "source": [
    "Cointegration is the trick that makes regression work on non-stationary series, so that memory is preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "316ShvIAxzws"
   },
   "source": [
    "## Integration  <a id=\"13\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "b5QuawArxzwt"
   },
   "source": [
    "### Moving Average Representation\n",
    "\n",
    "An important concept in time series analysis is moving average representation. We will discuss this briefly here, but a more complete explanation is available in the ARMA and ARIMA Backup. Also check Wikipedia as listed below.\n",
    "\n",
    "This representation expresses any time series $Y_t$ as \n",
    "\n",
    "$$Y_t = \\sum_{j=0}^\\infty b_j \\epsilon_{t-j} + \\eta_t$$\n",
    "\n",
    "* $\\epsilon$ is the residuals or errors - a stochastic white noise process\n",
    "* $b_j$ are the moving average weights of residuals\n",
    "* $\\eta$ is a deterministic series\n",
    "\n",
    "$\\eta$ is deterministic (such as a sine wave),something we could perfectly model it. The difference between predictions from this model ($\\eta$) and actual observations leads to residuals($\\epsilon$). The residuals are stochastic and there to simulate new information occuring over time. \n",
    "\n",
    "Specifically, $\\epsilon_t = \\hat Y_t - Y_t$ where $\\hat Y_t$ is the in the optimal forecast of $Y_t$(actual observed value) using only information from time before $t$. In other words, the best prediction you can make at time $t-1$ cannot account for the randomness in $\\epsilon$.\n",
    "\n",
    "Each $b_j$ just says how much previous values of $\\epsilon$ influence $Y_t$.\n",
    "\n",
    "\n",
    "### Back to Order of Integration\n",
    "\n",
    "We will note integration order-i as $I(i)$.\n",
    "\n",
    "A time series is said to be $I(0)$ if the following condition holds in a moving average representation. In simpler terms, the autocorrelation of the series decays to 0 sufficiently quickly.\n",
    "\n",
    "$$\\sum_{k=0}^\\infty |b_k|^2 < \\infty$$\n",
    "\n",
    "This property turns out to be true of all stationary series since autocorrelation is 0, but by itself is not enough for stationarity to hold. This means that stationarity implies $I(0)$, but $I(0)$ does not imply stationarity. For more on orders of integration, please see the following links.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Order_of_integration\n",
    "\n",
    "https://en.wikipedia.org/wiki/Wold%27s_theorem\n",
    "\n",
    "### Testing for $I(0)$\n",
    "\n",
    "In practice testing whether the sum of the autocorrelations is finite may not be possible. It is possible in a mathematical derivation, but when we have a finite set of data and a finite number of estimated autocorrelations, the sum will always be finite. Given this difficulty, tests for $I(0)$ rely on stationarity implying the property. If we find that a series is stationary, then it must also be $I(0)$.\n",
    "\n",
    "Let's take our original stationary series A. Because A is stationary, we know it's also $I(0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "JDfP_JL_xzwt",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(A)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(['Series A'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "DODsi4PFxzwv"
   },
   "source": [
    "### Building Up Orders of Integration\n",
    "\n",
    "If one takes an $I(0)$ series and cumulatively sums it (discrete integration), the new series will be $I(1)$. Notice how this is related to the calculus concept of integration. The same relation applies in general, to get $I(n)$ take an $I(0)$ series and iteratively take the cumulative sum $n$ times.\n",
    "\n",
    "Now let's make an $I(1)$ series by taking the cumulative sum of A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "l0U__NUIxzww"
   },
   "outputs": [],
   "source": [
    "A1 = np.cumsum(A)\n",
    "\n",
    "plt.plot(A1)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(['Series A1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "sSD_BlHJxzw4"
   },
   "source": [
    "### Breaking Down Orders of Integration\n",
    "\n",
    "Conversely, to find the order of integration of a given series, we perform the inverse of a cumulative sum, which is the $\\Delta$ or itemwise difference function. Specifically\n",
    "\n",
    "$$(1-L) X_t = X_t - X_{t-1} = \\Delta X$$\n",
    "\n",
    "$$(1-L)^d X_t$$\n",
    "\n",
    "In this case $L$ is the lag operator. Sometimes also written as $B$ for 'backshift'. $L$ fetches the second to last elements in a time series, and $L^k$ fetches the k-th to last elements. So \n",
    "\n",
    "$$L X_t = X_{t-1}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$(1-L) X_t = X_t - X_{t-1}$$\n",
    "\n",
    "A series $Y_t$ is $I(1)$ if the $Y_t - Y_t-1$ is $I(0)$. In other words, if you take an $I(0)$ series and cumulatively sum it, you should get an $I(1)$ series.\n",
    "\n",
    "\n",
    "### Once all the math has settled, remember that any stationary series is $I(0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "7cTlIvVhxzw5"
   },
   "source": [
    "## Cointegration <a id=\"14\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "kblR12KDxzw5"
   },
   "source": [
    "Finally, now that we've discussed stationarity and order of integration, we can discuss cointegration.\n",
    "\n",
    "A linear combination of the time series ($X_1$, $X_2$, $\\dots$, $X_k$) is a new time series $Y$ constructed as follows for any set of real numbers $b_1 \\dots b_k$\n",
    "\n",
    "$$Y = b_1X_1 + b_2X_2 + \\dots + b_kX_k$$\n",
    "\n",
    "For some set of time series ($X_1$, $X_2$, $\\dots$, $X_k$), if all series are $I(1)$, and some linear combination of them is $I(0)$, we say the set of time series is cointegrated.\n",
    "\n",
    "For example, $X_1$, $X_2$, and $X_3$ are all $I(1)$, and $2X_1 + X_2 + 0X_3 = 2X_1 + X_2$ is $I(0)$. In this case the time series are cointegrated.\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "The intuition here is that for some linear combination of the series, the result lacks much auto-covariance and is mostly noise. This is useful for cases such as pairs trading, in which we find two assets whose prices are cointegrated. Since the linear combination of their prices $b_1A_1 + b_2A_2$ is noise, we can bet on the relationship $b_1A_1 + b_2A_2$ mean reverting and place trades accordingly. See the Pairs Trading notebook for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "tnIQptrFxzw6"
   },
   "source": [
    "Let's make some data to demonstrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "nO5K7QvGxzw6"
   },
   "outputs": [],
   "source": [
    "# Length of series\n",
    "N = 100\n",
    "\n",
    "# Generate a stationary random X1\n",
    "X1 = np.random.normal(0, 1, N)\n",
    "# Integrate it to make it I(1)\n",
    "X1 = np.cumsum(X1)\n",
    "X1 = pd.Series(X1)\n",
    "X1.name = 'X1'\n",
    "\n",
    "# Make an X2 that is X1 plus some noise\n",
    "X2 = X1 + np.random.normal(0, 1, N)\n",
    "X2.name = 'X2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "yzmwl-W9xzw8"
   },
   "outputs": [],
   "source": [
    "plt.plot(X1)\n",
    "plt.plot(X2)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Series Value')\n",
    "plt.legend([X1.name, X2.name])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "XD11Wb84xzw-"
   },
   "source": [
    "Because $X_2$ is just an $I(1)$ series plus some stationary noise, it should still be $I(1)$. Let's check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "cWF0l-Z_xzw_"
   },
   "outputs": [],
   "source": [
    "Z = X2.diff()[1:]\n",
    "Z.name = 'Z'\n",
    "\n",
    "check_for_stationarity(Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "yZT8Z9sHxzxB"
   },
   "source": [
    "Looks good. Now to show cointegration we'll need to find some linear combination of $X_1$ and $X_2$ that is stationary. We can take $X_2-X_1$. All that's left over should be stationary noise by design. Let's check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ooBcGxPzxzxB"
   },
   "outputs": [],
   "source": [
    "Z = X2 - X1\n",
    "Z.name = 'Z'\n",
    "\n",
    "plt.plot(Z)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Series Value')\n",
    "plt.legend(['Z'])\n",
    "plt.show()\n",
    "\n",
    "check_for_stationarity(Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "iTIm0yohxzxL"
   },
   "source": [
    "### Testing for Cointegration\n",
    "\n",
    "There are a bunch of ways to test for cointegration. This [wikipedia article](https://en.wikipedia.org/wiki/Cointegration) describes some. In general we're just trying to solve for the coefficients $b_1, \\dots b_k$ that will produce an $I(0)$ linear combination. If our best guess for these coefficients does not pass a stationarity check, then we reject the hypothesis that the set is cointegrated. This will lead to risk of Type II errors (false negatives), as we will not exhaustively test for stationarity on all coefficent combinations. However Type II errors are generally okay here, as they are safe and do not lead to us making any wrong forecasts.\n",
    "\n",
    "In practice a common way to do this for pairs of time series is to use linear regression to estimate $\\beta$ in the following model.\n",
    "\n",
    "$$X_2 = \\alpha + \\beta X_1 + \\epsilon$$\n",
    "\n",
    "The idea is that if the two are cointegrated we can remove $X_2$'s depedency on $X_1$, leaving behind stationary noise. The combination $X_2 - \\beta X_1 = \\alpha + \\epsilon$ should be stationary.\n",
    "\n",
    "Let's try on some real data. We'll get prices and plot them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "teUfZcyyxzxM"
   },
   "outputs": [],
   "source": [
    "startDateStr = '2011/01/01'\n",
    "endDateStr = '2017/01/01'\n",
    "cachedFolderName = '../cointegration/'\n",
    "dataSetId = 'Data'\n",
    "instrumentIds = ['MSFT','ADBE']\n",
    "ds = YahooStockDataSource(cachedFolderName=cachedFolderName,\n",
    "                            dataSetId=dataSetId,\n",
    "                            instrumentIds=instrumentIds,\n",
    "                            startDateStr=startDateStr,\n",
    "                            endDateStr=endDateStr,\n",
    "                            event='history')\n",
    "data = ds.getBookDataByFeature()['adjClose']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ik3uH3kFxzxN"
   },
   "outputs": [],
   "source": [
    "X1 = data['MSFT']\n",
    "X2 = data['ADBE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "DhGpMKy7xzxP"
   },
   "outputs": [],
   "source": [
    "plt.plot(X1)\n",
    "plt.plot(X2)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Series Value')\n",
    "plt.legend([X1.name, X2.name])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "3I1spBK0xzxR"
   },
   "source": [
    "Now use linear regression to compute $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "OrmxZxCrxzxR"
   },
   "outputs": [],
   "source": [
    "X1 = sm.add_constant(X1)\n",
    "results = sm.OLS(X2, X1).fit()\n",
    "\n",
    "# Get rid of the constant column\n",
    "X1 = X1['MSFT']\n",
    "\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Eqyq6vYoxzxT"
   },
   "outputs": [],
   "source": [
    "b = results.params['MSFT']\n",
    "Z = X2 - b * X1\n",
    "Z.name = 'Z'\n",
    "\n",
    "plt.plot(Z.index, Z.values)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Series Value')\n",
    "plt.legend([Z.name])\n",
    "plt.show()\n",
    "\n",
    "check_for_stationarity(Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "TbSSpT6Cxzxe"
   },
   "source": [
    "We can see here that the resulting $Z$ was likely stationary over the time frame we looked at. This causes us to accept the hypothesis that our two assets were cointegrated over the same timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "ZodRYufWxzxf"
   },
   "source": [
    "**This is only a forecast!**\n",
    "\n",
    "Remember as with anything else, you should not assume that because some set of assets have passed a cointegration test historically, they will continue to remain cointegrated. You need to verify that consistent behavior occurs, and use various model validation techniques as you would with any model.\n",
    "\n",
    "One of the most important things done in finance is to make many independent bets. Here a quant would find many pairs of assets they hypothesize are cointegrated, and evenly distribute their dollars between them in bets. This only requires more than half of the asset pairs to remain cointegrated for the strategy to work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "kZq0e-lqxzxf"
   },
   "source": [
    "Luckily there are some pre-built tests for cointegration. Here's one. Read up on the [documentation](http://statsmodels.sourceforge.net/devel/_modules/statsmodels/tsa/stattools.html) on your own time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "OjeizjCwxzxg"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import coint\n",
    "\n",
    "coint(X1, X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "K_3aQBMNxzxj"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4. Todays Objective: processed dataset for our next lecture as feather pd <a id=\"15\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "NmWeuv8qxz5R"
   },
   "source": [
    "# 5. APPENDIX <a id=\"16\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details>\n",
    "    \n",
    "<summary>Click to expand!</summary>\n",
    "    \n",
    "  1. <a href=\"#19\">Measures of centrality</a>     \n",
    "  1. <a href=\"#20\">Measures of Dispersion </a>      \n",
    "  1. <a href=\"#21\">Covariance and Correlation</a>     \n",
    "  1. <a href=\"#22\">Confidence Intervals</a>     \n",
    "  1. <a href=\"#23\">Hypothesis Testing</a>     \n",
    "  1. <a href=\"#24\">AR, MA, ARIMA, ARCH and GARCH</a>     \n",
    "  1. <a href=\"#18\">Introduction to Python and NumPy </a>     \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Statistics 101 <a id=\"17\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "PSSrr6vRxz5S"
   },
   "source": [
    "## Measures of centrality <a id=\"19\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "8P7A4VLHxz5S"
   },
   "source": [
    "### Expected Value and Arithmetic mean\n",
    "\n",
    "The expected value of a random variable is the probability-weighted average of all possible values.\n",
    "When these probabilities are equal, the expected value is the same as arithmetic mean, defined as the sum of the observations divided by the number of observations:\n",
    "$$\\mu = \\frac{\\sum_{i=1}^N X_i}{N}$$\n",
    "\n",
    "where $X_1, X_2, \\ldots , X_N$ are our observations.\n",
    "\n",
    "For example, if a dice is rolled repeatedly many times, we expect all numbers from 1 - 6 to show up an equal number of times. So the expected value in rolling a six-sided die is 3.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "mmGl4u86xz5S"
   },
   "source": [
    "When the probabilities of different observations are not equal, i.e a random variable $X$ can take value $X_1$ with probability $p_1$, $X_2$ with probability $p_2$, and so on, the expected value of X is the same as <i>weighted</i> arithmetic mean.\n",
    "The weighted arithmetic mean is defined as\n",
    "$$\\sum_{i=1}^n p_i X_i $$\n",
    "\n",
    "where $\\sum_{i=1}^n p_i = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "LIegmd-zxz5S"
   },
   "source": [
    "Therefore, the expected value is the average of all values obtained you perform the experiment it represents many times. This follows from the law of large numbers - the average of the results obtained from a large number of repetitions of an experiment should be close to the expected value, and will tend to become closer as more trials are performed.\n",
    "\n",
    "#### Some properties of expected values that are handy:\n",
    "* The expected value of a constant is equal to the constant itself $E[c] = c$\n",
    "* The expected value is linear, i.e $E[aX+bY] = aE[X]+bE[Y]$ \n",
    "* If $X \\leq Y$ , then $E[X] \\leq E[Y]$\n",
    "* The expected value not multiplicative, i.e. $E[XY]$ is not necessarily equal to $E[X]E[Y]$. \n",
    "  The amount by which they differ is called the covariance, covered in a later notebook.\n",
    "  $Cov(X,Y)=E[XY]-E[X]E[Y]$\n",
    "  If X and Y are uncorrelated, $Cov(X,Y)=0$\n",
    "\n",
    "e.g. np.mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "CjYy-varxz5S"
   },
   "source": [
    "### Other measures of centrality that are commonly used are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "lIK50Fuvxz5T"
   },
   "source": [
    "### Median\n",
    "\n",
    "Number which appears in the middle of the list when it is sorted in increasing or decreasing order, i.e. the value in $(n+1)/2$ when $n$ is odd and the average of the values in $n/2$ and $(n+2)/2$ positions when $n$ is even. One advantage of using median in describing data compared to the mean is that it is not skewed so much by extremely large or small values\n",
    "\n",
    "The median uses the value that splits the data set in half, but not how much smaller or larger the other values are.\n",
    "e.g. np.median(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "gCoUbKfOxz5T"
   },
   "source": [
    "### Mode\n",
    "\n",
    "Most frequently occuring value in a data set. The mode of a probability distribution is the value x at which its probability distribution function takes its maximum value.\n",
    "e.g. stats.mode(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "kzytC58txz5T"
   },
   "source": [
    "### Geometric mean\n",
    "\n",
    "It is the central tendency of a set of numbers by using the product of their values (as opposed to the arithmetic mean which uses their sum). The geometric mean is defined as the nth root of the product of n numbers:\n",
    "$$ G = \\sqrt[n]{X_1X_1\\ldots X_n} $$\n",
    "\n",
    "for observations $X_i \\geq 0$. We can also rewrite it as an arithmetic mean using logarithms:\n",
    "$$ \\ln G = \\frac{\\sum_{i=1}^n \\ln X_i}{n} $$\n",
    "\n",
    "The geometric mean is always less than or equal to the arithmetic mean (when working with nonnegative observations), with equality only when all of the observations are the same.\n",
    "e.g. stats.gmean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "rSTUt1IIxz5T"
   },
   "source": [
    "If we have stocks returns $R_1, \\ldots, R_T$ over different times, we use the geometric mean to calculate average return $R_G$ so that if the rate of return over the whole time period were constant and equal to $R_G$, the final price of the security would be the same as in the case of returns $R_1, \\ldots, R_T$.\n",
    "$$ R_G = \\sqrt[T]{(1 + R_1)\\ldots (1 + R_T)} - 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Iv6oWKXIxz5T"
   },
   "source": [
    "### Harmonic mean\n",
    "\n",
    "The harmonic mean is less commonly used than the other types of means. It is defined as\n",
    "$$ H = \\frac{n}{\\sum_{i=1}^n \\frac{1}{X_i}} $$\n",
    "\n",
    "As with the geometric mean, we can rewrite the harmonic mean to look like an arithmetic mean. The reciprocal of the harmonic mean is the arithmetic mean of the reciprocals of the observations:\n",
    "$$ \\frac{1}{H} = \\frac{\\sum_{i=1}^n \\frac{1}{X_i}}{n} $$\n",
    "\n",
    "The harmonic mean for nonnegative numbers $X_i$ is always at most the geometric mean (which is at most the arithmetic mean), and they are equal only when all of the observations are equal.\n",
    "The harmonic mean can be used when the data can be naturally phrased in terms of ratios. \n",
    "\n",
    "e.g. stats.gmean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "yxqga1soxz5T"
   },
   "source": [
    "### Point Estimates Can Be Deceiving\n",
    "\n",
    "Means by nature hide a lot of information, as they collapse entire distributions into one number. As a result often 'point estimates' or metrics that use one number, can disguise large programs in your data. You should be careful to ensure that you are not losing key information by summarizing your data, and you should rarely, if ever, use a mean without also referring to a measure of spread.\n",
    "\n",
    "### Underlying Distribution Can be Wrong\n",
    "\n",
    "Even when you are using the right metrics for mean and spread, they can make no sense if your underlying distribution is not what you think it is. For instance, using standard deviation to measure frequency of an event will usually assume normality. Try not to assume distributions unless you have to, in which case you should rigourously check that the data do fit the distribution you are assuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "twts2ixPxz5U"
   },
   "source": [
    "## Measures of Dispersion <a id=\"20\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "3DOSxadVxz5U"
   },
   "source": [
    "### Range\n",
    "\n",
    "Range is simply the difference between the maximum and minimum values in a dataset. Not surprisingly, it is very sensitive to outliers. We'll use `numpy`'s peak to peak (ptp) function for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "D85fkxdexz5U"
   },
   "source": [
    "### Mean Absolute Deviation (MAD)\n",
    "\n",
    "The mean absolute deviation is the average of the distances of observations from the arithmetic mean. We use the absolute value of the deviation, so that 5 above the mean and 5 below the mean both contribute 5, because otherwise the deviations always sum to 0.\n",
    "\n",
    "$$ MAD = \\frac{\\sum_{i=1}^n |X_i - \\mu|}{n} $$\n",
    "\n",
    "where $n$ is the number of observations and $\\mu$ is their mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "ln9_C7rhxz5U"
   },
   "source": [
    "### Variance and standard deviation\n",
    "\n",
    "Instead of using absolute deviations, we can use the squared deviations: **The variance** $\\sigma^2$ is defined as the average of the squared deviations around the mean:\n",
    "$$ \\sigma^2 = \\frac{\\sum_{i=1}^n (X_i - \\mu)^2}{n} $$\n",
    "\n",
    "This is sometimes more convenient than the mean absolute deviation because absolute value is not differentiable, while squaring is smooth, and some optimization algorithms rely on differentiability.\n",
    "\n",
    "**Standard deviation** is simply the square root of the variance, $\\sigma$, and it is the easier of the two to interpret because it is in the same units as the observations.\n",
    "\n",
    "Note that variance is additive while standard deviation is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "EPZ9QfLYxz5U"
   },
   "source": [
    "One way to interpret standard deviation is by referring to Chebyshev's inequality. This tells us that the proportion of samples within $k$ standard deviations (that is, within a distance of $k \\cdot$ standard deviation) of the mean is at least $1 - 1/k^2$ for all $k>1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "G8IF6iZLxz5U"
   },
   "source": [
    "### Semivariance and semideviation\n",
    "\n",
    "Although variance and standard deviation tell us how volatile a quantity is, they do not differentiate between deviations upward and deviations downward. Often, such as in the case of returns on an asset, we are more worried about deviations downward. This is addressed by semivariance and semideviation, which only count the observations that fall below the mean. Semivariance is defined as\n",
    "$$ \\frac{\\sum_{X_i < \\mu} (X_i - \\mu)^2}{n_<} $$\n",
    "where $n_<$ is the number of observations which are smaller than the mean. Semideviation is the square root of the semivariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "2R2fWLSUxz5V"
   },
   "source": [
    "A related notion is target semivariance (and target semideviation), where we average the distance from a target of values which fall below that target:\n",
    "$$ \\frac{\\sum_{X_i < B} (X_i - B)^2}{n_{<B}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "wGGOLQo3xz5V"
   },
   "source": [
    "Standard deviation indicates the amount of variation in a set of data values. A low standard deviation indicates that the data points tend to be close to the expected value, while a high standard deviation indicates that the data points are spread out over a wider range of values.\n",
    " \n",
    "### Some properties of standard deviation that are handy:\n",
    "\n",
    "* The standard deviation of a constant is equal to 0\n",
    "* Standard deviations cannot be added. Therefore, $\\sigma(X+Y)\\neq \\sigma(X) + \\sigma(Y)$\n",
    "* However, variance, can be added. Infact, $\\sigma^2(X+Y) = \\sigma^2(X) + \\sigma^2(Y) + Cov(X,Y)$\n",
    "* If X and Y are uncorrelated,  $Cov(X,Y)=0$ and $\\sigma^2(X+Y) = \\sigma^2(X) + \\sigma^2(Y)$\n",
    "\n",
    "### Volatility\n",
    "\n",
    "If an experiment is performed daily and the results of an experiment on one day do not affect the on their results any other day, daily observation are uncorrelated. If we measure daily standard deviation as $\\sigma_i$ then we can calculate the standard deviation for an year, also called annualized standard deviation as:\n",
    "$$\\sigma_{ann} = \\sqrt{\\sum_{i=1}^T \\sigma_i^2}$$\n",
    "\n",
    "In finance, we sum over all trading days and this annualized standard deviation is called **Volatility**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "l8TPzKV2xz5V"
   },
   "source": [
    "### Example: Sharpe ratio\n",
    "\n",
    "One statistic often used to describe the performance of assets and portfolios is the Sharpe ratio, which measures the additional return per unit additional risk achieved by a portfolio, relative to a risk-free source of return such as Treasury bills:\n",
    "$$R = \\frac{E[r_a - r_b]}{\\sqrt{Var(r_a - r_b)}}$$\n",
    "\n",
    "where $r_a$ is the returns on our asset and $r_b$ is the risk-free rate of return. As with mean and standard deviation, we can compute a rolling Sharpe ratio to see how our estimate changes through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "WIxvwTYpxz5V"
   },
   "outputs": [],
   "source": [
    "def sharpe_ratio(asset, riskfree):\n",
    "    return np.mean(asset - riskfree)/np.std(asset - riskfree)\n",
    "\n",
    "start = '2012-01-01'\n",
    "end = '2015-01-01'\n",
    "# Use an ETF that tracks 3-month T-bills as our risk-free rate of return\n",
    "treasury_ret = get_pricing('BIL', fields='price', start_date=start, end_date=end).pct_change()[1:]\n",
    "pricing = get_pricing('AMZN', fields='price', start_date=start, end_date=end)\n",
    "returns = pricing.pct_change()[1:] # Get the returns on the asset\n",
    "\n",
    "# Compute the running Sharpe ratio\n",
    "running_sharpe = [sharpe_ratio(returns[i-90:i], treasury_ret[i-90:i]) for i in range(90, len(returns))]\n",
    "\n",
    "# Plot running Sharpe ratio up to 100 days before the end of the data set\n",
    "_, ax1 = plt.subplots()\n",
    "ax1.plot(range(90, len(returns)-100), running_sharpe[:-100]);\n",
    "ticks = ax1.get_xticks()\n",
    "ax1.set_xticklabels([pricing.index[i].date() for i in ticks[:-1]]) # Label x-axis with dates\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sharpe Ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "40XkNdwNxz5W"
   },
   "source": [
    "The Sharpe ratio looks rather volatile, and it's clear that just reporting it as a single value will not be very helpful for predicting future values. Instead, we can compute the mean and standard deviation of the data above, and then see if it helps us predict the Sharpe ratio for the next 100 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "E6wkFuPSxz5W"
   },
   "outputs": [],
   "source": [
    "# Compute the mean and std of the running Sharpe ratios up to 100 days before the end\n",
    "mean_rs = np.mean(running_sharpe[:-100])\n",
    "std_rs = np.std(running_sharpe[:-100])\n",
    "\n",
    "# Plot running Sharpe ratio\n",
    "_, ax2 = plt.subplots()\n",
    "ax2.set_xticklabels([pricing.index[i].date() for i in ticks[:-1]]) # Label x-axis with dates\n",
    "ax2.plot(range(90, len(returns)), running_sharpe)\n",
    "\n",
    "# Plot its mean and the +/- 1 standard deviation lines\n",
    "ax2.axhline(mean_rs)\n",
    "ax2.axhline(mean_rs + std_rs, linestyle='--')\n",
    "ax2.axhline(mean_rs - std_rs, linestyle='--')\n",
    "\n",
    "# Indicate where we computed the mean and standard deviations\n",
    "# Everything after this is 'out of sample' which we are comparing with the estimated mean and std\n",
    "ax2.axvline(len(returns) - 100, color='pink');\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.legend(['Sharpe Ratio', 'Mean', '+/- 1 Standard Deviation'])\n",
    "\n",
    "print 'Mean of running Sharpe ratio:', mean_rs\n",
    "print 'std of running Sharpe ratio:', std_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "TAHXYsylxz5X"
   },
   "source": [
    "The standard deviation in this case is about a quarter of the range, so this data is extremely volatile. Taking this into account when looking ahead gave a better prediction than just using the mean, although we still observed data more than one standard deviation away. We could also compute the rolling mean of the Sharpe ratio to try and follow trends; but in that case, too, we should keep in mind the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "FbfOMtLJxz5X"
   },
   "source": [
    "#### These are Only Estimates\n",
    "\n",
    "It is important to remember that when we are working with a subset of actual data, these computations will only give you sample statistics, that is mean and standard deviation of a sample of data. Whether or not this reflects the current true population mean and standard deviation is not always obvious, and more effort has to be put into determining that. This is especially problematic in finance because all data are time series and the mean and variance may change over time. In general do not assume that because something is true of your sample, it will remain true going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Olg2fCg6xz5X"
   },
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "EMB2PbfDxz5X"
   },
   "source": [
    "## Covariance and Correlation <a id=\"21\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "l7XWOZAzxz5X"
   },
   "source": [
    "### Covariance\n",
    "\n",
    "Covariance measures the extent to which the relationship between two variables is linear. The sign of the covariance shows the trend in the linear relationship between the variables, i.e if they tend to move together or in separate directions. A positive sign indicates that the variables are directly related, i.e. when one increases the other one also increases. A negative sign indicates that the variables are inversely related, so that when one increases the other decreases. It is calculated as\n",
    "$$Cov(X,Y) = E[XY] - E[X]E[Y] = E[(X- E[X])(Y-E[Y])]$$\n",
    "\n",
    "Note that\n",
    "$$Cov(X,X) = E[X^2] - E[X]^2 = E[(X- E[X])^2] = \\sigma^2 $$\n",
    "\n",
    "When the two variables are identical, covariance is same as  variance.\n",
    "\n",
    "##### Covariance isn't that meaningful by itself\n",
    "\n",
    "What does this mean? To make better sense of data, we introduce correlation\n",
    "\n",
    "\n",
    "\n",
    "### Correlation \n",
    "\n",
    "\n",
    "The correlation coefficient measures the extent to which the relationship between two variables is linear. Its value is always between -1 and 1. A positive coefficient indicates that the variables are directly related, i.e. when one increases the other one also increases. A negative coefficient indicates that the variables are inversely related, so that when one increases the other decreases. The closer to 0 the correlation coefficient is, the weaker the relationship between the variables.\n",
    "\n",
    "The correlation coefficient of two series $X$ and $Y$ is defined as\n",
    "$$r = \\frac{Cov(X,Y)}{std(X)std(Y)}$$\n",
    "\n",
    "So now what? What does this mean? Correlation uses information about the variance of X and Y to normalize this metric. Once we've normalized the metric to the -1 to 1 scale, we can make meaningful statements and compare correlations.\n",
    "\n",
    "To see how this is done consider the formula.\n",
    "\n",
    "$$\\frac{Cov(X, Y)}{std(X)std(Y)}$$\n",
    "\n",
    "$$= \\frac{Cov(X, Y)}{\\sqrt{var(X)}\\sqrt{var(Y)}}$$\n",
    "\n",
    "$$= \\frac{Cov(X, Y)}{\\sqrt{Cov(X, X)}\\sqrt{Cov(Y, Y)}}$$\n",
    "\n",
    "where $Cov$ is the covariance and $std$ is the standard deviation.\n",
    "\n",
    "Two random sets of data will have a correlation coefficient close to 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "dflbkf6Oxz5Y"
   },
   "source": [
    "### Correlation vs. Covariance\n",
    "\n",
    "Correlation is simply a normalized form of covariance. They are otherwise the same and are often used semi-interchangeably in everyday conversation. It is obviously important to be precise with language when discussing the two, but conceptually they are almost identical.\n",
    "\n",
    "#### Covariance isn't that meaningful by itself\n",
    "\n",
    "Let's say we have two variables $X$ and $Y$ and we take the covariance of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "azSv-Bdlxz5Y"
   },
   "source": [
    "### Why do both `np.cov` and `np.corrcoef` return matrices?\n",
    "\n",
    "The covariance matrix is an important concept in statistics. Often people will refer to the covariance of two variables $X$ and $Y$, but in reality that is just one entry in the covariance matrix of $X$ and $Y$. For each input variable we have one row and one column. The diagonal is just the variance of that variable, or $Cov(X, X)$, entries off the diagonal are covariances between different variables. The matrix is symmetric across the diagonal. Let's check that this is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "rLQRecWYxz5Y"
   },
   "source": [
    "### How is this useful in finance?\n",
    "\n",
    "#### Determining related assets\n",
    "\n",
    "Once we've established that two series are probably related, we can use that in an effort to predict future values of the series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "KGhiJIA_xz5Y"
   },
   "source": [
    "### Constructing a portfolio of uncorrelated assets\n",
    "\n",
    "Another reason that correlation is useful in finance is that uncorrelated assets produce the best portfolios. The intuition for this is that if the assets are uncorrelated, a drawdown in one will not correspond with a drawdown in another. This leads to a very stable return stream when many uncorrelated assets are combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "kiKA2hSaxz5Y"
   },
   "source": [
    "### Limitations\n",
    "\n",
    "#### Significance\n",
    "\n",
    "It's hard to rigorously determine whether or not a correlation is significant, especially when, as here, the variables are not normally distributed. Their correlation coefficient is close to 1, so it's pretty safe to say that the two stock prices are correlated over the time period we use, but is this indicative of future correlation? If we examine the correlation of each of them with the S&P 500, we see that it is also quite high. So, AAPL and LRCX are slightly more correlated with each other than with the average stock.\n",
    "\n",
    "One fundamental problem is that it is easy to datamine correlations by picking the right time period. To avoid this, one should compute the correlation of two quantities over many historical time periods and examine the distibution of the correlation coefficient. More details on why single point estimates are bad will be covered in future lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "IXgoEQ9fxz5Y"
   },
   "source": [
    "## Confidence Intervals <a id=\"22\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "YxzJTU7Dxz5Z"
   },
   "source": [
    "We mentioned in the notebook on Expected Value and Standard Deviation that statistics derived from a sample (data available to us) may differ from true value (population statistic). For example, we want to measure the population mean, but we can only calculate a sample mean. We then want to use the sample mean to estimate the population mean. We use confidence intervals in an attempt to determine how accurately our sample mean estimates the population mean.\n",
    "\n",
    "A confidence interval gives an estimated range of values between which the variable is likely to lie. This range is calculated from a given set of data or from a probability distribution\n",
    "The selection of a confidence level for the interval determines the probability that the confidence interval will contain the value of the variable *over many computations*(read subtelty note below).\n",
    "So, a 95% confidence interval for a variable states that the interval will contain the true population mean 95% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "mASchiyFxz5Z"
   },
   "source": [
    "This still doesn't help, to really get a sense of how our sample mean relates to the population mean we need to compute a standard error. The standard error is a measure of the variance of the sample mean.\n",
    "\n",
    "**IMPORTANT\n",
    "Computing a standard error involves assuming that the way you sample is unbaised, and that the data are normal and independent. If these conditions are violated, your standard error will be wrong. There are ways of testing for this and correcting.**\n",
    "\n",
    "The formula for standard error is.\n",
    "\n",
    "$$SE = \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "Where $\\sigma$ is the sample standard deviation and $n$ is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "XheX8sP7xz5Z"
   },
   "source": [
    "Assuming our data are normally distributed, we can use the standard error to compute our confidence interval. \n",
    "\n",
    "To do this we set the desired confidence level (say 95%) and determine 95% of data lies within a range how many standard deviations of mean for our data's distribution. \n",
    "\n",
    "For example, for a normal distributiom, 95% of the observations lie in a range$[-1.96*SE,1.96*SE]$ around the mean. When the samples are large enough (generally > 30 is taken as a threshold) the Central Limit Theorem applies and normality can be safely assumed; if sample sizes are smaller, a safer approach is to use a $t$-distribution with appropriately specified degrees of freedom. The actual way to compute the values is by using a [cumulative distribution function (CDF)](https://github.com/Auquan/Tutorials/blob/master/Random%20Variables.ipynb). If you need more background on Probability Distributions,  CDFs and inverse CDFs, read about them [here](https://en.wikipedia.org/wiki/Probability_density_function) and [here](https://en.wikipedia.org/wiki/Cumulative_distribution_function). Look [here](https://en.wikipedia.org/wiki/Student%27s_t-distribution) for information on the $t$-distribution. We can check the 95% number using one of the Python functions. \n",
    "\n",
    "NOTE: Be careful when applying the Central Limit Theorem, however, as many datasets in finance are fundamentally non-normal and it is not safe to apply the theorem casually or without attention to subtlety.\n",
    "\n",
    "We can visualize the 95% mass bounds here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "CXrRtPGVxz5Z"
   },
   "source": [
    "Now, rather than reporting our sample mean without any sense of the probability of it being correct, we can compute an interval and be much more confident that the population mean lies in that interval. To do this we take our sample mean $\\mu$ and report $\\left(\\mu-1.96 SE , \\mu+1.96SE\\right)$.\n",
    "\n",
    "This works because assuming normality, that interval will contain the population mean 95% of the time.\n",
    "\n",
    "#### SUBTLETY:\n",
    "Note that it is incorrect to say that \"The true mean lies in a range $\\left(\\mu-1.96 SE , \\mu+1.96SE\\right)$ with 95% probability,\" but unfortunately this is a very common misinterpretation. Rather, the 95% refers instead to the fact that over many computations of a 95% confidence interval, the true value will be in the interval in 95% of the cases (assuming correct calibration of the confidence interval, which we will discuss later). \n",
    "\n",
    "But in fact for a single sample and the single confidence interval computed from it, we have no way of assessing the probability that the interval contains the population mean. \n",
    "\n",
    "In the code below, we generate 100 95% Confidence Intervals around a mean of 0. Notice how for some of them, the mean lies completely outside the interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "CAq7tjg-xz5Z"
   },
   "source": [
    "Note that as your confidence increases, the interval necessarily widens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "pZWKDAAvxz5Z"
   },
   "source": [
    "#### What does this mean?\n",
    "\n",
    "Confidence intervals allow us to set our desired confidence, and then report a range that will likely contain the population mean. The higher our desired confidence, the larger range we report. In general once can never report a single point value, because the probability that any given point is the true population mean is incredibly small. Let's see how our intervals tighten as we change sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "SNLEuUUGxz5Z"
   },
   "source": [
    "--- \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "qlVzveScxz5a"
   },
   "source": [
    "## Hypothesis Testing <a id=\"23\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "KOFxtA6Wxz5a"
   },
   "source": [
    "Statistical inference, the practice of making predictions about a large group based on smaller samples, is traditionally broken into two segments, **estimation** and **hypothesis testing**. Estimation provides values for specific things that you may be interested in, such as mean or variance, with a provided confidence interval. A confidence interval provides a region within which you can expect to find the true value of the parameter you estimated, as an estimation will almost never be exact. Confidence intervals use a set confidence level to choose how wide the interval should be, to achieve a higher confidence, we must report a wider interval. For more information please see the [Confidence Intervals lecture](https://www.quantopian.com/lectures/confidence-intervals) from the Quantopian Lecture Series.\n",
    "\n",
    "For example, we might estimate a sample mean as $100$, with a confidence interval of $90, 110$ at a $95\\%$ confidence level. This doesn't mean that the true population mean is between $90$ and $110$ with $95\\%$ probability, as the true mean is a fixed value and the probability is $100\\%$ or $0\\%$ but we don't know which one. Instead what this means is that over many computations of a $95\\%$ confidence interval assuming underlying assumptions about distributions hold, the population mean will be in the interval $95\\%$ of the time.\n",
    "\n",
    "This gives us an idea of the specific characteristics that an overall population may exhibit, given a sample. Hypothesis testing provides a different focus, detailing a framework for statistical testing of hypothesized values. By making an assertion of what a value should be, you create a testable hypothesis.\n",
    "\n",
    "One thing to keep in mind is that statistical tests are designed such that if all the pre-requisite conditions are true, you should get the right answer about the data a certain percentage of the time. When you accept a hypothesis as true based on a test, that doesn't mean it's definitely true. It just means that you can know the probability you are wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Vjp-taiZxz5a"
   },
   "source": [
    "### The Null and Alternative Hypothesis\n",
    "\n",
    "The first thing we need to introduce is the null hypothesis, commonly written as $H_0$. The null hypothesis is the default case, generally reflecting the current common conception of the world. The alternative hypothesis is the one you are testing.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "The alternative hypothesis $H_A$ is that you own more than 10 pairs of shoes.\n",
    "The null hypothesis $H_0$ is that you do not own more than 10 pairs of shoes.\n",
    "\n",
    "The alternative hypothesis $H_A$ is that eating pizza is related with obesity.\n",
    "The null hypothesis $H_0$ is that it is not.\n",
    "\n",
    "The alternative hypothesis $H_A$ is that microsoft's mean returns > 0.\n",
    "The null hypothesis $H_0$ is that they <= 0.\n",
    "\n",
    "#### Difficulty of Testing\n",
    "\n",
    "Some hypotheses are easier to test than others. For instance the alternative hypothesis, \"I own more than 10 pairs of shoes.\" and the accompanying null hypothesis that you do not, is easily tested by counting the number of pairs you own. However, you will still not get a perfect answer all the time in this case, as there may be measurement error in the counting, albiet quite small.\n",
    "\n",
    "On the other hand, the hypothesis, \"The number of insect species is greater than the number of stars in the universe.\" would be more difficult to test and require lots of data gathering.\n",
    "\n",
    "#### Hypotheses Must be Testable\n",
    "\n",
    "A hypothesis cannot be vague, otherwise how will it be tested. For example, \"Momentum trading is a good way to make money.\" is not really testable. What does 'good' mean? What type of momentum trading are we discussing? Hypotheses should be very specific and the type of test needed should follow quickly from the hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "POmfsk5Jxz5a"
   },
   "source": [
    "#### How to Perform Hypothesis Testing\n",
    "\n",
    "The following are the main steps in hypothesis testing:\n",
    "\n",
    "1. State the hypothesis and the alternative to the hypothesis\n",
    "2. Identify the appropriate test statistic and its distribution. Ensure that any assumptions about the data are met (stationarity, normality, etc.)\n",
    "3. Specify the significance level, $\\alpha$\n",
    "4. From $\\alpha$ and the distribution compute the 'critical value'.\n",
    "5. Collect the data and calculate the test statistic\n",
    "6. Compare test statistic with critical value and decide whether to accept or reject the hypothesis.\n",
    "\n",
    "First we state the hypothesis that we wish to test. We do this by identifying a **null hypothesis** and an **alternative hypothesis**. The null hypothesis, $H_0$, is the one that we want to test, while the alternative hypothesis, $H_A$, is the hypothesis that is accepted in the case where $H_0$ is rejected.\n",
    "\n",
    "Let's say that we want to test whether the mean return of Microsoft stock is positive. The parameter that we are testing is denoted by $\\theta$ and the proposed value of the parameter is denoted by $\\theta_0$, which in this case is equal to $0$. So we say that our $H_0$ is $\\theta = \\theta_0$, that the returns are negative, and our $H_A$ is $\\theta \\neq \\theta_0$. Including this formation, there are three possible ways to formulate null and alternative hypotheses:\n",
    "\n",
    "1. $H_0: \\theta = \\theta_0$ versus $H_A: \\theta \\neq \\theta_0$ (A \"not equal to\" alternative hypothesis)\n",
    "2. $H_0: \\theta \\leq \\theta_0$ versus $H_A: \\theta > \\theta_0$ (A \"greater than\" alternative hypothesis)\n",
    "3. $H_0: \\theta \\geq \\theta_0$ versus $H_A: \\theta < \\theta_0$ (A \"less than\" alternative hypothesis)\n",
    "\n",
    "In this case, where we are testing the returns of MSFT, $\\theta = \\mu_{MSFT}$, representing the stock's mean returns. Since we are testing whether the returns are positive or negative, we have that $\\theta_0 = 0$. Our example follows the first formulation of a hypothesis test. This is a **two-sided hypothesis test** (or **two-tailed hypothesis test**). The second and third formulations are examples of a **one-sided hypothesis test** (or **one-tailed hypothesis test**). With a one-sided test, we reject the null in favor of the alternative only if the data indivates that $\\theta$ is repectively greater than or less than $\\theta_0$. A two-sided test rejects the null in favor of the alternative if the data indicates that $\\theta$ is either greater or less than $\\theta_0$.\n",
    "\n",
    "So if we were to write out our hypothesis for MSFT in more qualitative terms, we would have:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H_0 &:& \\text{The mean return on Microsoft stock is $0$}\\\\\n",
    "H_A &:& \\text{The mean return on Microsoft stock is not $0$}\n",
    "\\end{eqnarray}\n",
    "\n",
    "When forming a hypothesis test, the null and alternative hypothesis must be complementary to each other. Between them they must cover all values of $\\theta$. Regardless of the type of hypothesis test we are performing, we always test the null hypothesis as if $\\theta = \\theta_0$. In the case of either of the one-tailed tests, this will still provide more than enough evidence for us to make a decision. For example, if $H_0: \\theta \\leq 0$, $H_A: \\theta > 0$, and we have enough evidence to reject $H_0: \\theta = 0$ in favor of $H_A: \\theta > 0$, then this holds true for all values less than $0$ as well.\n",
    "\n",
    "The most common type of hypothesis test is the two-tailed, \"not equal to\", hypothesis test, because it presents a neutral view. The one-tailed hypothesis tests are less neutral than the \"not equal to\" test, reflecting the thoughts of the tester. One-tailed tests are often used to test \"hoped for\" results or results that the testers have a prior idea about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "_9oWYw5jxz5a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "M-8QEdwbxz5b"
   },
   "source": [
    "Let's get some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "pQMI2NhVxz5b"
   },
   "outputs": [],
   "source": [
    "start = \"2015-01-01\"\n",
    "end = \"2016-01-01\"\n",
    "pricing_sample = get_pricing(\"MSFT\", start_date = start, end_date = end, fields = 'price')\n",
    "\n",
    "#transform it into returns\n",
    "returns_sample = pricing_sample.pct_change()[1:]\n",
    "\n",
    "# plot it\n",
    "plt.plot(pricing_sample.index, pricing_sample.values)\n",
    "plt.ylabel('Price');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "3joIvusYxz5c"
   },
   "source": [
    "Here are the returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "QXqQ2D77xz5c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(returns_sample.index, returns_sample.values)\n",
    "plt.ylabel('Returns');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "VA5nyndfxz5d"
   },
   "source": [
    "#### Why is this all necessary?\n",
    "\n",
    "Why can't we just take the mean returns of microsoft and check if they're > 0? Because we can't look at the actual data generating process behind the returns, we can only sample returns on some limited time period. Because we only observe a sample, that sample may or may not reflect the true state of the underlying process. Because of this uncertainty we need to use statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "jmBm5sdLxz5d"
   },
   "source": [
    "Next, we identify the appropriate **test statistic** and its probability distribution. A test statistic usually takes the following form:\n",
    "\n",
    "$$ \\text{Test statistic} = \\frac{\\text{Sample statistic} - \\text{Value of the population parameter under $H_0$ ($\\theta_0$)}}{\\text{Standard error of the sample statistic}} $$\n",
    "\n",
    "A test statistic is calculated based on sample data and is compared to its probability distribution to determine whether to reject or not reject the null hypothesis. Since we are testing the mean return on MSFT stock, we can use the sample mean, $\\bar{X}_\\mu$, as our sample statistic. We calculate the standard error of the sample mean as $\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}$ if we know the standard deviation, $\\sigma$, or as $s_{\\bar{X}} = \\frac{s}{\\sqrt{n}}$, where $s$ is the sample standard deviation. So using these definitions, our test statistic can be calculated as:\n",
    "\n",
    "$$ \\frac{\\bar{X}_\\mu - \\theta_0}{s_{\\bar{X}}} = \\frac{\\bar{X}_\\mu - 0}{s/\\sqrt{n}} $$\n",
    "\n",
    "The four most common distributions for test statistics are as follows:\n",
    "\n",
    "* The $t$-distribution ($t$-test)\n",
    "* The standard normal distribution ($z$-test)\n",
    "* The chi-square ($\\chi^2$) distribution ($\\chi^2$-test)\n",
    "* The $F$-distribution ($F$-test)\n",
    "\n",
    "We will cover them in detail later. For now, we will say that we can use a $z$-test with our assumptions in the MSFT example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "8VYq6Wf6xz5d"
   },
   "source": [
    "After we identify the appropriate test statistic and probability distribution, we need to specify the **significance level** of the test, $\\alpha$. The values that we compare our test statistic to in order to reject or fail to reject \n",
    "the $H_0$ are determined based on our $\\alpha$ value. \n",
    "\n",
    "|| True Situation ||\n",
    "| :---: | :---: | :---: |\n",
    "| **Decision** | $H_0$ True | $H_0$ False |\n",
    "| Do not reject $H_0$ | Correct Decision | Type II Error |\n",
    "| Reject $H_0$ (accept $H_A$) | Type I Error | Correct Decision |\n",
    "\n",
    "Our significance level is equal to the probability of a Type I error (a \"false positive\") occuring. The probability of a Type II error (a \"false negative\") occuring is denoted by $\\beta$. If we try to decrease the probability of a Type I error occuring, we increase the probability of a Type II error occuring, resulting in a tradeoff. The only way to reduce the probability of both a Type I and a Type II error occuring is to increase the sample size.\n",
    "\n",
    "The conventional significance levels are $0.1$, $0.05$, and $0.01$. Rejecting the null at $0.1$ mean that we have some evidence null is false, $0.05$ means we have strong evidence null is false, rejecting at $0.01$ we have very strong evidence that null is false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "tkdMlO9bxz5e"
   },
   "source": [
    "#### Critical Value\n",
    "\n",
    "Now we figure out our critical value, or, rejection point. The critical value for our test statistic is the value that we compare the test statistic to when deciding whether to reject the null hypothesis. If we reject the null, we say that the result is **statistically significant**, while if we fail to reject the null, we say that the result is **not statistically significant**.\n",
    "\n",
    "We compare our test statistic to a **critical value** in order to decide whether to reject or not reject the null hypothesis. The critical value of a test is determined based on the $\\alpha$ of our hypothesis test as well as the chosen distribution. In our case, say that $\\alpha = 0.05$, so our significance level is $0.05$. With a one-sided $z$-test, there are two different ways to see the critical values:\n",
    "\n",
    "* If we test $H_0$: $\\theta \\leq \\theta_0$, $H_A$: $\\theta > \\theta_0$ at $\\alpha = 0.05$, our critical value is $z_{0.05} = 1.645$. So we compare our test statistic and we reject the null hypothesis if $z > 1.645$.\n",
    "* If we test $H_0$: $\\theta \\geq \\theta_0$, $H_A$: $\\theta < \\theta_0$ at $\\alpha = 0.05$, our critical value is $-z_{0.05} = -1.645$. As such, we compare our test statistic and we reject the null hypothesis if $z < -1.645$.\n",
    "\n",
    "A two-sided test is a slightly different situation. Since it is two-sided, there are two rejection points, negative and positive. Our $\\alpha$ is $0.05$, so the total probability of a Type I error must sum to $0.05$. As such, we split $0.05$ in half so that our two rejection points are $z_{0.025}$ and $-z_{0.025}$ for the positive and negative critical values, respectively. For a $z$-test, these values are $1.96$ and $-1.96$. Thus, we reject the null if $z < -1.96$ or if $z > 1.96$. If we find that $-1.96 \\leq z \\leq 1.96$, we fail to reject the null.\n",
    "\n",
    "When conducting a hypothesis test, you can also use a **$p$-value** to determine the result. A $p$-value is the minimum level of significance where you can reject the null hypothesis. Often people will interpret $p$-values as the \"probability that the null hypothesis is false\", but this is misleading. A $p$-value only makes sense when compared to the significance value. If a $p$-value is less than $\\alpha$, we reject the null and otherwise we do not. Lower $p$-values do not make something \"more statistically significant\". A lot of statistical outputs will calculate a $p$-value for you, but it is also possible to calculate it manually. The calculation depends both on your type of hypothesis test and the CDF (covered in the [random variables lecture](https://www.quantopian.com/lectures/random-variables)) of the distribution you are working with. To manually calculate a $p$-value, do the following:\n",
    "\n",
    "* In a 'less than or equal to' hypothesis test, the $p$-value is $1 - CDF(\\text{Test Statistic})$\n",
    "* In a 'greater than or equal to' hypothesis test, the $p$-value is $CDF(\\text{Test Statistic})$\n",
    "* In a 'not equal to' hypothesis test, the $p$-value is $2 * 1 - CDF(|\\text{Test Statistic}|)$\n",
    "\n",
    "Significance values tie very nicely into confidence intervals, which are covered more in-depth in our [confidence intervals lecture](https://www.quantopian.com/lectures/confidence-intervals). A confidence interval provides us with an estimate for a parameter's possible range in values given a certain significance level. For example, if our $99\\%$ confidence interval for the mean of MSFT returns was $(-0.0020, 0.0023)$, that would mean that there was a $99\\%$ chance that the true value of the mean was within that interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "lyG1pW3gxz5e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot a standard normal distribution and mark the critical regions with shading\n",
    "x = np.linspace(-3, 3, 100)\n",
    "norm_pdf = lambda x: (1/np.sqrt(2 * np.pi)) * np.exp(-x * x / 2)\n",
    "y = norm_pdf(x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, sharex=True)\n",
    "ax.plot(x, y)\n",
    "ax.fill_between(x, 0, y, where = x > 1.96)\n",
    "ax.fill_between(x, 0, y, where = x < -1.96)\n",
    "plt.title('Rejection regions for a two-tailed hypothesis test at 95% confidence')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "IgTfwoVdxz5f"
   },
   "source": [
    "Now we collect the relevant data for our test and calculate the test statistic for a two sided, $5\\%$ significance test. Keep in mind that any negative characteristics of the data will negatively affect our hypothesis test and possibly render it invalid. In the case of our test of MSFT returns, we may run into issues of time-period bias, or of look-ahead bias (if we prepare the test incorrectly). As always with historical data, the data that we work with may result in a specific test result that may not hold for the future. We also have to make sure that the data does not include any values that we would not have known during the time period we are testing (though this is more of an issue when comparing multiple things with hypothesis tests).\n",
    "\n",
    "Here we calculate the test statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "CI-9g1bCxz5f"
   },
   "outputs": [],
   "source": [
    "n = len(returns_sample)\n",
    "test_statistic = ((returns_sample.mean() - 0) /\n",
    "                (returns_sample.std()/np.sqrt(n)))\n",
    "print 't test statistic: ', test_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "RWSbk9rqxz5g"
   },
   "source": [
    "In order to make the statistical decision for the test, we compare our test statistic to our critical value. Our test statistic as stated above is between the two critical values for a 95% two-tailed $z$-test so in this example we **fail to reject** our $H_0$, our hypothesis that MSFT returns are **not** $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Z-ZMcp-bxz5g"
   },
   "source": [
    "If we chose instead to determine the result of this hypothesis test with a $p$-value, we would calculate the $p$-value in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "X3oV1qUPxz5g"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "2JKUX3GLxz5h",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_val = 2 * (1 - t.cdf(test_statistic, n - 1))\n",
    "print 'P-value is: ', p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "4uU02K0qxz5i"
   },
   "source": [
    "Because the $p$-value is greater than our significance level, $\\alpha = 0.05$, we **fail to reject** the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "pCXjbKqSxz5i"
   },
   "source": [
    "After we make the statistical decision, we have to translate it into real life. Oftentimes this may be difficult to act upon directly, but the results can have other implications. In the case of our example, we have found that the daily returns of Microsoft in 2015 were not significantly different from $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "Rn7IvhQqxz5i"
   },
   "source": [
    "#### Hypothesis Testing on Means\n",
    "\n",
    "A $z$-distribution, or a standard normal distribution, is an essential probability distribution in finance. We like it when things are normally distributed because it entails many useful properties. On top of this, many fundamental methods require an assumption of normality in order to proceed. However, in most cases a $z$-distribution will be inappropriate for our data. We rarely know the true parameter values (mean and variance) of our data and must rely upon approximations. In these cases, we should use the $t$-distribution, and approximation of the normal distribution. The $t$-distribution is more forgiving when it comes to small sample sizes and is meant to be used with sample mean and variance. It has fatter tails and a lower peak, giving more flexibility compared to a normal distribution.\n",
    "\n",
    "Both the $t$ and $z$-distributions rely upon an underlying assumption of normality, which is typically the case when we are analyzing financial data. As such, in addition to testing individual means, it makes sense to use them to compare between two or more mean values. We can use a hypothesis test to determine whether the means of several data-sets are statistically different from one another. Here, we will use a $t$-distribution to demonstrate. We will compare the mean returns of the S&P500 and Apple stock with a hypothesis test to see if the differences are statistically significant or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "OoqKUzeVxz5i"
   },
   "outputs": [],
   "source": [
    "symbol_list = [\"SPY\", \"AAPL\"]\n",
    "start = '2015-01-01'\n",
    "end = '2016-01-01'\n",
    "pricing_sample = get_pricing(symbol_list, start_date = start, end_date = end, fields='price')\n",
    "pricing_sample.columns = map(lambda x: x.symbol, pricing_sample.columns)\n",
    "returns_sample = pricing_sample.pct_change()[1:]\n",
    "returns_sample.plot()\n",
    "plt.ylabel('Returns');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "JPwxh-13xz5j"
   },
   "source": [
    "While these returns look to have the same mean, we still don't have enough evidence to say for sure. We use a hypothesis test to ground our suspicions in a statistical basis.\n",
    "\n",
    "When comparing two means, our hypothesis tests are stated as the following:\n",
    "\n",
    "1. $H_0: \\mu_1 - \\mu_2 = \\theta_0, \\ H_A: \\mu_1 - \\mu_2 \\neq \\theta_0$\n",
    "2. $H_0: \\mu_1 - \\mu_2 \\leq \\theta_0, \\ H_A: \\mu_1 - \\mu_2 > \\theta_0$\n",
    "3. $H_0: \\mu_1 - \\mu_2 \\geq \\theta_0, \\ H_A: \\mu_1 - \\mu_2 < \\theta_0$\n",
    "\n",
    "Where $\\mu_1, \\mu_2$ are the respective means of SPY and AAPL and $\\theta_0$ is the parameter we are testing. We will use the first hypothesis test to test the equality of the two returns. If we assume that the population variances are equal, our test statistic is calculated as:\n",
    "\n",
    "$$ t = \\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{(\\frac{s_p^2}{n_1} + \\frac{s_p^2}{n_2})^{1/2}} = \\frac{\\bar{X}_1 - \\bar{X}_2}{(\\frac{s_p^2}{n_1} + \\frac{s_p^2}{n_2})^{1/2}}$$\n",
    "\n",
    "With $s_p^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2)}{n_1 + n_2 - 2}$ as the estimator of the common variance, known as the pooled variance, and $n_1 + n_2 - 2$ as the number of degrees of freedom ($n_1 - 1$ and $n_2 - 1$ for each dataset). A typical $t$-test on a mean assumes that all variances involved are equal with underlying normal distributions. If we are assuming that the variances are not equal, we have to calculate our test statistic differently. Our test statistic in this case is:\n",
    "\n",
    "$$ t = \\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^{1/2}} = \\frac{\\bar{X}_1 - \\bar{X}_2}{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^{1/2}}$$\n",
    "\n",
    "Where the number of degrees of freedom used to find the critical statistic is the modified degrees of freedom, the number of values that are free to vary, $df = \\frac{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2}{\\frac{(s_1^2/n_1)^2}{n_1} + \\frac{(s_2^2/n_2)^2}{n_2}}$. This preserves the underlying normality of the data being tested while accounting for differing variances. Calculating the statistic this way removes a lot of problems that can occur if we have unequal variances, especially if the sample sizes of the underlying data differ as well. This specific case of a $t$-test is referred to as [\"Welch's unequal variances $t$-test\"](https://en.wikipedia.org/wiki/Welch%27s_t-test).\n",
    "\n",
    "For this example, we are assuming that the variances of SPY and AAPL returns are different. We think that AAPL will be riskier than SPY so we will use the second formulation of the test statistic. Let's say that $\\alpha = 0.05$ so we are computing a $95\\%$ hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "9KWM2gOzxz5j"
   },
   "outputs": [],
   "source": [
    "# Sample mean values\n",
    "mu_spy, mu_aapl = returns_sample.mean()\n",
    "s_spy, s_aapl = returns_sample.std()\n",
    "n_spy = len(returns_sample['SPY'])\n",
    "n_aapl = len(returns_sample['AAPL'])\n",
    "\n",
    "test_statistic = ((mu_spy - mu_aapl) - 0)/((s_spy**2/n_spy) + (s_aapl**2/n_aapl))**0.5\n",
    "df = ((s_spy**2/n_spy) + (s_aapl**2/n_aapl))**2/(((s_spy**2 / n_spy)**2 /n_spy)+((s_aapl**2 / n_aapl)**2/n_aapl))\n",
    "\n",
    "print 't test statistic: ', test_statistic\n",
    "print 'Degrees of freedom (modified): ', df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "eicr-oy0xz5k"
   },
   "source": [
    "Looking at a [t-table](https://en.wikipedia.org/wiki/Student%27s_t-distribution#Table_of_selected_values), we determine that the critical values for our two-sided hypothesis test are $-1.96$ and $1.96$.  Our test statistic is between these values so we **fail to reject** the null hypothesis and determine that the difference between SPY and AAPL returns is **not** significantly different from $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "ifLoBmY1xz5k"
   },
   "source": [
    "### Hypothesis Testing on Variances\n",
    "\n",
    "If we want to test the variances of populations, we need to use a different distribution from the $t$ and $z$ distributions. Variances must by definition be greater than (or equal to) $0$ and fact that the distributions we have worked with up until now allow negative values makes them unviable as testing distributions. Risk is quantified in terms of standard deviations and variances so this method of hypothesis testing is a handy addition to our financial toolbox.\n",
    "\n",
    "Instead of $t$ and $z$-distributions, we will be working with $\\chi^2$ distributions for single variance tests and $F$ distributions for comparisons of variance. These distributions are bounded below by $0$, making them viable for testing in this manner.\n",
    "\n",
    "Just like with all of our other hypothesis tests, tests of a single variance can take on three forms:\n",
    "\n",
    "1. $H_0: \\sigma^2 = \\sigma_0^2, \\ H_A: \\sigma^2 \\neq \\sigma_0^2$\n",
    "2. $H_0: \\sigma^2 \\leq \\sigma_0^2, \\ H_A: \\sigma^2 > \\sigma_0^2$\n",
    "3. $H_0: \\sigma^2 \\geq \\sigma_0^2, \\ H_A: \\sigma^2 < \\sigma_0^2$\n",
    "\n",
    "The $\\chi^2$ distribution is a family of functions with each different formulation determined by the number of degrees of freedom. The shape of the distribution is different for every different value of the number of degrees of freedom, $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "pOusqtfGxz5k"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "oZKe9fTvxz5l",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here we show what a chi-square looks like\n",
    "x = np.linspace(0, 8, 100)\n",
    "y_1 = chi2.pdf(x, 1)\n",
    "y_2 = chi2.pdf(x, 2)\n",
    "y_3 = chi2.pdf(x, 3)\n",
    "y_4 = chi2.pdf(x, 4)\n",
    "y_6 = chi2.pdf(x, 6)\n",
    "y_9 = chi2.pdf(x, 9)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y_1, label = 'k = 1')\n",
    "ax.plot(x, y_2, label = 'k = 2')\n",
    "ax.plot(x, y_3, label = 'k = 3')\n",
    "ax.plot(x, y_4, label = 'k = 4')\n",
    "ax.plot(x, y_6, label = 'k = 6')\n",
    "ax.plot(x, y_9, label = 'k = 9')\n",
    "ax.legend()\n",
    "plt.title('Chi-Square distribution with k degrees of freedom')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "LLXtg3pkxz5m"
   },
   "source": [
    "We calculate the $\\chi^2$ test statistic as:\n",
    "\n",
    "$$ \\chi^2 = \\frac{(n - 1)s^2}{\\sigma_0^2} $$\n",
    "\n",
    "Where $s^2$ is the sample variance and $n$ is the size of the dataset. The number of degrees of freedom is $n - 1$ and this is used in conjunction with the test statistic to determine the critical value(s) of our $\\chi^2$ hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "2j1ZZRMJxz5m",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = \"2015-01-01\"\n",
    "end = \"2016-01-01\"\n",
    "pricing_sample = get_pricing(\"MSFT\", start_date = start, end_date = end, fields = 'price')\n",
    "returns_sample = pricing_sample.pct_change()[1:]\n",
    "plt.plot(returns_sample.index, returns_sample.values)\n",
    "plt.ylabel('Returns');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "h4gosJKKxz5n"
   },
   "source": [
    "Now we will use a $\\chi^2$-test to test the value of the variance of Microsoft stock. Let's say that we want to use $\\alpha = 0.01$ to test whether the variance of MSFT is less than or equal to $0.0001$ (that the standard deviation, or risk, is less than or equal to $0.01$).\n",
    "\n",
    "$$ H_0: \\sigma^2 \\leq 0.0001, \\ H_A: \\sigma^2 > 0.0001 $$\n",
    "\n",
    "So now we calculate our test statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "sCslHop_xz5n"
   },
   "outputs": [],
   "source": [
    "test_statistic = (len(returns_sample) - 1) * returns_sample.std()**2 / 0.0001\n",
    "print 'Chi-square test statistic: ', test_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "nZdXGjxaxz5o"
   },
   "outputs": [],
   "source": [
    "# Here we calculate the critical value directly because our df is too high for most chisquare tables\n",
    "crit_value = chi2.ppf(0.99, len(returns_sample) - 1)\n",
    "print 'Critical value at a = 0.01 with 251 df: ', crit_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "zhfGqkcLxz5o"
   },
   "source": [
    "Because we are using the 'less than or equal to' formulation of a one-sided hypothesis test, we reject the null hypothesis if our test statistic is greater than the critical value. Since $805.372 > 304.940$, we **reject** the null hypothesis in favor of the alternative and assert that $\\sigma^2 > 0.0001$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "SA4MZBIrxz5p"
   },
   "source": [
    "### Comparing  Two Variances\n",
    "\n",
    "We can compare the variances of two separate things using the $F$ distribution. When constructing a comparison of \n",
    "variances using an $F$-test, the hypothesis formulations are (in case you don't feel like scrolling up):\n",
    "\n",
    "1. $H_0: \\sigma_1^2 = \\sigma_2^2, \\ H_A: \\sigma_1^2 \\neq \\sigma_2^2$\n",
    "2. $H_0: \\sigma_1^2 \\leq \\sigma_2^2, \\ H_A: \\sigma_1^2 > \\sigma_2^2$\n",
    "3. $H_0: \\sigma_1^2 \\geq \\sigma_2^2, \\ H_A: \\sigma_1^2 < \\sigma_2^2$\n",
    "\n",
    "The $F$ distribution is similar to the $\\chi^2$ distribution in that it is asymmetrical and bounded below by $0$. The $F$ distribution is defined with two different values of degrees of freedom. For the purposes of hypothesis testing, each one correlates to one of the factors that we are comparing. An $F$ distribution can be constructed from two separate $\\chi^2$ distributions. $X$ is a $F$ random variable if it can be written as $X = \\frac{Y_1/d_1}{Y_2/d_2}$, where $Y_1$ and $Y_2$ are $\\chi^2$ random variables with degrees of freedom $d_1$ and $d_2$, respectively.\n",
    "\n",
    "The an $F$ random variable is essentially a ratio of variances. Consequently, constructing the $F$ test statistic is done by taking the ratio of the sample variances of the data that we want to test. We can simply choose $\\sigma_1^2$ and $\\sigma_2^2$ to represent either of the variances that we are comparing in order so that our F-statistic is greater than $1$. \n",
    "\n",
    "$$ F = \\frac{s_1^2}{s_2^2} $$\n",
    "\n",
    "Let's compare SPY and AAPL to see whether their variances are the same (a 'not equal to' hypothesis test). We will use a $\\alpha = 0.05$ test. Recall that for a two-sided test, we calculate the lower and upper critical values using values of $\\alpha/2$. We gather the data and calculate the test statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "qmcXPVDAxz5p"
   },
   "outputs": [],
   "source": [
    "symbol_list = [\"SPY\", \"AAPL\"]\n",
    "start = \"2015-01-01\"\n",
    "end = \"2016-01-01\"\n",
    "pricing_sample = get_pricing(symbol_list, start_date = start, end_date = end, fields = 'price')\n",
    "pricing_sample.columns = map(lambda x: x.symbol, pricing_sample.columns)\n",
    "returns_sample = pricing_sample.pct_change()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "10MjN5P1xz5p"
   },
   "outputs": [],
   "source": [
    "# Take returns from above, AAPL and SPY, and compare their variances\n",
    "spy_std_dev, aapl_std_dev = returns_sample.std()\n",
    "print 'SPY standard deviation is: ', spy_std_dev\n",
    "print 'AAPL standard deviation is: ', aapl_std_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "mq1jxWcvxz5q"
   },
   "source": [
    "Note that the standard deviation of AAPL is greater than the standard deviation of SPY. As a result we choose $\\sigma_1^2$ to represent the variance of AAPL and $\\sigma_2^2$ to represent the variance of SPY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "4_O8UyxZxz5q"
   },
   "outputs": [],
   "source": [
    "test_statistic = (aapl_std_dev / spy_std_dev)**2\n",
    "print \"F Test statistic: \", test_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "3jGqjDtixz5r"
   },
   "outputs": [],
   "source": [
    "# Since these values are taken over the same time period, they will have the same number of degrees of freedom\n",
    "df1 = len(returns_sample['AAPL']) - 1\n",
    "df2 = len(returns_sample['SPY']) - 1\n",
    "\n",
    "print 'Degrees of freedom for SPY: ', df2\n",
    "print 'Degrees of freedom for AAPL: ', df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "YNHwJfCFxz5s"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "xXT_M8KNxz5t"
   },
   "outputs": [],
   "source": [
    "upper_crit_value = f.ppf(0.975, df1, df2)\n",
    "lower_crit_value = f.ppf(0.025, df1, df2)\n",
    "print 'Upper critical value at a = 0.05 with df1 = {0} and df2 = {1}: '.format(df1, df2), upper_crit_value\n",
    "print 'Lower critical value at a = 0.05 with df1 = {0} and df2 = {1}: '.format(df1, df2), lower_crit_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "bOSSlbsdxz5t"
   },
   "source": [
    "We see that our F-statistic value is greater than the upper critical value for our F test. Thus we **reject** the null hypothesis in favor of the alternative and conclude that the variances of AAPL and SPY indeed do differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "jHH9FaK7xz5t"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "7UnJ_ASmxz5u"
   },
   "source": [
    "## AR, MA, ARIMA, ARCH and GARCH <a id=\"24\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "diEBPz33xz5u"
   },
   "source": [
    "### Serial Correlation (Autocorrelation)\n",
    "\n",
    "A time series model decomposes the series into three components: trend, seasonal/cyclical, and random. \n",
    "\n",
    "The random component is called the residual or error - the difference between our predicted value(s) and the observed value(s). Autocorrelation is when these residuals (errors) are correlated with each other. That is, if the error of the $i_{th}$ is dependent on errors of any of the terms $0 .. i-1$ before. Essentially, it tells us how sequential observations in a time series affect each other. \n",
    "\n",
    "Formally, for a covariance-stationary time series (as #3 above, where covariance between sequential observations is not a function of time), autocorrelation $\\rho_k$ for lag $k$ (the number of time steps separating two sequantial observations), $$\n",
    "\\rho_k = \\frac{COV(x_t, x_{t - k})}{\\sigma_x^2} = \\frac{E[(x_t - \\mu)(x_{t - k} - \\mu)}{\\sigma_x^2}\n",
    "$$\n",
    "\n",
    "A significant value of $\\rho_k$ indicates that the error of the $i_{th}$ is dependent on the previous k terms from ${i-k} .. {i-1}$.\n",
    "\n",
    "#### Why Do We Care about Serial Correlation? \n",
    "\n",
    "Serial correlation is critical for the validity of our model predictions - The residuals (errors) of a stationary TS are serially uncorrelated by definition. It is critical we account for autocorrelation in our model otherwise the standard errors of our estimates for the parameters will be biased and underestimated, making any tests that we try to form using the model invalid In layman's terms, ignoring autocorrelation means we're likely to draw incorrect conclusions about the impact of the independent variables in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "jym-_9Kwxz5u"
   },
   "source": [
    "#### Autoregressive Models of order p $AR(p)$\n",
    "\n",
    "The autoregressive model is simply an extension of the random walk. It is essentially a regression model which depends linearly on the previous terms:\n",
    "$$x_t = \\alpha_1x_{t-1}+…+\\alpha_px_{t-p}+w_t = \\sum_{i=1}^{p} t_i\\alpha_ix_{t-i}+w_t$$\n",
    "\n",
    "This is an AR model of order \"p\",where $p$ represents the number of previous (or lagged) terms used within the model, $\\alpha_i$ is the coefficient, and $w_t$ is a white noise term. Note that an AR(1) model with $\\alpha_1$ set equal to 1 is a random walk!\n",
    "\n",
    "One of the most important aspects of the AR(p) model is that it is not always stationary. The stationarity of a particular model depends upon the parameters. For example, an AR(1) model with $\\alpha_1$ = 1 is a random walk and therefore not stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "GKHY8ng2xz5v"
   },
   "source": [
    "### Moving Average Models - MA(q)\n",
    "\n",
    "MA(q) models are very similar to AR(p) models. MA(q) model is a linear combination of past error terms as opposed to a linear combination of past observations like the AR(p) model. The motivation for the MA model is that we can explain \"shocks\" in the error process directly by fitting a model to the error terms. (In an AR(p) model these shocks are observed indirectly by using past observations) \n",
    "\n",
    "$$x_t=w_t+\\beta_1w_{t-1}+…+\\beta_qw_{t-q}$$ \n",
    "\n",
    "Where $w_t$ is white noise with $E(w_t)=0$ and variance $\\sigma^2$\n",
    "\n",
    "By definition, ACF $ρ_k$ should be zero for k>q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "K-WjjXh1xz5v"
   },
   "source": [
    "### Autoregressive Moving Average Models - ARMA(p, q)\n",
    "\n",
    "ARMA model is simply the merger between AR(p) and MA(q) models:\n",
    "\n",
    "* AR(p) models try to capture (explain) the momentum and mean reversion effects often observed in trading markets (market participant effects). \n",
    "* MA(q) models try to capture (explain) the shock effects observed in the white noise terms. These shock effects could be thought of as unexpected events affecting the observation process e.g. Surprise earnings, A terrorist attack, etc.\n",
    "\n",
    "Hence, an ARMA model attempts to capture both of these aspects when modelling financial time series. Note that an ARMA model does not take into account volatility clustering, a key empirical phenomena of many financial time series which we will discuss later. \n",
    "\n",
    "$$x_t=\\alpha_1x_{t-1}+…+\\alpha_px_{t-p}+w_t+\\beta_1w_{t-1}+…+\\beta_qw_{t-q}$$ \n",
    "\n",
    "Where $w_t$ is white noise with $E(w_t)=0$ and variance $\\sigma^2$\n",
    "\n",
    "An ARMA model will often require fewer parameters than an AR(p) or MA(q) model alone. That is, it is redundant in its parameters\n",
    "\n",
    "However, for trading purposes we just need to have a predictive power that exceeds chance and produces enough profit above transaction costs, in order to be profitable in the long run.\n",
    "\n",
    "**So how do we decide the values of $p$ and $q$ ?**\n",
    "\n",
    "We exapnd on the method described in previous sheet. To fit data to an ARMA model, we use the [Akaike Information Criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) across a subset of values for p,q to find the model with minimum AIC and then apply the [Ljung-Box test](https://en.wikipedia.org/wiki/Ljung%E2%80%93Box_test) to determine if a good fit has been achieved, for particular values of p,q. If the p value of the test is greater the required significance, we can conclude that the residuals are independent and white noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "_6jK9WWFxz5v"
   },
   "source": [
    "### Autoregressive Integrated Moving Average Models - ARIMA(p, d, q)\n",
    "\n",
    "ARIMA is a natural extension to the class of ARMA models - they are used because they can reduce a non-stationary series to a stationary series using a sequence of differences. \n",
    "\n",
    "As previously mentioned many of our TS are not stationary, however they can be made stationary by differencing. We saw an example of this when we took the first difference of nonstationary Guassian random walk and proved that it equals stationary white noise.\n",
    "\n",
    "ARIMA essentially performs same function, but does so repeatedly, $d$ times, in order to reduce a non-stationary series to a stationary one.\n",
    "\n",
    "**Without diving too deeply into the equation, just know that a time series $x_t$ is integrated of order $d$ if we difference the series $d$ times and receive a discrete white noise series.**\n",
    "\n",
    "\n",
    "A time series $x_t$ is an autoregressive integrated moving average model of order p, d, q, ARIMA(p,d,q) if the series $x_t$ is differenced $d$ times, and it then follows an ARMA(p,q) process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "VZrq8BaFxz5v"
   },
   "source": [
    "#### Conditional Heteroskedasticity\n",
    "\n",
    "The main motivation for studying conditional heteroskedasticity in finance is that of volatility of asset returns.\n",
    "\n",
    "A collection of random variables is **heteroskedastic** if there are certain groups, or subsets, of variables within the larger set that have a different variance from the remaining variables.\n",
    "\n",
    "In finance, an increase in variance maybe correlated to a further increase in variance. For instance, on a day that equities markets undergo a substantial drop, automated risk management sell orders in long only portfolios get triggered, which lead to further fall in the price of equities within these portfolios, leading to significant downward volatility.\n",
    "\n",
    "These \"sell-off\" periods, as well as many other forms of volatility, lead to heteroskedasticity that is serially correlated and hence conditional on periods of increased variance. Thus we say that such series are conditional heteroskedastic.\n",
    "\n",
    "One of the challenging aspects of conditional heteroskedastic series is ACF plots of a series with volatility might still appear to be a realisation of stationary discrete white noise. This is despite the fact that the series is most definitely non-stationary as its variance is not constant in time.\n",
    "\n",
    "To incorporate CH in our model, we can create a model that utilises an autoregressive process for the variance itself - a model that actually accounts for the changes in the variance over time using past values of the variance.\n",
    "\n",
    "This is the basis of the Autoregressive Conditional Heteroskedastic (ARCH) model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "jAT4AUcVxz5v"
   },
   "source": [
    "### Autoregressive Conditionally Heteroskedastic Models - ARCH(p)\n",
    "\n",
    "ARCH(p) models can be thought of as simply an AR(p) model applied to the variance of a time series.\n",
    "\n",
    "$$Var(\\epsilon_t) = \\alpha_0 + \\alpha_1Var(\\epsilon_{t-1})+…+\\alpha_pVar(\\epsilon_{t-p})+w_t$$\n",
    "\n",
    "The actual time series is given by:\n",
    "\n",
    "$$\\epsilon_t = w_t\\sqrt{\\alpha_0 + \\alpha_1\\epsilon_{t-1}^2+…+\\alpha_p\\epsilon_{t-p}^2}$$\n",
    "\n",
    "For ARCH(1), this reads:\n",
    "$$\\epsilon_t = w_t\\sqrt{\\alpha_0+\\alpha_1\\epsilon_{t-1}^2}$$\n",
    "\n",
    "#### When To Apply ARCH(p)?\n",
    "\n",
    "When we were attempting to fit an AR(p) model and are concerned with the decay of the $p$ lag on a ACF plot of the series, we can apply the same logic to the square of the residuals. If we find that we can apply an AR(p) to these square residuals then we have an indication that an ARCH(p) process may be appropriate.\n",
    "\n",
    "**Note that ARCH(p) should only ever be applied to a series that has already had an appropriate model fitted sufficient to leave the residuals looking like discrete white noise**. Since we can only tell whether ARCH is appropriate or not by squaring the residuals and examining the ACF, we also need to ensure that the mean of the residuals is zero.\n",
    "\n",
    "ARCH should only ever be applied to series that do not have any trends or seasonal effects, i.e. that has no (evident) serially correlation. ARIMA is often applied to such a series, at which point ARCH may be a good fit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "R9OVycRtxz5v"
   },
   "source": [
    "### Generalized Autoregressive Conditionally Heteroskedastic Models - GARCH(p,q)\n",
    "\n",
    "GARCH(p, q)  is an ARMA model applied to the variance of a time series i.e., it has an autoregressive term and a moving average term. The AR(p) models the variance of the residuals (squared errors) or simply our time series squared. The MA(q) portion models the variance of the process.\n",
    "\n",
    "$$\\epsilon_t = \\sigma_t w_t$$\n",
    "\n",
    "Where $w_t$ is discrete white noise, with zero mean and unit variance, and $\\sigma^2$ is given by:\n",
    "\n",
    "$$\\sigma_t^2=\\alpha_0+\\sum_{i=1}^{p}\\alpha_i\\epsilon_{t-i}^2+\\sum_{j=1}^{q}\\beta_j\\sigma_{t-j}^2$$\n",
    "\n",
    "Where $\\alpha_i$ and $\\beta_j$ are parameters of the model.For GARCH(1,1), $\\sigma^2$ is:\n",
    "\n",
    "$$\\sigma_t^2=\\alpha_0+\\alpha_1\\epsilon_{t-1}^2+\\beta_1\\sigma_{t-1}^2$$\n",
    "\n",
    "$\\alpha_1 + \\beta_1$ must be less than 1 or the model is unstable. We can simulate a GARCH(1, 1) process below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "1lZkvThDxz5v"
   },
   "source": [
    "### Maximum Likelihood Estimator for GARCH Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "56zHnwyZxz5w"
   },
   "source": [
    "Now we'll define functions that, given our data, will compute the MLE for the $\\mu$ and $\\sigma$ parameters of the normal distribution.\n",
    "\n",
    "Recall that\n",
    "\n",
    "$$\\hat\\mu = \\frac{1}{T}\\sum_{t=1}^{T} x_t$$\n",
    "\n",
    "$$\\hat\\sigma = \\sqrt{\\frac{1}{T}\\sum_{t=1}^{T}{(x_t - \\hat\\mu)^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "vx5xqv9vxz5w"
   },
   "source": [
    "### Exponential Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "dEUoXoRvxz5w"
   },
   "source": [
    "`numpy` defines the exponential distribution as\n",
    "$$\\frac{1}{\\lambda}e^{-\\frac{x}{\\lambda}}$$\n",
    "\n",
    "So we need to invert the MLE from the lecture notes. There it is\n",
    "\n",
    "$$\\hat\\lambda = \\frac{T}{\\sum_{t=1}^{T} x_t}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Q7qAcU_8xz5w"
   },
   "source": [
    "### MLE for Asset Returns\n",
    "\n",
    "Now we'll fetch some real returns and try to fit a normal distribution to them using MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "9J8O95uexz5w"
   },
   "outputs": [],
   "source": [
    "prices = get_pricing('TSLA', fields='price', start_date='2014-01-01', end_date='2015-01-01')\n",
    "# This will give us the number of dollars returned each day\n",
    "absolute_returns = np.diff(prices)\n",
    "# This will give us the percentage return over the last day's value\n",
    "# the [:-1] notation gives us all but the last item in the array\n",
    "# We do this because there are no returns on the final price in the array.\n",
    "returns = absolute_returns/prices[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "RBbq8f4xxz5x"
   },
   "source": [
    "Let's use `scipy`'s fit function to get the $\\mu$ and $\\sigma$ MLEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "l_S5Fagixz5x",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mu, std = scipy.stats.norm.fit(returns)\n",
    "pdf = scipy.stats.norm.pdf\n",
    "x = np.linspace(-1,1, num=100)\n",
    "h = plt.hist(returns, bins=x, normed='true')\n",
    "l = plt.plot(x, pdf(x, loc=mu, scale=std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "DvjGxxIjxz5y"
   },
   "source": [
    "--- \n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "rLnk4lcrxz5y"
   },
   "source": [
    "## Introduction to Python and NumPy <a id=\"18\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "uDtbgE1Pxz5z"
   },
   "source": [
    "NumPy is an incredibly powerful package in Python that is ubiquitous throughout the Quantopian platform. It has strong integration with Pandas, another tool we will be covering in the lecture series. NumPy adds support for multi-dimensional arrays and mathematical functions that allow you to easily perform linear algebra calculations. This lecture will be a collection of linear algebra examples computed using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "Pajm9cfNxz5z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "-n6WcacWxz50"
   },
   "source": [
    "### Basic NumPy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "598i8Rhrxz50"
   },
   "source": [
    "The most basic way that we could make use of NumPy in finance is calculating the mean return of a portfolio. Say that we have a list containing the historical return of several stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "pcW1GVEsxz50"
   },
   "outputs": [],
   "source": [
    "stock_list = [3.5, 5, 2, 8, 4.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "KmCjfsDJxz51"
   },
   "source": [
    "We can make an array by calling a function on the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ckXBa7Guxz51"
   },
   "outputs": [],
   "source": [
    "returns = np.array(stock_list)\n",
    "print returns, type(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "aVnfhOiqxz52"
   },
   "source": [
    "You'll notice that the type of our array is 'ndarray', not just 'array'. This is because NumPy arrays can be created with multiple dimensions. If we pass np.array() a list of lists, it will create a 2-dimensional array. If we pass a list of lists of lists, it will create a 3-dimensional array, and so on and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ZsN5IlEkxz52"
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "print A, type(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "WgQyiUYCxz53"
   },
   "source": [
    "We can access the dimensions of an array by looking at its `shape` member variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "4hsIOLmhxz53"
   },
   "outputs": [],
   "source": [
    "print A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "43I16Xbsxz54"
   },
   "source": [
    "Arrays are indexed in much the same way as lists in Python. Elements of a list begin indexing from $0$ and end at $n - 1$, where $n$ is the length of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "mtyy2T3gxz54"
   },
   "outputs": [],
   "source": [
    "print returns[0], returns[len(returns) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "LvOpSIFtxz55"
   },
   "source": [
    "We can take a slice of an array using a colon, just like in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "37zCJLvUxz55"
   },
   "outputs": [],
   "source": [
    "print returns[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "JMcHwCxFxz56"
   },
   "source": [
    "A slice of an array, like in a list, will select a group of elements in the array starting from the first element indicated and going up to (but not including) the last element indicated.\n",
    "\n",
    "In the case of multidimensional arrays, many of the same conventions with slicing and indexing hold. We can access the first column of a 2-dimensional array like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "XRqhV3VMxz56"
   },
   "outputs": [],
   "source": [
    "print A[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "BWBawrzXxz56"
   },
   "source": [
    "And the first row of a 2-dimensional array like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Ck2tdHlHxz57"
   },
   "outputs": [],
   "source": [
    "print A[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "bLLkwEifxz58"
   },
   "source": [
    "Notice that each slice of the array returns yet another array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "YNocpWsyxz58"
   },
   "outputs": [],
   "source": [
    "print type(A[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "PZQxNy3Qxz59"
   },
   "source": [
    "Passing only one index to a 2-dimensional array will result in returning the row with the given index as well, providing us with another way to access individual rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "0iJrqcvqxz59"
   },
   "outputs": [],
   "source": [
    "print A[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "TWuV3Dcgxz5-"
   },
   "source": [
    "Accessing the index of an individual element will return only the element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "UV5f1Y-Ixz5-"
   },
   "outputs": [],
   "source": [
    "print A[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "NEhTYtdzxz5_"
   },
   "source": [
    "#### Array functions\n",
    "\n",
    "Functions built into NumPy can be easily called on arrays. Most functions are applied to an array element-wise (as scalar multiplication is). For example, if we call `log()` on an array, the logarithm will be taken of each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "XnDk5ZSLxz5_"
   },
   "outputs": [],
   "source": [
    "print np.log(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "aH30ym6Cxz6A"
   },
   "source": [
    "Some functions return a single value. This is because they treat the array as a collection (similar to a list), performing the designated function. For example, the `mean()` function will do exactly what you expect, calculating the mean of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "U-dJ4PRIxz6A"
   },
   "outputs": [],
   "source": [
    "print np.mean(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "jKGr0FWbxz6B"
   },
   "source": [
    "Or the `max()` function will return the maximum element of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "-fFFrRKoxz6B"
   },
   "outputs": [],
   "source": [
    "print np.max(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "UGBcPZsUxz6C"
   },
   "source": [
    "For further reading on the universal functions in NumPy, check out the [documentation](https://docs.scipy.org/doc/numpy/user/quickstart.html#universal-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "LiwmHescxz6C"
   },
   "source": [
    "### Return to the returns\n",
    "\n",
    "Now let's modify our returns array with scalar values. If we add a scalar value to an array it will be added to every element of the array. If we multiply an array by a scalar value it will be multiplied against every element of the array. If we do both, both will happen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "5UQex9I4xz6C",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "returns*2 + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "ljSe7ng0xz6D"
   },
   "source": [
    "NumPy also has functions specifically built to operate on arrays. Let's take the mean and standard deviation of this group of returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "5UzrJdpPxz6D"
   },
   "outputs": [],
   "source": [
    "print \"Mean: \", np.mean(returns), \"Std Dev: \", np.std(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "wCG04I92xz6E"
   },
   "source": [
    "Let's simulate a universe of stocks using NumPy's functions. First we need to create the arrays to hold the assets and returns that we will use to build a portfolio. This is because arrays are created with a fixed size. Their dimensions can't be changed without creating a new array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "VFSIibuoxz6E"
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "assets = np.zeros((N, 100))\n",
    "returns = np.zeros((N, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "mvIA7E9hxz6F"
   },
   "source": [
    "This function, `zeroes()`, creates a NumPy array with the given dimensions that is entirely filled in with $0$. We can pass a single value or a tuple of as many dimensions as we like. Passing in the tuple `(N, 100)`, will return a two-dimensional array with $N$ rows and $100$ columns. Our result is a $N \\times 100$ array.\n",
    "\n",
    "Now we will simulate a base asset. We want the universe of stocks to be correlated with each other so we will use this initial value to generate the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "4nN08sEXxz6F"
   },
   "outputs": [],
   "source": [
    "R_1 = np.random.normal(1.01, 0.03, 100)\n",
    "returns[0] = R_1\n",
    "assets[0] = np.cumprod(R_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "I8LNKLJpxz6G"
   },
   "source": [
    "The `random` module in NumPy is exceedingly useful. It contains methods for sampling from many different probability distributions, some of which are covered in the [random variables lecture](https://www.quantopian.com/lectures/random-variables) in the Quantopian lecture series. In this case we draw $N = 100$ random samples from a normal distribution with mean $1.01$ and standard deviation $0.03$. We treat these as the daily percentage returns of our asset and take the cumulative product of these samples to get the current price.\n",
    "\n",
    "The way we have generated our universe, the the individual $R_i$ vectors are each 1-dimensional arrays and the `returns` and `assets` variables contain 2-dimensional arrays. Above, we set the initial row of both `returns` and `assets` to be the first $R_i$ vector and the cumulative asset price based on those returns, respectively.\n",
    "\n",
    "We will now use this base asset to create a few other random assets that are correlated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "xkA0ABWExz6G"
   },
   "outputs": [],
   "source": [
    "# Generate assets that are correlated with R_1\n",
    "for i in range(1, N):\n",
    "    R_i = R_1 + np.random.normal(0.001, 0.02, 100)\n",
    "    returns[i] = R_i # Set each row of returns equal to the new R_i array\n",
    "    assets[i] = np.cumprod(R_i)\n",
    "    \n",
    "mean_returns = [(np.mean(R) - 1)*100 for R in returns]\n",
    "return_volatilities = [np.std(R) for R in returns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "FV5nz1pZxz6H"
   },
   "source": [
    "Here we generate the remaining $N - 1$ securities that we want in our universe by adding random noise to $R_1$. This ensures that our $N - 1$ other assets will be correlated with the base asset because they have some underlying information that is shared.\n",
    "\n",
    "Let's plot what the mean return of each asset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "iWRwZxOHxz6H",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.bar(np.arange(len(mean_returns)), mean_returns)\n",
    "plt.xlabel('Stock')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('Returns for {0} Random Assets'.format(N));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "RIuGsgeKxz6I"
   },
   "source": [
    "### Calculating Expected Return\n",
    "\n",
    "So we have a universe of stocks. Great! Now let's put them together in a portfolio and calculate its expected return and risk.\n",
    "\n",
    "We will start off by generating $N$ random weights for each asset in our portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "5XAa_GTTxz6I"
   },
   "outputs": [],
   "source": [
    "weights = np.random.uniform(0, 1, N)\n",
    "weights = weights/np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "58VN6r_uxz6J"
   },
   "source": [
    "We have to rescale the weights so that they all add up to $1$. We do this by scaling the weights vector by the sum total of all the weights. This step ensures that we will be using $100\\%$ of the portfolio's cash.\n",
    "\n",
    "To calculate the mean return of the portfolio, we have to scale each asset's return by its designated weight. We can pull each element of each array and multiply them individually, but it's quicker to use NumPy's linear algebra methods. The function that we want is `dot()`. This will calculate the dot product between two arrays for us. So if $v = \\left[ 1, 2, 3 \\right]$ and $w = \\left[4, 5, 6 \\right]$, then:\n",
    "\n",
    "$$ v \\cdot w = 1 \\times 4 + 2 \\times 5 + 3 \\times 6 $$\n",
    "\n",
    "For a one-dimensional vector, the dot product will multiply each element pointwise and add all the products together! In our case, we have a vector of weights, $\\omega = \\left[ \\omega_1, \\omega_2, \\dots \\omega_N\\right]$ and a vector of returns, $\\mu = \\left[ \\mu_1, \\mu_2, \\dots, \\mu_N\\right]$. If we take the dot product of these two we will get:\n",
    "\n",
    "$$ \\omega \\cdot \\mu = \\omega_1\\mu_1 + \\omega_2\\mu_2 + \\dots + \\omega_N\\mu_N = \\mu_P $$\n",
    "\n",
    "This yields the sum of all the asset returns scaled by their respective weights. This the the portfolio's overall expected return!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "mUBjIPmKxz6J"
   },
   "outputs": [],
   "source": [
    "p_returns = np.dot(weights, mean_returns)\n",
    "print \"Expected return of the portfolio: \", p_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "akhuhu0mxz6K"
   },
   "source": [
    "Calculating the mean return is fairly intuitive and does not require too much explanation of linear algebra. However, calculating the variance of our portfolio requires a bit more background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "cD_SHVs9xz6K"
   },
   "source": [
    "#### Beware of NaN values\n",
    "\n",
    "Most of the time, all of these calculations will work without an issue. However, when working with real data we run the risk of having `nan` values in our arrays. This is NumPy's way of saying that the data there is missing or doesn't exist. These `nan` values can lead to errors in mathematical calculations so it is important to be aware of whether your array contains `nan` values and to know how to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "bG6XIWphxz6K"
   },
   "outputs": [],
   "source": [
    "v = np.array([1, 2, np.nan, 4, 5])\n",
    "print v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "XsNyn0ZJxz6L"
   },
   "source": [
    "Let's see what happens when we try to take the mean of this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "x-q35iydxz6L"
   },
   "outputs": [],
   "source": [
    "print np.mean(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "V_HnnPs7xz6M"
   },
   "source": [
    "Clearly, `nan` values can have a large impact on our calculations. Fortunately, we can check for `nan` values with the `isnan()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "jfAu49hJxz6M"
   },
   "outputs": [],
   "source": [
    "np.isnan(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "CrVdDuV6xz6N"
   },
   "source": [
    "Calling `isnan()` on an array will call the function on each value of the array, returning a value of `True` if the element is `nan` and `False` if the element is valid. Now, knowing whether your array contains `nan` values is all well and good, but how do we remove `nan`s? Handily enough, NumPy arrays can be indexed by boolean values (`True` or `False`). If we use a boolean array to index an array, we will remove all values of the array that register as `False` under the condition. We use the `isnan()` function in create a boolean array, assigning a `True` value to everything that is *not* `nan` and a `False` to the `nan`s and we use that to index the same array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "iF0iEsVdxz6N"
   },
   "outputs": [],
   "source": [
    "ix = ~np.isnan(v) # the ~ indicates a logical not, inverting the bools\n",
    "print v[ix] # We can also just write v = v[~np.isnan(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "JgpnXIfdxz6O"
   },
   "outputs": [],
   "source": [
    "print np.mean(v[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "I1Kw7uEWxz6P"
   },
   "source": [
    "There are a few shortcuts to this process in the form of NumPy functions specifically built to handle them, such as `nanmean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "152M7UnRxz6P"
   },
   "outputs": [],
   "source": [
    "print np.nanmean(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "lDNerQttxz6S"
   },
   "source": [
    "The `nanmean()` function simply calculates the mean of the array as if there were no `nan` values at all! There are a few more of these functions, so feel free to read more about them in the [documentation](https://docs.scipy.org/doc/numpy/user/index.html). These indeterminate values are more an issue with data than linear algebra itself so it is helpful that there are ways to handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "SmSh0B9vxz6T"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Linear algebra is pervasive in finance and in general. For example, the calculation of *optimal* weights according to modern portfolio theory is done using linear algebra techniques. The arrays and functions in NumPy allow us to handle these calculations in an intuitive way. For a quick intro to linear algebra and how to use NumPy to do more significant matrix calculations, proceed to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "dAJqTNVrxz6T"
   },
   "source": [
    "### A brief foray into linear algebra\n",
    "\n",
    "Let's start with a basic overview of some linear algebra. Linear algebra comes down to the multiplication and composition of scalar and matrix values. A scalar value is just a real number that we multiply against an array. When we scale a matrix or array using a scalar, we multiply each individual element of that matrix or array by the scalar.\n",
    "\n",
    "A matrix is a collection of values, typically represented by an $m \\times n$ grid, where $m$ is the number of rows and $n$ is the number of columns. The edge lengths $m$ and $n$ do not necessarily have to be different. If we have $m = n$, we call this a square matrix. A particularly interesting case of a matrix is when $m = 1$ or $n = 1$. In this case we have a special case of a matrix that we call a vector. While there is a matrix object in NumPy we will be doing everything using NumPy arrays because they can have dimensions greater than $2$. For the purpose of this section, we will be using matrix and array interchangeably.\n",
    "\n",
    "We can express the matrix equation as:\n",
    "\n",
    "$$ y = A\\cdot x $$\n",
    "\n",
    "Where $A$ is an $m \\times n$ matrix, $y$ is a $m \\times 1$ vector, and $x$ is a $n \\times 1$ vector. On the right-hand side of the equation we are multiplying a matrix by a vector. This requires a little bit more clarification, lest we think that we can go about multiplying any matrices by any other matrices.\n",
    "\n",
    "#### Matrix multiplication\n",
    "\n",
    "With matrix multiplication, the order in which the matrices are multiplied matters. Multiplying a matrix on the left side by another matrix may be just fine, but multiplying on the right may be undefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "id": "y0Z3cL3Qxz6T"
   },
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "        [1, 2, 3, 12, 6],\n",
    "        [4, 5, 6, 15, 20],\n",
    "        [7, 8, 9, 10, 10]        \n",
    "    ])\n",
    "B = np.array([\n",
    "        [4, 4, 2],\n",
    "        [2, 3, 1],\n",
    "        [6, 5, 8],\n",
    "        [9, 9, 9]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "jYWYicwNxz6U"
   },
   "source": [
    "Notice that the above-defined matrices, $A$ and $B$, have different dimensions. $A$ is $3 \\times 5$ and $B$ is $4 \\times 3$. The general rule of what can and cannot be multiplied in which order is based on the dimensions of the matrices. Specifically, the number of columns in the matrix on the left must be equal to the number of rows in the matrix on the right. In super informal terms, let's say that we have an $m \\times n$ matrix and a $p \\times q$ matrix. If we multiply the first by the second on the right, we get the following:\n",
    "\n",
    "$$ (m \\times n) \\cdot (p \\times q) = (m \\times q) $$\n",
    "\n",
    "So the resultant product has the same number of rows as the left matrix and the same number of columns as the right matrix. This limitation of matrix multiplication with regards to dimensions is important to keep track of when writing code. To demonstrate this, we use the `dot()` function to multiply our matrices below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "_FHfDkCcxz6U"
   },
   "outputs": [],
   "source": [
    "print np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "0KVH41KMxz6V"
   },
   "source": [
    "These results make sense in accordance with our rule. Multiplying a $3 \\times 5$ matrix on the right by a $4 \\times 3$ matrix results in an error while multiplying a $4 \\times 3$ matrix on the right by a $3 \\times 5$ matrix results in a $4 \\times 5$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "JSGdJik_xz6V"
   },
   "outputs": [],
   "source": [
    "print np.dot(B, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "JeYDLa21xz6W"
   },
   "source": [
    "### Portfolio Variance\n",
    "\n",
    "Let's return to our portfolio example from before. We calculated the expected return of the portfolio, but how do we calculate the variance? We start by trying to evaluate the portfolio as a sum of each individual asset, scaled by it's weight.\n",
    "\n",
    "$$ VAR[P] = VAR[\\omega_1 S_1 + \\omega_2 S_2 + \\cdots + \\omega_N S_N] $$\n",
    "\n",
    "Where $S_0, \\cdots, S_N$ are the assets contained within our universe. If all of our assets were independent of each other, we could simply evaluate this as\n",
    "\n",
    "$$ VAR[P] = VAR[\\omega_1 S_1] + VAR[\\omega_2 S_2] + \\cdots + VAR[\\omega_N S_N] = \\omega_1^2\\sigma_1^2 + \\omega_2^2\\sigma_2^2 + \\cdots + \\omega_N^2\\sigma_N^2 $$\n",
    "\n",
    "However, all of our assets depend on each other by their construction. They are all in some way related to our base asset and therefore each other. We thus have to calculate the variance of the portfolio by including the individual pairwise covariances of each asset. Our formula for the variance of the portfolio:\n",
    "\n",
    "$$ VAR[P] = \\sigma_P^2 = \\sum_i \\omega_i^2\\sigma_i^2 + \\sum_i\\sum_{i\\neq j} \\omega_i\\omega_j\\sigma_i\\sigma_j\\rho_{i, j}, \\ i, j \\in \\lbrace 1, 2, \\cdots, N \\rbrace $$\n",
    "\n",
    "Where $\\rho_{i,j}$ is the correlation between $S_i$ and $S_j$, $\\rho_{i, j} = \\frac{COV[S_i, S_j]}{\\sigma_i\\sigma_j}$. This seems exceedingly complicated, but we can easily handle all of this using NumPy arrays. First, we calculate the covariance matrix that relates all the individual stocks in our universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "6CnyYP-Bxz6W"
   },
   "outputs": [],
   "source": [
    "cov_mat = np.cov(returns)\n",
    "print cov_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "fOdYhhRYxz6X"
   },
   "source": [
    "This array is not formatted particularly nicely, but a covariance matrix is a very important concept. The covariance matrix is of the form:\n",
    "\n",
    "$$ \\left[\\begin{matrix}\n",
    "VAR[S_1] & COV[S_1, S_2] & \\cdots & COV[S_1, S_N] \\\\\n",
    "COV[S_2, S_1] & VAR[S_2] & \\cdots & COV[S_2, S_N] \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "COV[S_N, S_1] & COV[S_N, S_2] & \\cdots & VAR[S_N]\n",
    "\\end{matrix}\\right] $$\n",
    "\n",
    "So each diagonal entry is the variance of that asset at that index and each off-diagonal holds the covariance of two assets indexed by the column and row number. What is important is that once we have the covariance matrix we are able to do some very quick linear algebra to calculate the variance of the overall portfolio. We can represent the variance of the portfolio in array form as:\n",
    "\n",
    "$$ \\sigma_p^2 = \\omega \\ C \\ \\omega^\\intercal$$\n",
    "\n",
    "Where $C$ is the covariance matrix of all the assets and $\\omega$ is the array containing the weights of each individual asset. The superscript $\\intercal$ on the second $\\omega$ listed above denotes the **transpose** of $\\omega$. For a reference on the evaluation of the variance of a portfolio as a matrix equation, please see the Wikipedia article on [modern portfolio theory](https://en.wikipedia.org/wiki/Modern_portfolio_theory).\n",
    "\n",
    "The transpose of an array is what you get when you switch the rows and columns of an array. This has the effect of reflecting an array across what you might imagine as a diagonal. For example, take our array $A$ from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "DxMF7py9xz6X"
   },
   "outputs": [],
   "source": [
    "print A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "uuvNJIqhxz6Y"
   },
   "source": [
    "The transpose looks like a mirror image of the same array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "1uRmD_tJxz6Y"
   },
   "outputs": [],
   "source": [
    "print np.transpose(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "CJ8GR62Hxz6Z"
   },
   "source": [
    "But $\\omega$ here is a 1-dimensional array, a vector! It makes perfect to take the transpose of $A$, a $3 \\times 5$ array, as the output will be a $5 \\times 3$ array, but a 1-dimensional array is not quite as intuitive. A typical 1-dimensional array can be thought of as a $1 \\times n$ horizontal vector. Thus, taking the tranpose of this array essentially means changing it into a $n \\times 1$ vertical vector. This makes sense because 1-dimensional arrays are still arrays and any multiplication done between 1-dimensional and higher dimensional arrays must keep in line with our dimensionality issue of matrix multiplication.\n",
    "\n",
    "To make a long story short, we think of $\\omega$ as $1 \\times N$ since we have $N$ securities. This makes it so that $\\omega^\\intercal$ is $N \\times 1$. Again, our covariance matrix is $N \\times N$. So the overall multiplication works out like so, in informal terms:\n",
    "\n",
    "$$ \\text{Dimensions}(\\sigma_p^2) = \\text{Dimensions}(\\omega C \\omega^\\intercal) = (1 \\times N)\\cdot (N \\times N)\\cdot (N \\times 1) = (1 \\times 1)$$\n",
    "\n",
    "Multiplying the covariance matrix on the left by the plain horizontal vector and on the right by that vector's transpose results in the calculation of a single scalar ($1 \\times 1$) value, our portfolio's variance.\n",
    "\n",
    "So knowing this, let's proceed and calculate the portfolio variance! We can easily calculate the product of these arrays by using `dot()` for matrix multiplication, though this time we have to do it twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "UU3sTH9Yxz6Z",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculating the portfolio volatility\n",
    "var_p = np.dot(np.dot(weights, cov_mat), weights.T)\n",
    "vol_p = np.sqrt(var_p)\n",
    "print \"Portfolio volatility: \", vol_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "z1WdvSuLxz6Z"
   },
   "source": [
    "To confirm this calculation, let's simply evaluate the volatility of the portfolio using only NumPy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "H623Wgu8xz6a"
   },
   "outputs": [],
   "source": [
    "# Confirming calculation\n",
    "vol_p_alt = np.sqrt(np.var(np.dot(weights, returns), ddof=1))\n",
    "print \"Portfolio volatility: \", vol_p_alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "BppfMYVJxz6b"
   },
   "source": [
    "The `ddof` parameter is a simple integer input that tells the function the number of degrees of freedom to take into account. This is a more statistical concept, but what this tells us that our matrix calculation is correct!\n",
    "\n",
    "A lot of this might not make sense at first glance. It helps to go back and forth between the theory and the code representations until you have a better grasp of the mathematics involved. It is definitely not necessary to be an expert on linear algebra and on matrix operations, but linear algebra can help to streamline the process of working with large amounts of data. For further reading on NumPy, check out the [documentation](https://docs.scipy.org/doc/numpy/user/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for completing this lecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p>Copyright &copy; 2020 Jonas Gottal. This notebook and its source code are released exclusively for the <a href=\"https://www.tuinvest.de/\">TU Investment Club e.V.</a>.</p>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [
    "AxaGJt3Gxzsx",
    "ymOTnjUnxztD",
    "-X41cM4TxztG",
    "JLo3LokoxztW",
    "jhwYp5QUxzt5",
    "MOaXfC1rxzuE",
    "qjgL62JFxzu8",
    "846iUw-ixzvK",
    "2ur7GMcXxzvy",
    "A7FsRDU1xzv0",
    "z1N2KYfSxzv8",
    "qKGUNdSixzv9",
    "1f8Irg3Exzv9",
    "dHjtiZrixzwA",
    "WbwUc2ymxzwB",
    "2mDdqYKaxzwC",
    "uu_99BfWxzwD",
    "s1TEhfQ_xzwH",
    "knZ3jKa6xzwI",
    "5E_0bxuNxzwI",
    "6TCsYvjZxzwJ",
    "eShse07zxzwK",
    "ij5Fx7c2xzwM",
    "9QZl9SeSxzwP",
    "hG0RiF3nxzwr",
    "316ShvIAxzws",
    "b5QuawArxzwt",
    "DODsi4PFxzwv",
    "sSD_BlHJxzw4",
    "7cTlIvVhxzw5",
    "kblR12KDxzw5",
    "NmWeuv8qxz5R",
    "PSSrr6vRxz5S",
    "8P7A4VLHxz5S",
    "LIegmd-zxz5S",
    "CjYy-varxz5S",
    "lIK50Fuvxz5T",
    "gCoUbKfOxz5T",
    "kzytC58txz5T",
    "Iv6oWKXIxz5T",
    "yxqga1soxz5T",
    "twts2ixPxz5U",
    "3DOSxadVxz5U",
    "D85fkxdexz5U",
    "ln9_C7rhxz5U",
    "G8IF6iZLxz5U",
    "wGGOLQo3xz5V",
    "l8TPzKV2xz5V",
    "FbfOMtLJxz5X",
    "EMB2PbfDxz5X",
    "l7XWOZAzxz5X",
    "dflbkf6Oxz5Y",
    "azSv-Bdlxz5Y",
    "rLQRecWYxz5Y",
    "KGhiJIA_xz5Y",
    "kiKA2hSaxz5Y",
    "IXgoEQ9fxz5Y",
    "CXrRtPGVxz5Z",
    "pZWKDAAvxz5Z",
    "qlVzveScxz5a",
    "Vjp-taiZxz5a",
    "POmfsk5Jxz5a",
    "VA5nyndfxz5d",
    "tkdMlO9bxz5e",
    "Rn7IvhQqxz5i",
    "ifLoBmY1xz5k",
    "SA4MZBIrxz5p",
    "7UnJ_ASmxz5u",
    "diEBPz33xz5u",
    "jym-_9Kwxz5u",
    "GKHY8ng2xz5v",
    "K-WjjXh1xz5v",
    "_6jK9WWFxz5v",
    "VZrq8BaFxz5v",
    "jAT4AUcVxz5v",
    "R9OVycRtxz5v",
    "1lZkvThDxz5v",
    "vx5xqv9vxz5w",
    "Q7qAcU_8xz5w",
    "rLnk4lcrxz5y",
    "-n6WcacWxz50",
    "NEhTYtdzxz5_",
    "LiwmHescxz6C",
    "RIuGsgeKxz6I",
    "cD_SHVs9xz6K",
    "SmSh0B9vxz6T",
    "dAJqTNVrxz6T",
    "JeYDLa21xz6W"
   ],
   "name": "Lecture1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
